{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf86f21",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:39.676414Z",
     "iopub.status.busy": "2024-08-29T11:37:39.676089Z",
     "iopub.status.idle": "2024-08-29T11:37:40.504251Z",
     "shell.execute_reply": "2024-08-29T11:37:40.503350Z"
    },
    "papermill": {
     "duration": 0.868385,
     "end_time": "2024-08-29T11:37:40.506700",
     "exception": false,
     "start_time": "2024-08-29T11:37:39.638315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_EmotionMoodtags_Dataset.csv\n",
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_dataset_main.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a06ef",
   "metadata": {
    "papermill": {
     "duration": 0.035154,
     "end_time": "2024-08-29T11:37:40.579157",
     "exception": false,
     "start_time": "2024-08-29T11:37:40.544003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATASET & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0267d3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:40.650318Z",
     "iopub.status.busy": "2024-08-29T11:37:40.649539Z",
     "iopub.status.idle": "2024-08-29T11:37:40.905153Z",
     "shell.execute_reply": "2024-08-29T11:37:40.904315Z"
    },
    "papermill": {
     "duration": 0.293282,
     "end_time": "2024-08-29T11:37:40.907203",
     "exception": false,
     "start_time": "2024-08-29T11:37:40.613921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For emoji cleaning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "'''For emoji cleaning'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4d730e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:40.979713Z",
     "iopub.status.busy": "2024-08-29T11:37:40.979246Z",
     "iopub.status.idle": "2024-08-29T11:37:41.028087Z",
     "shell.execute_reply": "2024-08-29T11:37:41.027352Z"
    },
    "papermill": {
     "duration": 0.087075,
     "end_time": "2024-08-29T11:37:41.030069",
     "exception": false,
     "start_time": "2024-08-29T11:37:40.942994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/kaggle/input/dataset-tachygraphy/Tachygraphy_EmotionMoodtags_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51028760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:41.102872Z",
     "iopub.status.busy": "2024-08-29T11:37:41.102594Z",
     "iopub.status.idle": "2024-08-29T11:37:41.106483Z",
     "shell.execute_reply": "2024-08-29T11:37:41.105634Z"
    },
    "papermill": {
     "duration": 0.042283,
     "end_time": "2024-08-29T11:37:41.108423",
     "exception": false,
     "start_time": "2024-08-29T11:37:41.066140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9615f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:41.180395Z",
     "iopub.status.busy": "2024-08-29T11:37:41.180084Z",
     "iopub.status.idle": "2024-08-29T11:37:41.202349Z",
     "shell.execute_reply": "2024-08-29T11:37:41.201482Z"
    },
    "papermill": {
     "duration": 0.060874,
     "end_time": "2024-08-29T11:37:41.204346",
     "exception": false,
     "start_time": "2024-08-29T11:37:41.143472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
    "\n",
    "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
    "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
    "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "                'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471ae094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:41.275220Z",
     "iopub.status.busy": "2024-08-29T11:37:41.274950Z",
     "iopub.status.idle": "2024-08-29T11:37:41.287529Z",
     "shell.execute_reply": "2024-08-29T11:37:41.286681Z"
    },
    "papermill": {
     "duration": 0.050209,
     "end_time": "2024-08-29T11:37:41.289357",
     "exception": false,
     "start_time": "2024-08-29T11:37:41.239148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "    text = str(text).lower()    #Making Text Lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for word in mapping.keys():\n",
    "        if \"\"+word+\"\" in text:\n",
    "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    '''Cleans special characters present(if any)'''   \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    '''Corrects common spelling errors'''   \n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fae05b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:41.362231Z",
     "iopub.status.busy": "2024-08-29T11:37:41.361919Z",
     "iopub.status.idle": "2024-08-29T11:37:41.366881Z",
     "shell.execute_reply": "2024-08-29T11:37:41.366020Z"
    },
    "papermill": {
     "duration": 0.043976,
     "end_time": "2024-08-29T11:37:41.368702",
     "exception": false,
     "start_time": "2024-08-29T11:37:41.324726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_text(text)\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_special_chars(text, punct, punct_mapping)\n",
    "#     text = correct_spelling(text, mispell_dict)\n",
    "    text = remove_space(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd3c37f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:41.442171Z",
     "iopub.status.busy": "2024-08-29T11:37:41.441856Z",
     "iopub.status.idle": "2024-08-29T11:37:41.455846Z",
     "shell.execute_reply": "2024-08-29T11:37:41.455010Z"
    },
    "papermill": {
     "duration": 0.052854,
     "end_time": "2024-08-29T11:37:41.457775",
     "exception": false,
     "start_time": "2024-08-29T11:37:41.404921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                last session of the day \n",
       "1       shanghai is also really exciting precisely  sk...\n",
       "2                                  submit the report asap\n",
       "3                                              happy bday\n",
       "4                                      the ogs  i like it\n",
       "                              ...                        \n",
       "4953      make a pet face  wtf wrong with me tonight haha\n",
       "4954         i dnt care anymore  boyz aint worth d drama \n",
       "4955    no relationship is perfect tho me bae goo from...\n",
       "4956    over here tryna get my nail polishes and shit lol\n",
       "4957                       no one was loved d way i luv u\n",
       "Name: text, Length: 4958, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "082bb659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:41.528676Z",
     "iopub.status.busy": "2024-08-29T11:37:41.528401Z",
     "iopub.status.idle": "2024-08-29T11:37:41.540425Z",
     "shell.execute_reply": "2024-08-29T11:37:41.539723Z"
    },
    "papermill": {
     "duration": 0.049729,
     "end_time": "2024-08-29T11:37:41.542438",
     "exception": false,
     "start_time": "2024-08-29T11:37:41.492709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "df[emotion_columns] = df[emotion_columns].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea4f1e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:41.614171Z",
     "iopub.status.busy": "2024-08-29T11:37:41.613542Z",
     "iopub.status.idle": "2024-08-29T11:37:41.618289Z",
     "shell.execute_reply": "2024-08-29T11:37:41.617499Z"
    },
    "papermill": {
     "duration": 0.042572,
     "end_time": "2024-08-29T11:37:41.620323",
     "exception": false,
     "start_time": "2024-08-29T11:37:41.577751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "# df['anger'] = df['anger'].astype(float)\n",
    "# df['disgust'] = df['disgust'].astype(float)\n",
    "# df['fear'] = df['fear'].astype(float)\n",
    "# df['joy'] = df['joy'].astype(float)\n",
    "# df['neutral'] = df['neutral'].astype(float)\n",
    "# df['sadness'] = df['sadness'].astype(float)\n",
    "# df['surprise'] = df['surprise'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "852fcd5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:41.692972Z",
     "iopub.status.busy": "2024-08-29T11:37:41.692691Z",
     "iopub.status.idle": "2024-08-29T11:37:44.147979Z",
     "shell.execute_reply": "2024-08-29T11:37:44.147227Z"
    },
    "papermill": {
     "duration": 2.494658,
     "end_time": "2024-08-29T11:37:44.150347",
     "exception": false,
     "start_time": "2024-08-29T11:37:41.655689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: text_preprocessing_pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ac266d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:44.222508Z",
     "iopub.status.busy": "2024-08-29T11:37:44.221741Z",
     "iopub.status.idle": "2024-08-29T11:37:44.229326Z",
     "shell.execute_reply": "2024-08-29T11:37:44.228339Z"
    },
    "papermill": {
     "duration": 0.04516,
     "end_time": "2024-08-29T11:37:44.231217",
     "exception": false,
     "start_time": "2024-08-29T11:37:44.186057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 last session of the day\n",
       "1       shanghai is also really exciting precisely sky...\n",
       "2                                  submit the report asap\n",
       "3                                              happy bday\n",
       "4                                       the ogs i like it\n",
       "                              ...                        \n",
       "4953       make a pet face wtf wrong with me tonight haha\n",
       "4954           i dnt care anymore boyz aint worth d drama\n",
       "4955    no relationship is perfect tho me bae goo from...\n",
       "4956    over here tryna get my nail polishes and shit lol\n",
       "4957                       no one was loved d way i luv u\n",
       "Name: text, Length: 4958, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dbe1434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:44.302544Z",
     "iopub.status.busy": "2024-08-29T11:37:44.302220Z",
     "iopub.status.idle": "2024-08-29T11:37:44.309754Z",
     "shell.execute_reply": "2024-08-29T11:37:44.309010Z"
    },
    "papermill": {
     "duration": 0.045061,
     "end_time": "2024-08-29T11:37:44.311606",
     "exception": false,
     "start_time": "2024-08-29T11:37:44.266545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['serial', 'pred', 'label', 'score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f593fc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:44.383609Z",
     "iopub.status.busy": "2024-08-29T11:37:44.383036Z",
     "iopub.status.idle": "2024-08-29T11:37:44.387065Z",
     "shell.execute_reply": "2024-08-29T11:37:44.386239Z"
    },
    "papermill": {
     "duration": 0.042142,
     "end_time": "2024-08-29T11:37:44.388899",
     "exception": false,
     "start_time": "2024-08-29T11:37:44.346757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_label_mapping = {\n",
    "    0: \"anger\", 1: \"disgust\", 2: \"fear\", 3: \"joy\", 4: \"neutral\",\n",
    "    5: \"sadness\", 6: \"surprise\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb7f589d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:44.495453Z",
     "iopub.status.busy": "2024-08-29T11:37:44.495077Z",
     "iopub.status.idle": "2024-08-29T11:37:44.499526Z",
     "shell.execute_reply": "2024-08-29T11:37:44.498745Z"
    },
    "papermill": {
     "duration": 0.077504,
     "end_time": "2024-08-29T11:37:44.501486",
     "exception": false,
     "start_time": "2024-08-29T11:37:44.423982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMOTION_LABELS = [\n",
    "    \"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\",\n",
    "    \"sadness\", \"surprise\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91c6a318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:44.575170Z",
     "iopub.status.busy": "2024-08-29T11:37:44.574899Z",
     "iopub.status.idle": "2024-08-29T11:37:44.579644Z",
     "shell.execute_reply": "2024-08-29T11:37:44.578753Z"
    },
    "papermill": {
     "duration": 0.043522,
     "end_time": "2024-08-29T11:37:44.581503",
     "exception": false,
     "start_time": "2024-08-29T11:37:44.537981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_train_split(dataset, test_ratio=0.1):\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  return dataset[~test_indices], dataset[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b5c6b8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:44.653923Z",
     "iopub.status.busy": "2024-08-29T11:37:44.653629Z",
     "iopub.status.idle": "2024-08-29T11:37:44.663369Z",
     "shell.execute_reply": "2024-08-29T11:37:44.662263Z"
    },
    "papermill": {
     "duration": 0.04814,
     "end_time": "2024-08-29T11:37:44.665519",
     "exception": false,
     "start_time": "2024-08-29T11:37:44.617379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4503 examples in training, 455 examples in testing.\n"
     ]
    }
   ],
   "source": [
    "train_ds_pd, validation_ds_pd = test_train_split(df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_ds_pd), len(validation_ds_pd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c846877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:44.740410Z",
     "iopub.status.busy": "2024-08-29T11:37:44.739623Z",
     "iopub.status.idle": "2024-08-29T11:37:44.745256Z",
     "shell.execute_reply": "2024-08-29T11:37:44.744561Z"
    },
    "papermill": {
     "duration": 0.043538,
     "end_time": "2024-08-29T11:37:44.747118",
     "exception": false,
     "start_time": "2024-08-29T11:37:44.703580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_pd = train_ds_pd.reset_index(drop=True)\n",
    "validation_ds_pd = validation_ds_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b263f42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:44.820811Z",
     "iopub.status.busy": "2024-08-29T11:37:44.820547Z",
     "iopub.status.idle": "2024-08-29T11:37:48.365964Z",
     "shell.execute_reply": "2024-08-29T11:37:48.364982Z"
    },
    "papermill": {
     "duration": 3.583665,
     "end_time": "2024-08-29T11:37:48.368394",
     "exception": false,
     "start_time": "2024-08-29T11:37:44.784729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8313c35",
   "metadata": {
    "papermill": {
     "duration": 0.035368,
     "end_time": "2024-08-29T11:37:48.440411",
     "exception": false,
     "start_time": "2024-08-29T11:37:48.405043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SETTING CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89420b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:48.512880Z",
     "iopub.status.busy": "2024-08-29T11:37:48.512401Z",
     "iopub.status.idle": "2024-08-29T11:37:48.596106Z",
     "shell.execute_reply": "2024-08-29T11:37:48.595090Z"
    },
    "papermill": {
     "duration": 0.122674,
     "end_time": "2024-08-29T11:37:48.598096",
     "exception": false,
     "start_time": "2024-08-29T11:37:48.475422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7049c7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:48.670174Z",
     "iopub.status.busy": "2024-08-29T11:37:48.669854Z",
     "iopub.status.idle": "2024-08-29T11:37:48.674221Z",
     "shell.execute_reply": "2024-08-29T11:37:48.673406Z"
    },
    "papermill": {
     "duration": 0.04265,
     "end_time": "2024-08-29T11:37:48.676064",
     "exception": false,
     "start_time": "2024-08-29T11:37:48.633414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e742571d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:48.747753Z",
     "iopub.status.busy": "2024-08-29T11:37:48.747466Z",
     "iopub.status.idle": "2024-08-29T11:37:51.170918Z",
     "shell.execute_reply": "2024-08-29T11:37:51.170150Z"
    },
    "papermill": {
     "duration": 2.461855,
     "end_time": "2024-08-29T11:37:51.173213",
     "exception": false,
     "start_time": "2024-08-29T11:37:48.711358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import DebertaV2Model, DebertaV2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25c60c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:51.245693Z",
     "iopub.status.busy": "2024-08-29T11:37:51.245206Z",
     "iopub.status.idle": "2024-08-29T11:37:52.723001Z",
     "shell.execute_reply": "2024-08-29T11:37:52.721840Z"
    },
    "papermill": {
     "duration": 1.516931,
     "end_time": "2024-08-29T11:37:52.725784",
     "exception": false,
     "start_time": "2024-08-29T11:37:51.208853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8f0578f4474aa9a776fddb4f74ef40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c832b0e97943b886a2a1e820f8fef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e332006586547d893f1fe2b52912eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cfae7c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:52.801138Z",
     "iopub.status.busy": "2024-08-29T11:37:52.800800Z",
     "iopub.status.idle": "2024-08-29T11:37:52.804903Z",
     "shell.execute_reply": "2024-08-29T11:37:52.804027Z"
    },
    "papermill": {
     "duration": 0.043755,
     "end_time": "2024-08-29T11:37:52.806790",
     "exception": false,
     "start_time": "2024-08-29T11:37:52.763035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423183e9",
   "metadata": {
    "papermill": {
     "duration": 0.03982,
     "end_time": "2024-08-29T11:37:52.882146",
     "exception": false,
     "start_time": "2024-08-29T11:37:52.842326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TORCH IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "507c8d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:37:52.967110Z",
     "iopub.status.busy": "2024-08-29T11:37:52.966403Z",
     "iopub.status.idle": "2024-08-29T11:38:10.714085Z",
     "shell.execute_reply": "2024-08-29T11:38:10.713231Z"
    },
    "papermill": {
     "duration": 17.787881,
     "end_time": "2024-08-29T11:38:10.716670",
     "exception": false,
     "start_time": "2024-08-29T11:37:52.928789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 11:37:55,212\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-08-29 11:37:55,778\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "# from optuna.integration import PyTorchLightningPruner\n",
    "from ray import tune\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "# from ray.tune.integration.pytorch import TuneReportCallback\n",
    "from torch.amp import GradScaler, autocast\n",
    "# from ray.tune.integration.optuna import OptunaSearch\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from torch import autocast\n",
    "# from ray import tune\n",
    "# from ray.tune.integration.tensorboard import TensorBoardReporter\n",
    "from ray.tune.logger import TBXLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ray.train import report\n",
    "# from ray.tune.integration.jupyter import JupyterNotebookReporter\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from ray.tune.schedulers import HyperBandScheduler, AsyncHyperBandScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2adae",
   "metadata": {
    "papermill": {
     "duration": 0.036326,
     "end_time": "2024-08-29T11:38:10.791315",
     "exception": false,
     "start_time": "2024-08-29T11:38:10.754989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATASET CONFIG & ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8640446a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:10.867301Z",
     "iopub.status.busy": "2024-08-29T11:38:10.865621Z",
     "iopub.status.idle": "2024-08-29T11:38:10.875042Z",
     "shell.execute_reply": "2024-08-29T11:38:10.874267Z"
    },
    "papermill": {
     "duration": 0.049837,
     "end_time": "2024-08-29T11:38:10.876984",
     "exception": false,
     "start_time": "2024-08-29T11:38:10.827147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         text = self.data.iloc[index, 0]\n",
    "#         labels = self.data.iloc[index, 1:].values.astype(float)\n",
    "        text = str(self.data.iloc[index]['text'])\n",
    "        labels = self.data.iloc[index][['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']].values.astype(np.float32)\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3325d5",
   "metadata": {
    "papermill": {
     "duration": 0.036116,
     "end_time": "2024-08-29T11:38:10.950705",
     "exception": false,
     "start_time": "2024-08-29T11:38:10.914589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5458ed56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.024362Z",
     "iopub.status.busy": "2024-08-29T11:38:11.023708Z",
     "iopub.status.idle": "2024-08-29T11:38:11.032088Z",
     "shell.execute_reply": "2024-08-29T11:38:11.031268Z"
    },
    "papermill": {
     "duration": 0.047089,
     "end_time": "2024-08-29T11:38:11.033961",
     "exception": false,
     "start_time": "2024-08-29T11:38:10.986872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.roberta = roberta_model\n",
    "        self.drop = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.roberta.config.hidden_size, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.out = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_states = output.last_hidden_state\n",
    "        \n",
    "        # Extract the [CLS] token representation (first token in the sequence)\n",
    "        cls_token_state = output.last_hidden_state[:, 0, :]\n",
    "        output = self.drop(cls_token_state)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.drop(output)\n",
    "        output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aedc81",
   "metadata": {
    "papermill": {
     "duration": 0.036076,
     "end_time": "2024-08-29T11:38:11.106673",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.070597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Other Models to try out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fbfbc95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.183753Z",
     "iopub.status.busy": "2024-08-29T11:38:11.183396Z",
     "iopub.status.idle": "2024-08-29T11:38:11.188043Z",
     "shell.execute_reply": "2024-08-29T11:38:11.187131Z"
    },
    "papermill": {
     "duration": 0.045555,
     "end_time": "2024-08-29T11:38:11.190048",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.144493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "#         super(EmotionModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.out = nn.Linear(self.roberta.config.hidden_size, n_classes)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         output = self.drop(output.pooler_output)\n",
    "#         return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1946ea5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.265568Z",
     "iopub.status.busy": "2024-08-29T11:38:11.265215Z",
     "iopub.status.idle": "2024-08-29T11:38:11.269826Z",
     "shell.execute_reply": "2024-08-29T11:38:11.269001Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044264,
     "end_time": "2024-08-29T11:38:11.271593",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.227329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "#         super(EmotionModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.fc1 = nn.Linear(self.roberta.config.hidden_size, 512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.out = nn.Linear(256, n_classes)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Get the RoBERTa output\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         # Use the pooled output for classification tasks\n",
    "#         pooled_output = output.pooler_output\n",
    "#         # Pass through the custom layers\n",
    "#         output = self.drop(pooled_output)\n",
    "#         output = self.relu(self.fc1(output))\n",
    "#         output = self.drop(output)\n",
    "#         output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "#         return self.out(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff540abc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.345437Z",
     "iopub.status.busy": "2024-08-29T11:38:11.345065Z",
     "iopub.status.idle": "2024-08-29T11:38:11.350033Z",
     "shell.execute_reply": "2024-08-29T11:38:11.349039Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044696,
     "end_time": "2024-08-29T11:38:11.352199",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.307503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate, hidden_size):\n",
    "#         super(EmotionModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.fc1 = nn.Linear(self.roberta.config.hidden_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "#         self.out = nn.Linear(hidden_size // 2, n_classes)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Get the RoBERTa output\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         # Use the pooled output for classification tasks\n",
    "#         pooled_output = output.pooler_output\n",
    "#         # Pass through the custom layers\n",
    "#         output = self.drop(pooled_output)\n",
    "#         output = self.relu(self.fc1(output))\n",
    "#         output = self.drop(output)\n",
    "#         output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "#         return self.out(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29246bf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.425986Z",
     "iopub.status.busy": "2024-08-29T11:38:11.425682Z",
     "iopub.status.idle": "2024-08-29T11:38:11.430054Z",
     "shell.execute_reply": "2024-08-29T11:38:11.429129Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.04331,
     "end_time": "2024-08-29T11:38:11.432151",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.388841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class RoBERTaEmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model_name, num_emotions=7):\n",
    "#         super(RoBERTaEmotionModel, self).__init__()\n",
    "#         self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "#         self.drop = nn.Dropout(p=0.3)\n",
    "#         self.out = nn.Linear(self.roberta.config.hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.roberta(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         pooled_output = outputs[1]  # CLS token\n",
    "#         output = self.drop(pooled_output)\n",
    "#         return self.out(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af911d",
   "metadata": {
    "papermill": {
     "duration": 0.036322,
     "end_time": "2024-08-29T11:38:11.504919",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.468597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN & VALIDATION \n",
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0fe5c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.577888Z",
     "iopub.status.busy": "2024-08-29T11:38:11.577529Z",
     "iopub.status.idle": "2024-08-29T11:38:11.582604Z",
     "shell.execute_reply": "2024-08-29T11:38:11.581741Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044138,
     "end_time": "2024-08-29T11:38:11.584462",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.540324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, n_epochs):\n",
    "#     for epoch in range(n_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for data in train_loader:\n",
    "#             input_ids = data['input_ids'].to(device)\n",
    "#             attention_mask = data['attention_mask'].to(device)\n",
    "#             labels = data['labels'].to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             with autocast(device_type=device.type):\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             if scheduler:\n",
    "#                 scheduler.step()\n",
    "\n",
    "#             train_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "#         train_accuracy = correct / total\n",
    "#         val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "#         print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "059255c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.656619Z",
     "iopub.status.busy": "2024-08-29T11:38:11.656353Z",
     "iopub.status.idle": "2024-08-29T11:38:11.660583Z",
     "shell.execute_reply": "2024-08-29T11:38:11.659736Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042558,
     "end_time": "2024-08-29T11:38:11.662369",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.619811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def eval_model(model, val_loader, loss_fn, device):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for data in val_loader:\n",
    "#             input_ids = data['input_ids'].to(device)\n",
    "#             attention_mask = data['attention_mask'].to(device)\n",
    "#             labels = data['labels'].to(device)\n",
    "\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "#     val_accuracy = correct / total\n",
    "#     return val_loss / len(val_loader), val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1dd2618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.734350Z",
     "iopub.status.busy": "2024-08-29T11:38:11.734060Z",
     "iopub.status.idle": "2024-08-29T11:38:11.738824Z",
     "shell.execute_reply": "2024-08-29T11:38:11.738015Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.043024,
     "end_time": "2024-08-29T11:38:11.740724",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.697700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     hidden_size = trial.suggest_int('hidden_size', 64, 256)\n",
    "#     dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [2, 4, 8, 16])\n",
    "#     learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "#     epochs = 5  # Adjust as needed\n",
    "\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "#     # Load your data here\n",
    "# #     data = df  # Replace with your data loading logic\n",
    "# #     train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "# #         data.iloc[:, 0],  # Assuming text is in the first column\n",
    "# #         data.iloc[:, 1:],  # Labels are in the remaining columns\n",
    "# #         test_size=0.2\n",
    "# #     )\n",
    "\n",
    "\n",
    "# #     train_dataset = EmotionDataset(pd.DataFrame({'text': train_texts, **pd.DataFrame(train_labels)}), tokenizer, max_len=128)\n",
    "# #     val_dataset = EmotionDataset(pd.DataFrame({'text': val_texts, **pd.DataFrame(val_labels)}), tokenizer, max_len=128)\n",
    "    \n",
    "    \n",
    "#     train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "#     val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     model = EmotionClassifier(hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
    "#     loss_fn = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "#     scaler = GradScaler()\n",
    "\n",
    "#     train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, epochs)\n",
    "#     val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "    \n",
    "#     return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b41d5d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.815016Z",
     "iopub.status.busy": "2024-08-29T11:38:11.814373Z",
     "iopub.status.idle": "2024-08-29T11:38:11.819903Z",
     "shell.execute_reply": "2024-08-29T11:38:11.819036Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044937,
     "end_time": "2024-08-29T11:38:11.821804",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.776867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_fn(config):\n",
    "#     try:\n",
    "#         hidden_size = config['hidden_size']\n",
    "#         dropout_rate = config['dropout_rate']\n",
    "#         batch_size = config['batch_size']\n",
    "#         learning_rate = config['lr']\n",
    "#         epochs = 5\n",
    "\n",
    "#         tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#         # Load your data here\n",
    "# #         data = pd.read_csv('path_to_your_data.csv')  # Replace with your data loading logic\n",
    "# #         train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "# #             data.iloc[:, 0],  # Assuming text is in the first column\n",
    "# #             data.iloc[:, 1:],  # Labels are in the remaining columns\n",
    "# #             test_size=0.2\n",
    "# #         )\n",
    "\n",
    "#         train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "#         val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         model = EmotionClassifier(hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
    "#         loss_fn = nn.BCEWithLogitsLoss()\n",
    "#         optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#         scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "#         scaler = GradScaler()\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, epochs)\n",
    "#             val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "#             tune.report(loss=val_loss, accuracy=val_accuracy)\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d4e734e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.896358Z",
     "iopub.status.busy": "2024-08-29T11:38:11.896008Z",
     "iopub.status.idle": "2024-08-29T11:38:11.900829Z",
     "shell.execute_reply": "2024-08-29T11:38:11.900060Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044681,
     "end_time": "2024-08-29T11:38:11.902720",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.858039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def tune_model(config):\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#     train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#     train_dataset = EmotionDataset(train_df, tokenizer, config['max_len'])\n",
    "#     val_dataset = EmotionDataset(val_df, tokenizer, config['max_len'])\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = RoBERTaEmotionModel('roberta-base').to(device)\n",
    "\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "#     loss_fn = nn.MSELoss().to(device)\n",
    "#     scaler = GradScaler()\n",
    "\n",
    "#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config['lr'], steps_per_epoch=len(train_loader), epochs=config['epochs'])\n",
    "\n",
    "#     train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, config['epochs'])\n",
    "\n",
    "#     val_loss = eval_model(model, val_loader, loss_fn, device)\n",
    "#     tune.report(val_loss=val_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0badcaa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:11.976580Z",
     "iopub.status.busy": "2024-08-29T11:38:11.976229Z",
     "iopub.status.idle": "2024-08-29T11:38:11.981141Z",
     "shell.execute_reply": "2024-08-29T11:38:11.980284Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044354,
     "end_time": "2024-08-29T11:38:11.983044",
     "exception": false,
     "start_time": "2024-08-29T11:38:11.938690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'max_len': tune.choice([128, 192, 256]),\n",
    "#     'batch_size': tune.choice([8, 16, 32]),\n",
    "#     'lr': tune.loguniform(1e-5, 5e-5),\n",
    "#     'epochs': tune.choice([3, 5, 7])\n",
    "# }\n",
    "\n",
    "# scheduler = ASHAScheduler(\n",
    "#     metric='val_loss',\n",
    "#     mode='min',\n",
    "#     max_t=10,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )\n",
    "\n",
    "# reporter = CLIReporter(\n",
    "#     metric_columns=['val_loss', 'training_iteration']\n",
    "# )\n",
    "\n",
    "# analysis = tune.run(\n",
    "#     tune_model,\n",
    "#     resources_per_trial={'cpu': 2, 'gpu': 1},\n",
    "#     config=config,\n",
    "#     num_samples=20,\n",
    "#     scheduler=scheduler,\n",
    "#     progress_reporter=reporter\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf2f3b2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:12.056742Z",
     "iopub.status.busy": "2024-08-29T11:38:12.056448Z",
     "iopub.status.idle": "2024-08-29T11:38:12.062611Z",
     "shell.execute_reply": "2024-08-29T11:38:12.061764Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.045538,
     "end_time": "2024-08-29T11:38:12.064373",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.018835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_model(config, train_dataset, val_dataset, device):\n",
    "#     model = EmotionModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=7,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "#     model = model.to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "#     model.train()\n",
    "#     for epoch in range(config[\"epochs\"]):\n",
    "#         total_train_loss = 0.0\n",
    "#         correct_train_preds = 0\n",
    "#         total_train_preds = 0\n",
    "        \n",
    "#         for batch in train_loader:\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "        \n",
    "#         avg_train_loss = total_train_loss / len(train_loader)\n",
    "#         train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0.0\n",
    "#         correct_val_preds = 0\n",
    "#         total_val_preds = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 input_ids = batch[\"input_ids\"].to(device)\n",
    "#                 attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#                 labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "\n",
    "#         avg_val_loss = total_val_loss / len(val_loader)\n",
    "#         val_accuracy = correct_val_preds / total_val_preds\n",
    "\n",
    "#         tune.report(loss=avg_val_loss, accuracy=val_accuracy, train_loss=avg_train_loss, train_accuracy=train_accuracy)\n",
    "#         model_save_path = os.path.join(tune.get_trial_dir(), \"checkpoint.pt\")\n",
    "#         torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# def train_fn(config):\n",
    "#     # Load your data here\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#     train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "#     val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     train_model(config, train_dataset, val_dataset, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045bf13b",
   "metadata": {
    "papermill": {
     "duration": 0.038458,
     "end_time": "2024-08-29T11:38:12.138227",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.099769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NEW TRAIN & VALIDATION 2.0\n",
    "### Custom metric is a metric for determining the best model free from any overfitting, underfitting. ```custom_metric = (avg_val_loss−val_accuracy) + α × ∣avg_train_loss−avg_val_loss∣ + β × ∣val_accuracy-train_accuracy∣``` This metric balances between low validation loss, high validation accuracy, and penalizing large discrepancies between training and validation loss. Note that, in the model selection we have used ```α = β = 0.5```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68dccd",
   "metadata": {
    "papermill": {
     "duration": 0.036144,
     "end_time": "2024-08-29T11:38:12.211316",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.175172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### But since, we have probabilistic values as input and output we cannot directly calculate the accuracy, infact we need to rely on loss only. ```custom_metric = avg_val_loss + α × ∣avg_train_loss−avg_val_loss∣```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1da1c91c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:12.284092Z",
     "iopub.status.busy": "2024-08-29T11:38:12.283724Z",
     "iopub.status.idle": "2024-08-29T11:38:12.288676Z",
     "shell.execute_reply": "2024-08-29T11:38:12.287784Z"
    },
    "papermill": {
     "duration": 0.043611,
     "end_time": "2024-08-29T11:38:12.290602",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.246991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom metric calculation function\n",
    "def calculate_custom_metric(avg_val_loss, avg_train_loss, alpha=0.5):\n",
    "    loss_diff = abs(avg_train_loss - avg_val_loss)\n",
    "    custom_metric = avg_val_loss + alpha * loss_diff\n",
    "    return custom_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37df4f39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:12.362960Z",
     "iopub.status.busy": "2024-08-29T11:38:12.362656Z",
     "iopub.status.idle": "2024-08-29T11:38:12.366571Z",
     "shell.execute_reply": "2024-08-29T11:38:12.365808Z"
    },
    "papermill": {
     "duration": 0.042397,
     "end_time": "2024-08-29T11:38:12.368386",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.325989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Custom metric calculation function\n",
    "# def calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy, alpha=0.5, beta=0.5):\n",
    "#     loss_diff = abs(avg_train_loss - avg_val_loss)\n",
    "#     accuracy_diff = abs(train_accuracy - val_accuracy)\n",
    "#     custom_metric = avg_val_loss - val_accuracy + alpha * loss_diff + beta * accuracy_diff\n",
    "#     return custom_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c1aab8",
   "metadata": {
    "papermill": {
     "duration": 0.035965,
     "end_time": "2024-08-29T11:38:12.440013",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.404048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08ef34ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:12.513993Z",
     "iopub.status.busy": "2024-08-29T11:38:12.513632Z",
     "iopub.status.idle": "2024-08-29T11:38:12.521516Z",
     "shell.execute_reply": "2024-08-29T11:38:12.520550Z"
    },
    "papermill": {
     "duration": 0.047648,
     "end_time": "2024-08-29T11:38:12.523600",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.475952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        EarlyStopping to stop training when a metric has stopped improving.\n",
    "\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss improved to {val_loss:.4f}')\n",
    "#             torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        else:\n",
    "            self.counter += 1\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss did not improve. Patience: {self.patience}, Counter: {self.counter}')\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02ff64fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:12.599056Z",
     "iopub.status.busy": "2024-08-29T11:38:12.598700Z",
     "iopub.status.idle": "2024-08-29T11:38:12.621232Z",
     "shell.execute_reply": "2024-08-29T11:38:12.620327Z"
    },
    "papermill": {
     "duration": 0.062577,
     "end_time": "2024-08-29T11:38:12.623178",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.560601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the training function with variable parameters\n",
    "def train_model(config, train_dataset, val_dataset, device):\n",
    "#     model = EmotionModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=7,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "    model = EmotionModel(\n",
    "        roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "        n_classes=7,\n",
    "        dropout_rate=config[\"dropout_rate\"]\n",
    "    )\n",
    "    model = model.to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = GradScaler()  # Initialize GradScaler\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):  # Ensure epochs are passed from config\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            with autocast(device_type=device.type):  # Mixed precision\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "            correct_train_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "            total_train_preds += labels.numel()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val_preds = 0\n",
    "        total_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "                correct_val_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "                total_val_preds += labels.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_preds / total_val_preds\n",
    "#         custom_metric = avg_val_loss - val_accuracy\n",
    "#         custom_metric = calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy)\n",
    "        custom_metric = calculate_custom_metric(avg_val_loss, avg_train_loss)\n",
    "\n",
    "        # Report metrics using ray.train.report\n",
    "        report({\n",
    "            \"loss\": avg_val_loss,\n",
    "            \"accuracy\": val_accuracy,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"custom_metric\": custom_metric,\n",
    "            \"early_stopping_epoch\": epoch + 1,\n",
    "        })\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Check for early stopping\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "\n",
    "        # Save the model checkpoint after each epoch\n",
    "#         trial_dir = os.path.join(os.environ[\"TUNE_TRIAL_DIR\"], \"checkpoint.pt\")\n",
    "#         torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "#         checkpoint_path = f\"/kaggle/working/checkpoint_epoch_{epoch}.pt\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_fn(config):\n",
    "    # Load your data here\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "    train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "    val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(config, train_dataset, val_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83359cc",
   "metadata": {
    "papermill": {
     "duration": 0.036509,
     "end_time": "2024-08-29T11:38:12.695694",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.659185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEARCH SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da95dca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:12.774926Z",
     "iopub.status.busy": "2024-08-29T11:38:12.774562Z",
     "iopub.status.idle": "2024-08-29T11:38:12.781208Z",
     "shell.execute_reply": "2024-08-29T11:38:12.780321Z"
    },
    "papermill": {
     "duration": 0.046449,
     "end_time": "2024-08-29T11:38:12.783150",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.736701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'dropout_rate': tune.choice([0.2, 0.25, 0.27, 0.3, 0.35, 0.4]),\n",
    "    'batch_size': tune.choice([32, 64]),\n",
    "    'weight_decay': tune.choice([1e-6, 2e-6, 1e-5, 2e-5, 1e-4, 2e-4, 5e-5, 5e-6, 1e-2, 1e-3]),\n",
    "    'lr': tune.choice([1e-6, 1e-5, 2e-5, 1e-4, 2e-4, 2e-6, 3e-6, 5e-6, 3e-5, 5e-5, 2e-7, 1e-7, 5e-7, 1e-3]),\n",
    "    'epochs': tune.choice([5, 7, 10, 12, 15, 20])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6655c4e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:12.856784Z",
     "iopub.status.busy": "2024-08-29T11:38:12.856483Z",
     "iopub.status.idle": "2024-08-29T11:38:12.860736Z",
     "shell.execute_reply": "2024-08-29T11:38:12.859621Z"
    },
    "papermill": {
     "duration": 0.043958,
     "end_time": "2024-08-29T11:38:12.863538",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.819580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'hidden_size': tune.choice([32, 64, 128, 256]),\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.choice([3, 5, 7, 10, 12, 15, 20])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ef25389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:12.954645Z",
     "iopub.status.busy": "2024-08-29T11:38:12.953963Z",
     "iopub.status.idle": "2024-08-29T11:38:12.958461Z",
     "shell.execute_reply": "2024-08-29T11:38:12.957546Z"
    },
    "papermill": {
     "duration": 0.051114,
     "end_time": "2024-08-29T11:38:12.960614",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.909500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.choice([3, 5, 7, 10, 12, 15, 20])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1791cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.035016Z",
     "iopub.status.busy": "2024-08-29T11:38:13.034330Z",
     "iopub.status.idle": "2024-08-29T11:38:13.038838Z",
     "shell.execute_reply": "2024-08-29T11:38:13.037870Z"
    },
    "papermill": {
     "duration": 0.04356,
     "end_time": "2024-08-29T11:38:13.040792",
     "exception": false,
     "start_time": "2024-08-29T11:38:12.997232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'hidden_size': tune.choice([32, 64, 128, 256]),\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.randint(3, 21)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0cfef1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.119854Z",
     "iopub.status.busy": "2024-08-29T11:38:13.119502Z",
     "iopub.status.idle": "2024-08-29T11:38:13.123664Z",
     "shell.execute_reply": "2024-08-29T11:38:13.122744Z"
    },
    "papermill": {
     "duration": 0.046975,
     "end_time": "2024-08-29T11:38:13.125779",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.078804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'hidden_size': tune.choice([256, 512]),\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([4, 8, 16]),\n",
    "#     'lr': tune.loguniform(1e-5, 1e-2)\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6f4c4",
   "metadata": {
    "papermill": {
     "duration": 0.037758,
     "end_time": "2024-08-29T11:38:13.202495",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.164737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEARCH ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aaa54d3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.276045Z",
     "iopub.status.busy": "2024-08-29T11:38:13.275312Z",
     "iopub.status.idle": "2024-08-29T11:38:13.279527Z",
     "shell.execute_reply": "2024-08-29T11:38:13.278712Z"
    },
    "papermill": {
     "duration": 0.042812,
     "end_time": "2024-08-29T11:38:13.281337",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.238525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optuna_search = OptunaSearch(\n",
    "    metric=\"accuracy\",\n",
    "    mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df858ed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.355182Z",
     "iopub.status.busy": "2024-08-29T11:38:13.354143Z",
     "iopub.status.idle": "2024-08-29T11:38:13.358852Z",
     "shell.execute_reply": "2024-08-29T11:38:13.358072Z"
    },
    "papermill": {
     "duration": 0.043543,
     "end_time": "2024-08-29T11:38:13.360702",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.317159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optuna_search = OptunaSearch(\n",
    "#     metric=[\n",
    "#         tune.MultiObjective(\"loss\", \"min\"),\n",
    "#         tune.MultiObjective(\"custom_metric\", \"min\"),\n",
    "#         tune.MultiObjective(\"accuracy\", \"max\")\n",
    "#     ],\n",
    "#     mode=[\n",
    "#         \"min\",  # for loss\n",
    "#         \"min\",  # for custom metric\n",
    "#         \"max\"   # for accuracy\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2de8fa2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.433169Z",
     "iopub.status.busy": "2024-08-29T11:38:13.432869Z",
     "iopub.status.idle": "2024-08-29T11:38:13.436889Z",
     "shell.execute_reply": "2024-08-29T11:38:13.435934Z"
    },
    "papermill": {
     "duration": 0.04255,
     "end_time": "2024-08-29T11:38:13.438930",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.396380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optuna_search = OptunaSearch(\n",
    "#     metric=\"custom_metric\",\n",
    "#     mode=\"min\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d95dea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.512773Z",
     "iopub.status.busy": "2024-08-29T11:38:13.512428Z",
     "iopub.status.idle": "2024-08-29T11:38:13.516125Z",
     "shell.execute_reply": "2024-08-29T11:38:13.515309Z"
    },
    "papermill": {
     "duration": 0.04292,
     "end_time": "2024-08-29T11:38:13.518096",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.475176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Setup Optuna for hyperparameter optimization\n",
    "# optuna_search = OptunaSearch(\n",
    "#     metric=\"loss\",\n",
    "#     mode=\"min\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9997de19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.592966Z",
     "iopub.status.busy": "2024-08-29T11:38:13.592677Z",
     "iopub.status.idle": "2024-08-29T11:38:13.596365Z",
     "shell.execute_reply": "2024-08-29T11:38:13.595557Z"
    },
    "papermill": {
     "duration": 0.043295,
     "end_time": "2024-08-29T11:38:13.598214",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.554919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"loss\",\n",
    "#     mode=\"min\",\n",
    "#     max_t=15,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9425768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.671856Z",
     "iopub.status.busy": "2024-08-29T11:38:13.671566Z",
     "iopub.status.idle": "2024-08-29T11:38:13.675415Z",
     "shell.execute_reply": "2024-08-29T11:38:13.674538Z"
    },
    "papermill": {
     "duration": 0.042882,
     "end_time": "2024-08-29T11:38:13.677254",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.634372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"accuracy\",\n",
    "#     mode=\"max\",\n",
    "#     max_t=15,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33061c23",
   "metadata": {
    "papermill": {
     "duration": 0.035768,
     "end_time": "2024-08-29T11:38:13.748785",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.713017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SCHEDULER ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d054b110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.821320Z",
     "iopub.status.busy": "2024-08-29T11:38:13.821002Z",
     "iopub.status.idle": "2024-08-29T11:38:13.825407Z",
     "shell.execute_reply": "2024-08-29T11:38:13.824604Z"
    },
    "papermill": {
     "duration": 0.042787,
     "end_time": "2024-08-29T11:38:13.827347",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.784560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    metric='accuracy',\n",
    "    mode='max',\n",
    "    max_t=22,\n",
    "    grace_period=3,\n",
    "    reduction_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e7c01dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.904676Z",
     "iopub.status.busy": "2024-08-29T11:38:13.904341Z",
     "iopub.status.idle": "2024-08-29T11:38:13.908204Z",
     "shell.execute_reply": "2024-08-29T11:38:13.907432Z"
    },
    "papermill": {
     "duration": 0.045383,
     "end_time": "2024-08-29T11:38:13.910148",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.864765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = AsyncHyperBandScheduler(\n",
    "#     metric='accuracy',\n",
    "#     mode='max',\n",
    "#     max_t=22,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9651cd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:13.984221Z",
     "iopub.status.busy": "2024-08-29T11:38:13.983930Z",
     "iopub.status.idle": "2024-08-29T11:38:13.987927Z",
     "shell.execute_reply": "2024-08-29T11:38:13.987181Z"
    },
    "papermill": {
     "duration": 0.04231,
     "end_time": "2024-08-29T11:38:13.989782",
     "exception": false,
     "start_time": "2024-08-29T11:38:13.947472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=[\n",
    "#         tune.MultiObjective(\"loss\", \"min\"),\n",
    "#         tune.MultiObjective(\"custom_metric\", \"min\"),\n",
    "#         tune.MultiObjective(\"accuracy\", \"max\")\n",
    "#     ],\n",
    "#     mode=[\n",
    "#         \"min\",  # for loss\n",
    "#         \"min\",  # for custom metric\n",
    "#         \"max\"   # for accuracy\n",
    "#     ],\n",
    "#     max_t=22,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "309e0288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:14.062440Z",
     "iopub.status.busy": "2024-08-29T11:38:14.062133Z",
     "iopub.status.idle": "2024-08-29T11:38:14.065737Z",
     "shell.execute_reply": "2024-08-29T11:38:14.064943Z"
    },
    "papermill": {
     "duration": 0.042356,
     "end_time": "2024-08-29T11:38:14.067525",
     "exception": false,
     "start_time": "2024-08-29T11:38:14.025169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"custom_metric\",\n",
    "#     mode=\"min\",\n",
    "#     max_t=22,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98317b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:14.142571Z",
     "iopub.status.busy": "2024-08-29T11:38:14.142191Z",
     "iopub.status.idle": "2024-08-29T11:38:14.146634Z",
     "shell.execute_reply": "2024-08-29T11:38:14.145713Z"
    },
    "papermill": {
     "duration": 0.045762,
     "end_time": "2024-08-29T11:38:14.148530",
     "exception": false,
     "start_time": "2024-08-29T11:38:14.102768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = HyperBandScheduler(\n",
    "#     time_attr='training_iteration',  # The attribute to use for the time dimension (similar to ASHA's `max_t`)\n",
    "#     metric='custom_metric',          # The metric to optimize (similar to ASHA's `metric`)\n",
    "#     mode='min',                      # Optimization mode (minimize `custom_metric`)\n",
    "#     max_t=22,                        # Maximum number of iterations per trial (similar to ASHA's `max_t`)\n",
    "#     reduction_factor=2               # Reduction factor (similar to ASHA)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "674994f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:14.225670Z",
     "iopub.status.busy": "2024-08-29T11:38:14.224895Z",
     "iopub.status.idle": "2024-08-29T11:38:14.229431Z",
     "shell.execute_reply": "2024-08-29T11:38:14.228566Z"
    },
    "papermill": {
     "duration": 0.045089,
     "end_time": "2024-08-29T11:38:14.231387",
     "exception": false,
     "start_time": "2024-08-29T11:38:14.186298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "    print_intermediate_tables=False\n",
    ")\n",
    "reporter.verbosity = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9052b97",
   "metadata": {
    "papermill": {
     "duration": 0.035318,
     "end_time": "2024-08-29T11:38:14.302159",
     "exception": false,
     "start_time": "2024-08-29T11:38:14.266841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom CLI Reporter Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20cd41e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:14.375436Z",
     "iopub.status.busy": "2024-08-29T11:38:14.375105Z",
     "iopub.status.idle": "2024-08-29T11:38:14.379756Z",
     "shell.execute_reply": "2024-08-29T11:38:14.378904Z"
    },
    "papermill": {
     "duration": 0.043587,
     "end_time": "2024-08-29T11:38:14.381589",
     "exception": false,
     "start_time": "2024-08-29T11:38:14.338002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FinalTableCLIReporter(CLIReporter):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.results = []\n",
    "\n",
    "#     def _update(self, *args, **kwargs):\n",
    "#         # Collect results without printing intermediate updates\n",
    "#         self.results.append(kwargs)\n",
    "\n",
    "#     def print_table(self):\n",
    "#         # Print only the final results\n",
    "#         if self.results:\n",
    "#             final_results = [result for result in self.results]\n",
    "#             headers = list(final_results[0].keys()) if final_results else []\n",
    "#             table = [headers] + [list(result.values()) for result in final_results]\n",
    "#             for row in table:\n",
    "#                 print(\" | \".join(str(cell) for cell in row))\n",
    "\n",
    "#     def _report(self, *args, **kwargs):\n",
    "#         # Override this to prevent intermediate print\n",
    "#         self._update(*args, **kwargs)\n",
    "\n",
    "#     def _finalize(self):\n",
    "#         # Call this to print the final table\n",
    "#         self.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088182de",
   "metadata": {
    "papermill": {
     "duration": 0.035428,
     "end_time": "2024-08-29T11:38:14.452642",
     "exception": false,
     "start_time": "2024-08-29T11:38:14.417214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## REPORTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0294ca29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:14.525242Z",
     "iopub.status.busy": "2024-08-29T11:38:14.524968Z",
     "iopub.status.idle": "2024-08-29T11:38:14.529011Z",
     "shell.execute_reply": "2024-08-29T11:38:14.528224Z"
    },
    "papermill": {
     "duration": 0.042631,
     "end_time": "2024-08-29T11:38:14.530859",
     "exception": false,
     "start_time": "2024-08-29T11:38:14.488228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To ignore warinings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b32a2bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T11:38:14.604716Z",
     "iopub.status.busy": "2024-08-29T11:38:14.604012Z",
     "iopub.status.idle": "2024-08-29T16:46:57.979081Z",
     "shell.execute_reply": "2024-08-29T16:46:57.977900Z"
    },
    "papermill": {
     "duration": 18523.415093,
     "end_time": "2024-08-29T16:46:57.981760",
     "exception": false,
     "start_time": "2024-08-29T11:38:14.566667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-08-29 16:46:57</td></tr>\n",
       "<tr><td>Running for: </td><td>05:08:26.68        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.2/31.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=45<br>Bracket: Iter 12.000: 0.9210361067503925 | Iter 6.000: 0.9227629513343799 | Iter 3.000: 0.9056514913657772<br>Logical resource usage: 2.0/4 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  custom_metric</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">  early_stopping_epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_ae5bca6c</td><td>TERMINATED</td><td>172.19.2.2:339 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0338125</td><td style=\"text-align: right;\">  0.917425</td><td style=\"text-align: right;\">      0.0367419</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0279537 </td><td style=\"text-align: right;\">        0.931728</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_063ecdc5</td><td>TERMINATED</td><td>172.19.2.2:374 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.191018 </td><td style=\"text-align: right;\">  0.759184</td><td style=\"text-align: right;\">      0.19278  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.187493  </td><td style=\"text-align: right;\">        0.626376</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_10d36af6</td><td>TERMINATED</td><td>172.19.2.2:480 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0396148</td><td style=\"text-align: right;\">  0.911774</td><td style=\"text-align: right;\">      0.0428984</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">  0.0330475 </td><td style=\"text-align: right;\">        0.924812</td><td style=\"text-align: right;\">                    15</td></tr>\n",
       "<tr><td>train_fn_225c26d2</td><td>TERMINATED</td><td>172.19.2.2:562 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0340854</td><td style=\"text-align: right;\">  0.915856</td><td style=\"text-align: right;\">      0.0386347</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">  0.0249867 </td><td style=\"text-align: right;\">        0.937121</td><td style=\"text-align: right;\">                    15</td></tr>\n",
       "<tr><td>train_fn_6406f961</td><td>TERMINATED</td><td>172.19.2.2:694 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0337511</td><td style=\"text-align: right;\">  0.919937</td><td style=\"text-align: right;\">      0.0386409</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0239714 </td><td style=\"text-align: right;\">        0.937026</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_e75aaa28</td><td>TERMINATED</td><td>172.19.2.2:785 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0573424</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0590471</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.053933  </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_ab4e8af9</td><td>TERMINATED</td><td>172.19.2.2:863 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0399348</td><td style=\"text-align: right;\">  0.908948</td><td style=\"text-align: right;\">      0.0408165</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0381714 </td><td style=\"text-align: right;\">        0.916722</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_b4b39181</td><td>TERMINATED</td><td>172.19.2.2:955 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.184515 </td><td style=\"text-align: right;\">  0.696389</td><td style=\"text-align: right;\">      0.184679 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.184843  </td><td style=\"text-align: right;\">        0.575553</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_0a82d48b</td><td>TERMINATED</td><td>172.19.2.2:1041</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.18828  </td><td style=\"text-align: right;\">  0.704867</td><td style=\"text-align: right;\">      0.189134 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.186572  </td><td style=\"text-align: right;\">        0.595095</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_79b0f963</td><td>TERMINATED</td><td>172.19.2.2:1129</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.181916 </td><td style=\"text-align: right;\">  0.764521</td><td style=\"text-align: right;\">      0.182449 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.18085   </td><td style=\"text-align: right;\">        0.681863</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_ee36f293</td><td>TERMINATED</td><td>172.19.2.2:1201</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0577699</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0579388</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0574321 </td><td style=\"text-align: right;\">        0.889756</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_f768ed09</td><td>TERMINATED</td><td>172.19.2.2:1286</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0576401</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0595379</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0538445 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_2ac5758f</td><td>TERMINATED</td><td>172.19.2.2:1372</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0262221</td><td style=\"text-align: right;\">  0.929042</td><td style=\"text-align: right;\">      0.0324192</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.013828  </td><td style=\"text-align: right;\">        0.961264</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_80b190ce</td><td>TERMINATED</td><td>172.19.2.2:1456</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0271654</td><td style=\"text-align: right;\">  0.925275</td><td style=\"text-align: right;\">      0.0364824</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00853131</td><td style=\"text-align: right;\">        0.972082</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_99de567e</td><td>TERMINATED</td><td>172.19.2.2:1571</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0294117</td><td style=\"text-align: right;\">  0.922763</td><td style=\"text-align: right;\">      0.0372908</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0136537 </td><td style=\"text-align: right;\">        0.962533</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_48daa01a</td><td>TERMINATED</td><td>172.19.2.2:1663</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0317628</td><td style=\"text-align: right;\">  0.920879</td><td style=\"text-align: right;\">      0.0398855</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0155175 </td><td style=\"text-align: right;\">        0.956473</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_59d0338e</td><td>TERMINATED</td><td>172.19.2.2:1779</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0308876</td><td style=\"text-align: right;\">  0.921821</td><td style=\"text-align: right;\">      0.0398593</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0129441 </td><td style=\"text-align: right;\">        0.963643</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_bedea489</td><td>TERMINATED</td><td>172.19.2.2:1863</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0383358</td><td style=\"text-align: right;\">  0.917111</td><td style=\"text-align: right;\">      0.044753 </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0255015 </td><td style=\"text-align: right;\">        0.936709</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_fb0c861f</td><td>TERMINATED</td><td>172.19.2.2:1963</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0282649</td><td style=\"text-align: right;\">  0.921193</td><td style=\"text-align: right;\">      0.0353133</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0141679 </td><td style=\"text-align: right;\">        0.96082 </td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_a122d433</td><td>TERMINATED</td><td>172.19.2.2:2049</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.12888  </td><td style=\"text-align: right;\">  0.884772</td><td style=\"text-align: right;\">      0.136526 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.144172  </td><td style=\"text-align: right;\">        0.837886</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_a81e9f86</td><td>TERMINATED</td><td>172.19.2.2:2135</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.055086 </td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0554138</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0544305 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_da8ede27</td><td>TERMINATED</td><td>172.19.2.2:2234</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0391503</td><td style=\"text-align: right;\">  0.918367</td><td style=\"text-align: right;\">      0.0534259</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0105991 </td><td style=\"text-align: right;\">        0.9691  </td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_1e0b22ca</td><td>TERMINATED</td><td>172.19.2.2:2306</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0307493</td><td style=\"text-align: right;\">  0.924647</td><td style=\"text-align: right;\">      0.0370106</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0182268 </td><td style=\"text-align: right;\">        0.952635</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_9640b55c</td><td>TERMINATED</td><td>172.19.2.2:2412</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0575477</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0593397</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0539636 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_5e5e8f79</td><td>TERMINATED</td><td>172.19.2.2:2505</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0300904</td><td style=\"text-align: right;\">  0.924647</td><td style=\"text-align: right;\">      0.0350102</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0202509 </td><td style=\"text-align: right;\">        0.945782</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_f6b38bd4</td><td>TERMINATED</td><td>172.19.2.2:2580</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0359795</td><td style=\"text-align: right;\">  0.921821</td><td style=\"text-align: right;\">      0.0432314</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0214756 </td><td style=\"text-align: right;\">        0.94445 </td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_96e60111</td><td>TERMINATED</td><td>172.19.2.2:2681</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0310676</td><td style=\"text-align: right;\">  0.920251</td><td style=\"text-align: right;\">      0.0379593</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0172841 </td><td style=\"text-align: right;\">        0.95384 </td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_d6e284d4</td><td>TERMINATED</td><td>172.19.2.2:2756</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0387797</td><td style=\"text-align: right;\">  0.913972</td><td style=\"text-align: right;\">      0.0481829</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0199733 </td><td style=\"text-align: right;\">        0.947019</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_bb71d314</td><td>TERMINATED</td><td>172.19.2.2:2857</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0349156</td><td style=\"text-align: right;\">  0.926845</td><td style=\"text-align: right;\">      0.042551 </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0196447 </td><td style=\"text-align: right;\">        0.946893</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_b5fa2bcb</td><td>TERMINATED</td><td>172.19.2.2:2932</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.18439  </td><td style=\"text-align: right;\">  0.816327</td><td style=\"text-align: right;\">      0.185325 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.186261  </td><td style=\"text-align: right;\">        0.570604</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_be78d7b2</td><td>TERMINATED</td><td>172.19.2.2:3019</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.057416 </td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0593937</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0534607 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_b0ed2ac1</td><td>TERMINATED</td><td>172.19.2.2:3104</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0401062</td><td style=\"text-align: right;\">  0.909576</td><td style=\"text-align: right;\">      0.0422111</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.044316  </td><td style=\"text-align: right;\">        0.898036</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_0133f70c</td><td>TERMINATED</td><td>172.19.2.2:3177</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0654631</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0692963</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0731296 </td><td style=\"text-align: right;\">        0.889629</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_662bf4ff</td><td>TERMINATED</td><td>172.19.2.2:3281</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0300192</td><td style=\"text-align: right;\">  0.923705</td><td style=\"text-align: right;\">      0.0360422</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0179732 </td><td style=\"text-align: right;\">        0.953015</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_93adb0dd</td><td>TERMINATED</td><td>172.19.2.2:3353</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0282159</td><td style=\"text-align: right;\">  0.924647</td><td style=\"text-align: right;\">      0.0331385</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0183707 </td><td style=\"text-align: right;\">        0.94978 </td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_9e277cab</td><td>TERMINATED</td><td>172.19.2.2:3458</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0329546</td><td style=\"text-align: right;\">  0.921821</td><td style=\"text-align: right;\">      0.0424837</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0138963 </td><td style=\"text-align: right;\">        0.961962</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_1794a632</td><td>TERMINATED</td><td>172.19.2.2:3529</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.074378 </td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0815375</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0886969 </td><td style=\"text-align: right;\">        0.889217</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_51954b81</td><td>TERMINATED</td><td>172.19.2.2:3616</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0624654</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.063904 </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0653427 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_37b68ddc</td><td>TERMINATED</td><td>172.19.2.2:3716</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0606492</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0639497</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">  0.0540483 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     4</td></tr>\n",
       "<tr><td>train_fn_5af5d4ea</td><td>TERMINATED</td><td>172.19.2.2:3789</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0290251</td><td style=\"text-align: right;\">  0.925903</td><td style=\"text-align: right;\">      0.0375587</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.011958  </td><td style=\"text-align: right;\">        0.966752</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_bd106eb8</td><td>TERMINATED</td><td>172.19.2.2:3878</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0300595</td><td style=\"text-align: right;\">  0.920565</td><td style=\"text-align: right;\">      0.0406162</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00894599</td><td style=\"text-align: right;\">        0.97075 </td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_552615d0</td><td>TERMINATED</td><td>172.19.2.2:3987</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.189879 </td><td style=\"text-align: right;\">  0.63956 </td><td style=\"text-align: right;\">      0.191027 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.187582  </td><td style=\"text-align: right;\">        0.527775</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_1b4fba34</td><td>TERMINATED</td><td>172.19.2.2:4076</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0287956</td><td style=\"text-align: right;\">  0.924961</td><td style=\"text-align: right;\">      0.0372534</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.01188   </td><td style=\"text-align: right;\">        0.965356</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_edbc9818</td><td>TERMINATED</td><td>172.19.2.2:4157</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0290477</td><td style=\"text-align: right;\">  0.9281  </td><td style=\"text-align: right;\">      0.0374842</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0121745 </td><td style=\"text-align: right;\">        0.965864</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_df4621cf</td><td>TERMINATED</td><td>172.19.2.2:4272</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.028335 </td><td style=\"text-align: right;\">  0.921507</td><td style=\"text-align: right;\">      0.0360145</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0129761 </td><td style=\"text-align: right;\">        0.962374</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_fccbaed7</td><td>TERMINATED</td><td>172.19.2.2:4353</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0298937</td><td style=\"text-align: right;\">  0.925589</td><td style=\"text-align: right;\">      0.0380234</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0136341 </td><td style=\"text-align: right;\">        0.963358</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_1bb08f6c</td><td>TERMINATED</td><td>172.19.2.2:4469</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.027636 </td><td style=\"text-align: right;\">  0.926531</td><td style=\"text-align: right;\">      0.0353081</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0122919 </td><td style=\"text-align: right;\">        0.963548</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_d8602e9a</td><td>TERMINATED</td><td>172.19.2.2:4551</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0575201</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0592307</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0540989 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_409726d9</td><td>TERMINATED</td><td>172.19.2.2:4640</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.184906 </td><td style=\"text-align: right;\">  0.753846</td><td style=\"text-align: right;\">      0.18699  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.180736  </td><td style=\"text-align: right;\">        0.634656</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_2b8aa2a4</td><td>TERMINATED</td><td>172.19.2.2:4729</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.178844 </td><td style=\"text-align: right;\">  0.724647</td><td style=\"text-align: right;\">      0.179092 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.179339  </td><td style=\"text-align: right;\">        0.616034</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_a72c9295</td><td>TERMINATED</td><td>172.19.2.2:4810</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0594936</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0622851</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0650766 </td><td style=\"text-align: right;\">        0.889597</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_a3542e26</td><td>TERMINATED</td><td>172.19.2.2:4888</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0459207</td><td style=\"text-align: right;\">  0.897645</td><td style=\"text-align: right;\">      0.0474478</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.048975  </td><td style=\"text-align: right;\">        0.892231</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_70a3a08b</td><td>TERMINATED</td><td>172.19.2.2:4970</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0502012</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0513468</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0524924 </td><td style=\"text-align: right;\">        0.889788</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_36eee68c</td><td>TERMINATED</td><td>172.19.2.2:5063</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0263622</td><td style=\"text-align: right;\">  0.926217</td><td style=\"text-align: right;\">      0.0338867</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0113132 </td><td style=\"text-align: right;\">        0.968053</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_52caf3af</td><td>TERMINATED</td><td>172.19.2.2:5145</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.03035  </td><td style=\"text-align: right;\">  0.917425</td><td style=\"text-align: right;\">      0.0397139</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0116223 </td><td style=\"text-align: right;\">        0.966625</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_ac3ad2d5</td><td>TERMINATED</td><td>172.19.2.2:5261</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0288391</td><td style=\"text-align: right;\">  0.925903</td><td style=\"text-align: right;\">      0.0376204</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0112765 </td><td style=\"text-align: right;\">        0.967799</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_5989e21b</td><td>TERMINATED</td><td>172.19.2.2:5343</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0303167</td><td style=\"text-align: right;\">  0.920251</td><td style=\"text-align: right;\">      0.0339713</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0230074 </td><td style=\"text-align: right;\">        0.939754</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_9dbbd248</td><td>TERMINATED</td><td>172.19.2.2:5442</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0341819</td><td style=\"text-align: right;\">  0.918995</td><td style=\"text-align: right;\">      0.0384761</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0255933 </td><td style=\"text-align: right;\">        0.933663</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_11986bb6</td><td>TERMINATED</td><td>172.19.2.2:5528</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0290798</td><td style=\"text-align: right;\">  0.924647</td><td style=\"text-align: right;\">      0.0370559</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0131277 </td><td style=\"text-align: right;\">        0.963009</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_a14b4564</td><td>TERMINATED</td><td>172.19.2.2:5617</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0330339</td><td style=\"text-align: right;\">  0.920879</td><td style=\"text-align: right;\">      0.0357998</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.027502  </td><td style=\"text-align: right;\">        0.931855</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_5a0f52ae</td><td>TERMINATED</td><td>172.19.2.2:5721</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0324903</td><td style=\"text-align: right;\">  0.919623</td><td style=\"text-align: right;\">      0.0380626</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0213456 </td><td style=\"text-align: right;\">        0.941848</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_c9324bbd</td><td>TERMINATED</td><td>172.19.2.2:5795</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0355723</td><td style=\"text-align: right;\">  0.917425</td><td style=\"text-align: right;\">      0.0417266</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0232637 </td><td style=\"text-align: right;\">        0.940643</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_0bfb89ae</td><td>TERMINATED</td><td>172.19.2.2:5891</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0323876</td><td style=\"text-align: right;\">  0.924961</td><td style=\"text-align: right;\">      0.0373502</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0224625 </td><td style=\"text-align: right;\">        0.941499</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_5aa55c70</td><td>TERMINATED</td><td>172.19.2.2:5971</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0278667</td><td style=\"text-align: right;\">  0.922763</td><td style=\"text-align: right;\">      0.0363194</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0109611 </td><td style=\"text-align: right;\">        0.96726 </td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_514e9434</td><td>TERMINATED</td><td>172.19.2.2:6061</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0272954</td><td style=\"text-align: right;\">  0.923391</td><td style=\"text-align: right;\">      0.0354778</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0109306 </td><td style=\"text-align: right;\">        0.968021</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_02b89f07</td><td>TERMINATED</td><td>172.19.2.2:6168</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0256589</td><td style=\"text-align: right;\">  0.92967 </td><td style=\"text-align: right;\">      0.0333164</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0103437 </td><td style=\"text-align: right;\">        0.968941</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_958e552f</td><td>TERMINATED</td><td>172.19.2.2:6258</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0325279</td><td style=\"text-align: right;\">  0.919623</td><td style=\"text-align: right;\">      0.0431581</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0112675 </td><td style=\"text-align: right;\">        0.966181</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_22214d57</td><td>TERMINATED</td><td>172.19.2.2:6365</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0377084</td><td style=\"text-align: right;\">  0.918681</td><td style=\"text-align: right;\">      0.0437552</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0256147 </td><td style=\"text-align: right;\">        0.934044</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_a0f06c83</td><td>TERMINATED</td><td>172.19.2.2:6455</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0572508</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0587131</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0543262 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_4c9eb432</td><td>TERMINATED</td><td>172.19.2.2:6540</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.192203 </td><td style=\"text-align: right;\">  0.566091</td><td style=\"text-align: right;\">      0.193182 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.190247  </td><td style=\"text-align: right;\">        0.50487 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_e2f73372</td><td>TERMINATED</td><td>172.19.2.2:6613</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.16888  </td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.169074 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.168492  </td><td style=\"text-align: right;\">        0.781638</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_64351fb5</td><td>TERMINATED</td><td>172.19.2.2:6699</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.193834 </td><td style=\"text-align: right;\">  0.496389</td><td style=\"text-align: right;\">      0.195264 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.190975  </td><td style=\"text-align: right;\">        0.507027</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_4abbd44e</td><td>TERMINATED</td><td>172.19.2.2:6770</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.168629 </td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.169166 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.169703  </td><td style=\"text-align: right;\">        0.802988</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_49ded67f</td><td>TERMINATED</td><td>172.19.2.2:6856</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0277195</td><td style=\"text-align: right;\">  0.928414</td><td style=\"text-align: right;\">      0.0355543</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0120499 </td><td style=\"text-align: right;\">        0.966181</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_3f9e4f5f</td><td>TERMINATED</td><td>172.19.2.2:6927</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0274514</td><td style=\"text-align: right;\">  0.926531</td><td style=\"text-align: right;\">      0.0354769</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0114003 </td><td style=\"text-align: right;\">        0.967133</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_f5488994</td><td>TERMINATED</td><td>172.19.2.2:7053</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0313137</td><td style=\"text-align: right;\">  0.921507</td><td style=\"text-align: right;\">      0.0363026</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0213358 </td><td style=\"text-align: right;\">        0.945148</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_d4ad7c65</td><td>TERMINATED</td><td>172.19.2.2:7124</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0286139</td><td style=\"text-align: right;\">  0.925589</td><td style=\"text-align: right;\">      0.0364914</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">  0.0128588 </td><td style=\"text-align: right;\">        0.963072</td><td style=\"text-align: right;\">                     9</td></tr>\n",
       "<tr><td>train_fn_a90b23db</td><td>TERMINATED</td><td>172.19.2.2:7228</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0814802</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0899964</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0985125 </td><td style=\"text-align: right;\">        0.875289</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_b4b9c4e9</td><td>TERMINATED</td><td>172.19.2.2:7316</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0985974</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.109556 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.120514  </td><td style=\"text-align: right;\">        0.876939</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_94acbbb5</td><td>TERMINATED</td><td>172.19.2.2:7353</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0560191</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.056127 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0558034 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_ce563672</td><td>TERMINATED</td><td>172.19.2.2:7447</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0572238</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.058868 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0539356 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_650fbd0e</td><td>TERMINATED</td><td>172.19.2.2:7512</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0589811</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0614457</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0540519 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_2965a8be</td><td>TERMINATED</td><td>172.19.2.2:7605</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0585873</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0608519</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.054058  </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_4d25da4b</td><td>TERMINATED</td><td>172.19.2.2:7677</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0277793</td><td style=\"text-align: right;\">  0.922135</td><td style=\"text-align: right;\">      0.0367394</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.00985907</td><td style=\"text-align: right;\">        0.97002 </td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_d2e65972</td><td>TERMINATED</td><td>172.19.2.2:7763</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0275532</td><td style=\"text-align: right;\">  0.927159</td><td style=\"text-align: right;\">      0.0363165</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0100265 </td><td style=\"text-align: right;\">        0.967958</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_e34c82e7</td><td>TERMINATED</td><td>172.19.2.2:7875</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.027498 </td><td style=\"text-align: right;\">  0.924019</td><td style=\"text-align: right;\">      0.0363402</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.00981354</td><td style=\"text-align: right;\">        0.968846</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_def7ede9</td><td>TERMINATED</td><td>172.19.2.2:7960</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0271228</td><td style=\"text-align: right;\">  0.928728</td><td style=\"text-align: right;\">      0.0348823</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0116037 </td><td style=\"text-align: right;\">        0.966594</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_4a9bc3f6</td><td>TERMINATED</td><td>172.19.2.2:8072</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0296404</td><td style=\"text-align: right;\">  0.923391</td><td style=\"text-align: right;\">      0.0388208</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0112794 </td><td style=\"text-align: right;\">        0.967038</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_5901cc90</td><td>TERMINATED</td><td>172.19.2.2:8157</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0277938</td><td style=\"text-align: right;\">  0.926217</td><td style=\"text-align: right;\">      0.0365815</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0102182 </td><td style=\"text-align: right;\">        0.968307</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_c0520f72</td><td>TERMINATED</td><td>172.19.2.2:8269</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0277927</td><td style=\"text-align: right;\">  0.928728</td><td style=\"text-align: right;\">      0.0361217</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0111347 </td><td style=\"text-align: right;\">        0.967165</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_b081ffc3</td><td>TERMINATED</td><td>172.19.2.2:8354</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0278383</td><td style=\"text-align: right;\">  0.9281  </td><td style=\"text-align: right;\">      0.0338247</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0158656 </td><td style=\"text-align: right;\">        0.956442</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_81085d2a</td><td>TERMINATED</td><td>172.19.2.2:8465</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0828381</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0901217</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0974053 </td><td style=\"text-align: right;\">        0.889217</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_35cd3e9f</td><td>TERMINATED</td><td>172.19.2.2:8537</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0841907</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0927685</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.101346  </td><td style=\"text-align: right;\">        0.885854</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_a9d4d225</td><td>TERMINATED</td><td>172.19.2.2:8622</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0631352</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0672489</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0713626 </td><td style=\"text-align: right;\">        0.889756</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_36fe7adb</td><td>TERMINATED</td><td>172.19.2.2:8694</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0292576</td><td style=\"text-align: right;\">  0.927473</td><td style=\"text-align: right;\">      0.0361782</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0154165 </td><td style=\"text-align: right;\">        0.958789</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_385deca0</td><td>TERMINATED</td><td>172.19.2.2:8781</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.028926 </td><td style=\"text-align: right;\">  0.925589</td><td style=\"text-align: right;\">      0.0358357</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0151064 </td><td style=\"text-align: right;\">        0.957838</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_0bdeb84a</td><td>TERMINATED</td><td>172.19.2.2:8876</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0283191</td><td style=\"text-align: right;\">  0.921193</td><td style=\"text-align: right;\">      0.0349802</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0149968 </td><td style=\"text-align: right;\">        0.958662</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_675943dd</td><td>TERMINATED</td><td>172.19.2.2:8962</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0334637</td><td style=\"text-align: right;\">  0.919623</td><td style=\"text-align: right;\">      0.0399243</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0205425 </td><td style=\"text-align: right;\">        0.946988</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_188d2aa5</td><td>TERMINATED</td><td>172.19.2.2:9056</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0331513</td><td style=\"text-align: right;\">  0.920565</td><td style=\"text-align: right;\">      0.042379 </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0146958 </td><td style=\"text-align: right;\">        0.958472</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_6bac8651</td><td>TERMINATED</td><td>172.19.2.2:9137</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0295607</td><td style=\"text-align: right;\">  0.918367</td><td style=\"text-align: right;\">      0.0364413</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0157994 </td><td style=\"text-align: right;\">        0.955458</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 11:38:18,969\tINFO worker.py:1753 -- Started a local Ray instance.\n",
      "2024-08-29 11:38:20,202\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-08-29 11:38:20,209\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "[I 2024-08-29 11:38:20,272] A new study created in memory with name: optuna\n",
      "\u001b[36m(train_fn pid=339)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=339)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=374)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=374)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  custom_metric</th><th style=\"text-align: right;\">  early_stopping_epoch</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_0133f70c</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0692963</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0654631</td><td style=\"text-align: right;\">        0.889629</td><td style=\"text-align: right;\">  0.0731296 </td></tr>\n",
       "<tr><td>train_fn_02b89f07</td><td style=\"text-align: right;\">  0.92967 </td><td style=\"text-align: right;\">      0.0333164</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0256589</td><td style=\"text-align: right;\">        0.968941</td><td style=\"text-align: right;\">  0.0103437 </td></tr>\n",
       "<tr><td>train_fn_063ecdc5</td><td style=\"text-align: right;\">  0.759184</td><td style=\"text-align: right;\">      0.19278  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.191018 </td><td style=\"text-align: right;\">        0.626376</td><td style=\"text-align: right;\">  0.187493  </td></tr>\n",
       "<tr><td>train_fn_0a82d48b</td><td style=\"text-align: right;\">  0.704867</td><td style=\"text-align: right;\">      0.189134 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.18828  </td><td style=\"text-align: right;\">        0.595095</td><td style=\"text-align: right;\">  0.186572  </td></tr>\n",
       "<tr><td>train_fn_0bdeb84a</td><td style=\"text-align: right;\">  0.921193</td><td style=\"text-align: right;\">      0.0349802</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0283191</td><td style=\"text-align: right;\">        0.958662</td><td style=\"text-align: right;\">  0.0149968 </td></tr>\n",
       "<tr><td>train_fn_0bfb89ae</td><td style=\"text-align: right;\">  0.924961</td><td style=\"text-align: right;\">      0.0373502</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0323876</td><td style=\"text-align: right;\">        0.941499</td><td style=\"text-align: right;\">  0.0224625 </td></tr>\n",
       "<tr><td>train_fn_10d36af6</td><td style=\"text-align: right;\">  0.911774</td><td style=\"text-align: right;\">      0.0428984</td><td style=\"text-align: right;\">                    15</td><td style=\"text-align: right;\">0.0396148</td><td style=\"text-align: right;\">        0.924812</td><td style=\"text-align: right;\">  0.0330475 </td></tr>\n",
       "<tr><td>train_fn_11986bb6</td><td style=\"text-align: right;\">  0.924647</td><td style=\"text-align: right;\">      0.0370559</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0290798</td><td style=\"text-align: right;\">        0.963009</td><td style=\"text-align: right;\">  0.0131277 </td></tr>\n",
       "<tr><td>train_fn_1794a632</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0815375</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.074378 </td><td style=\"text-align: right;\">        0.889217</td><td style=\"text-align: right;\">  0.0886969 </td></tr>\n",
       "<tr><td>train_fn_188d2aa5</td><td style=\"text-align: right;\">  0.920565</td><td style=\"text-align: right;\">      0.042379 </td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0331513</td><td style=\"text-align: right;\">        0.958472</td><td style=\"text-align: right;\">  0.0146958 </td></tr>\n",
       "<tr><td>train_fn_1b4fba34</td><td style=\"text-align: right;\">  0.924961</td><td style=\"text-align: right;\">      0.0372534</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0287956</td><td style=\"text-align: right;\">        0.965356</td><td style=\"text-align: right;\">  0.01188   </td></tr>\n",
       "<tr><td>train_fn_1bb08f6c</td><td style=\"text-align: right;\">  0.926531</td><td style=\"text-align: right;\">      0.0353081</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.027636 </td><td style=\"text-align: right;\">        0.963548</td><td style=\"text-align: right;\">  0.0122919 </td></tr>\n",
       "<tr><td>train_fn_1e0b22ca</td><td style=\"text-align: right;\">  0.924647</td><td style=\"text-align: right;\">      0.0370106</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0307493</td><td style=\"text-align: right;\">        0.952635</td><td style=\"text-align: right;\">  0.0182268 </td></tr>\n",
       "<tr><td>train_fn_22214d57</td><td style=\"text-align: right;\">  0.918681</td><td style=\"text-align: right;\">      0.0437552</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0377084</td><td style=\"text-align: right;\">        0.934044</td><td style=\"text-align: right;\">  0.0256147 </td></tr>\n",
       "<tr><td>train_fn_225c26d2</td><td style=\"text-align: right;\">  0.915856</td><td style=\"text-align: right;\">      0.0386347</td><td style=\"text-align: right;\">                    15</td><td style=\"text-align: right;\">0.0340854</td><td style=\"text-align: right;\">        0.937121</td><td style=\"text-align: right;\">  0.0249867 </td></tr>\n",
       "<tr><td>train_fn_2965a8be</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0608519</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0585873</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.054058  </td></tr>\n",
       "<tr><td>train_fn_2ac5758f</td><td style=\"text-align: right;\">  0.929042</td><td style=\"text-align: right;\">      0.0324192</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0262221</td><td style=\"text-align: right;\">        0.961264</td><td style=\"text-align: right;\">  0.013828  </td></tr>\n",
       "<tr><td>train_fn_2b8aa2a4</td><td style=\"text-align: right;\">  0.724647</td><td style=\"text-align: right;\">      0.179092 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.178844 </td><td style=\"text-align: right;\">        0.616034</td><td style=\"text-align: right;\">  0.179339  </td></tr>\n",
       "<tr><td>train_fn_35cd3e9f</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0927685</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0841907</td><td style=\"text-align: right;\">        0.885854</td><td style=\"text-align: right;\">  0.101346  </td></tr>\n",
       "<tr><td>train_fn_36eee68c</td><td style=\"text-align: right;\">  0.926217</td><td style=\"text-align: right;\">      0.0338867</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0263622</td><td style=\"text-align: right;\">        0.968053</td><td style=\"text-align: right;\">  0.0113132 </td></tr>\n",
       "<tr><td>train_fn_36fe7adb</td><td style=\"text-align: right;\">  0.927473</td><td style=\"text-align: right;\">      0.0361782</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0292576</td><td style=\"text-align: right;\">        0.958789</td><td style=\"text-align: right;\">  0.0154165 </td></tr>\n",
       "<tr><td>train_fn_37b68ddc</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0639497</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0606492</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0540483 </td></tr>\n",
       "<tr><td>train_fn_385deca0</td><td style=\"text-align: right;\">  0.925589</td><td style=\"text-align: right;\">      0.0358357</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.028926 </td><td style=\"text-align: right;\">        0.957838</td><td style=\"text-align: right;\">  0.0151064 </td></tr>\n",
       "<tr><td>train_fn_3f9e4f5f</td><td style=\"text-align: right;\">  0.926531</td><td style=\"text-align: right;\">      0.0354769</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0274514</td><td style=\"text-align: right;\">        0.967133</td><td style=\"text-align: right;\">  0.0114003 </td></tr>\n",
       "<tr><td>train_fn_409726d9</td><td style=\"text-align: right;\">  0.753846</td><td style=\"text-align: right;\">      0.18699  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.184906 </td><td style=\"text-align: right;\">        0.634656</td><td style=\"text-align: right;\">  0.180736  </td></tr>\n",
       "<tr><td>train_fn_48daa01a</td><td style=\"text-align: right;\">  0.920879</td><td style=\"text-align: right;\">      0.0398855</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0317628</td><td style=\"text-align: right;\">        0.956473</td><td style=\"text-align: right;\">  0.0155175 </td></tr>\n",
       "<tr><td>train_fn_49ded67f</td><td style=\"text-align: right;\">  0.928414</td><td style=\"text-align: right;\">      0.0355543</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0277195</td><td style=\"text-align: right;\">        0.966181</td><td style=\"text-align: right;\">  0.0120499 </td></tr>\n",
       "<tr><td>train_fn_4a9bc3f6</td><td style=\"text-align: right;\">  0.923391</td><td style=\"text-align: right;\">      0.0388208</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0296404</td><td style=\"text-align: right;\">        0.967038</td><td style=\"text-align: right;\">  0.0112794 </td></tr>\n",
       "<tr><td>train_fn_4abbd44e</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.169166 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.168629 </td><td style=\"text-align: right;\">        0.802988</td><td style=\"text-align: right;\">  0.169703  </td></tr>\n",
       "<tr><td>train_fn_4c9eb432</td><td style=\"text-align: right;\">  0.566091</td><td style=\"text-align: right;\">      0.193182 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.192203 </td><td style=\"text-align: right;\">        0.50487 </td><td style=\"text-align: right;\">  0.190247  </td></tr>\n",
       "<tr><td>train_fn_4d25da4b</td><td style=\"text-align: right;\">  0.922135</td><td style=\"text-align: right;\">      0.0367394</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0277793</td><td style=\"text-align: right;\">        0.97002 </td><td style=\"text-align: right;\">  0.00985907</td></tr>\n",
       "<tr><td>train_fn_514e9434</td><td style=\"text-align: right;\">  0.923391</td><td style=\"text-align: right;\">      0.0354778</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0272954</td><td style=\"text-align: right;\">        0.968021</td><td style=\"text-align: right;\">  0.0109306 </td></tr>\n",
       "<tr><td>train_fn_51954b81</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.063904 </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0624654</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0653427 </td></tr>\n",
       "<tr><td>train_fn_52caf3af</td><td style=\"text-align: right;\">  0.917425</td><td style=\"text-align: right;\">      0.0397139</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.03035  </td><td style=\"text-align: right;\">        0.966625</td><td style=\"text-align: right;\">  0.0116223 </td></tr>\n",
       "<tr><td>train_fn_552615d0</td><td style=\"text-align: right;\">  0.63956 </td><td style=\"text-align: right;\">      0.191027 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.189879 </td><td style=\"text-align: right;\">        0.527775</td><td style=\"text-align: right;\">  0.187582  </td></tr>\n",
       "<tr><td>train_fn_5901cc90</td><td style=\"text-align: right;\">  0.926217</td><td style=\"text-align: right;\">      0.0365815</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0277938</td><td style=\"text-align: right;\">        0.968307</td><td style=\"text-align: right;\">  0.0102182 </td></tr>\n",
       "<tr><td>train_fn_5989e21b</td><td style=\"text-align: right;\">  0.920251</td><td style=\"text-align: right;\">      0.0339713</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0303167</td><td style=\"text-align: right;\">        0.939754</td><td style=\"text-align: right;\">  0.0230074 </td></tr>\n",
       "<tr><td>train_fn_59d0338e</td><td style=\"text-align: right;\">  0.921821</td><td style=\"text-align: right;\">      0.0398593</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0308876</td><td style=\"text-align: right;\">        0.963643</td><td style=\"text-align: right;\">  0.0129441 </td></tr>\n",
       "<tr><td>train_fn_5a0f52ae</td><td style=\"text-align: right;\">  0.919623</td><td style=\"text-align: right;\">      0.0380626</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0324903</td><td style=\"text-align: right;\">        0.941848</td><td style=\"text-align: right;\">  0.0213456 </td></tr>\n",
       "<tr><td>train_fn_5aa55c70</td><td style=\"text-align: right;\">  0.922763</td><td style=\"text-align: right;\">      0.0363194</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0278667</td><td style=\"text-align: right;\">        0.96726 </td><td style=\"text-align: right;\">  0.0109611 </td></tr>\n",
       "<tr><td>train_fn_5af5d4ea</td><td style=\"text-align: right;\">  0.925903</td><td style=\"text-align: right;\">      0.0375587</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0290251</td><td style=\"text-align: right;\">        0.966752</td><td style=\"text-align: right;\">  0.011958  </td></tr>\n",
       "<tr><td>train_fn_5e5e8f79</td><td style=\"text-align: right;\">  0.924647</td><td style=\"text-align: right;\">      0.0350102</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0300904</td><td style=\"text-align: right;\">        0.945782</td><td style=\"text-align: right;\">  0.0202509 </td></tr>\n",
       "<tr><td>train_fn_6406f961</td><td style=\"text-align: right;\">  0.919937</td><td style=\"text-align: right;\">      0.0386409</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0337511</td><td style=\"text-align: right;\">        0.937026</td><td style=\"text-align: right;\">  0.0239714 </td></tr>\n",
       "<tr><td>train_fn_64351fb5</td><td style=\"text-align: right;\">  0.496389</td><td style=\"text-align: right;\">      0.195264 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.193834 </td><td style=\"text-align: right;\">        0.507027</td><td style=\"text-align: right;\">  0.190975  </td></tr>\n",
       "<tr><td>train_fn_650fbd0e</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0614457</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0589811</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0540519 </td></tr>\n",
       "<tr><td>train_fn_662bf4ff</td><td style=\"text-align: right;\">  0.923705</td><td style=\"text-align: right;\">      0.0360422</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0300192</td><td style=\"text-align: right;\">        0.953015</td><td style=\"text-align: right;\">  0.0179732 </td></tr>\n",
       "<tr><td>train_fn_675943dd</td><td style=\"text-align: right;\">  0.919623</td><td style=\"text-align: right;\">      0.0399243</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0334637</td><td style=\"text-align: right;\">        0.946988</td><td style=\"text-align: right;\">  0.0205425 </td></tr>\n",
       "<tr><td>train_fn_6bac8651</td><td style=\"text-align: right;\">  0.918367</td><td style=\"text-align: right;\">      0.0364413</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0295607</td><td style=\"text-align: right;\">        0.955458</td><td style=\"text-align: right;\">  0.0157994 </td></tr>\n",
       "<tr><td>train_fn_70a3a08b</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0513468</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0502012</td><td style=\"text-align: right;\">        0.889788</td><td style=\"text-align: right;\">  0.0524924 </td></tr>\n",
       "<tr><td>train_fn_79b0f963</td><td style=\"text-align: right;\">  0.764521</td><td style=\"text-align: right;\">      0.182449 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.181916 </td><td style=\"text-align: right;\">        0.681863</td><td style=\"text-align: right;\">  0.18085   </td></tr>\n",
       "<tr><td>train_fn_80b190ce</td><td style=\"text-align: right;\">  0.925275</td><td style=\"text-align: right;\">      0.0364824</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0271654</td><td style=\"text-align: right;\">        0.972082</td><td style=\"text-align: right;\">  0.00853131</td></tr>\n",
       "<tr><td>train_fn_81085d2a</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0901217</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0828381</td><td style=\"text-align: right;\">        0.889217</td><td style=\"text-align: right;\">  0.0974053 </td></tr>\n",
       "<tr><td>train_fn_93adb0dd</td><td style=\"text-align: right;\">  0.924647</td><td style=\"text-align: right;\">      0.0331385</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0282159</td><td style=\"text-align: right;\">        0.94978 </td><td style=\"text-align: right;\">  0.0183707 </td></tr>\n",
       "<tr><td>train_fn_94acbbb5</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.056127 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0560191</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0558034 </td></tr>\n",
       "<tr><td>train_fn_958e552f</td><td style=\"text-align: right;\">  0.919623</td><td style=\"text-align: right;\">      0.0431581</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0325279</td><td style=\"text-align: right;\">        0.966181</td><td style=\"text-align: right;\">  0.0112675 </td></tr>\n",
       "<tr><td>train_fn_9640b55c</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0593397</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0575477</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0539636 </td></tr>\n",
       "<tr><td>train_fn_96e60111</td><td style=\"text-align: right;\">  0.920251</td><td style=\"text-align: right;\">      0.0379593</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0310676</td><td style=\"text-align: right;\">        0.95384 </td><td style=\"text-align: right;\">  0.0172841 </td></tr>\n",
       "<tr><td>train_fn_99de567e</td><td style=\"text-align: right;\">  0.922763</td><td style=\"text-align: right;\">      0.0372908</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0294117</td><td style=\"text-align: right;\">        0.962533</td><td style=\"text-align: right;\">  0.0136537 </td></tr>\n",
       "<tr><td>train_fn_9dbbd248</td><td style=\"text-align: right;\">  0.918995</td><td style=\"text-align: right;\">      0.0384761</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0341819</td><td style=\"text-align: right;\">        0.933663</td><td style=\"text-align: right;\">  0.0255933 </td></tr>\n",
       "<tr><td>train_fn_9e277cab</td><td style=\"text-align: right;\">  0.921821</td><td style=\"text-align: right;\">      0.0424837</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0329546</td><td style=\"text-align: right;\">        0.961962</td><td style=\"text-align: right;\">  0.0138963 </td></tr>\n",
       "<tr><td>train_fn_a0f06c83</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0587131</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0572508</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0543262 </td></tr>\n",
       "<tr><td>train_fn_a122d433</td><td style=\"text-align: right;\">  0.884772</td><td style=\"text-align: right;\">      0.136526 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.12888  </td><td style=\"text-align: right;\">        0.837886</td><td style=\"text-align: right;\">  0.144172  </td></tr>\n",
       "<tr><td>train_fn_a14b4564</td><td style=\"text-align: right;\">  0.920879</td><td style=\"text-align: right;\">      0.0357998</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0330339</td><td style=\"text-align: right;\">        0.931855</td><td style=\"text-align: right;\">  0.027502  </td></tr>\n",
       "<tr><td>train_fn_a3542e26</td><td style=\"text-align: right;\">  0.897645</td><td style=\"text-align: right;\">      0.0474478</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0459207</td><td style=\"text-align: right;\">        0.892231</td><td style=\"text-align: right;\">  0.048975  </td></tr>\n",
       "<tr><td>train_fn_a72c9295</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0622851</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0594936</td><td style=\"text-align: right;\">        0.889597</td><td style=\"text-align: right;\">  0.0650766 </td></tr>\n",
       "<tr><td>train_fn_a81e9f86</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0554138</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.055086 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0544305 </td></tr>\n",
       "<tr><td>train_fn_a90b23db</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0899964</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0814802</td><td style=\"text-align: right;\">        0.875289</td><td style=\"text-align: right;\">  0.0985125 </td></tr>\n",
       "<tr><td>train_fn_a9d4d225</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0672489</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0631352</td><td style=\"text-align: right;\">        0.889756</td><td style=\"text-align: right;\">  0.0713626 </td></tr>\n",
       "<tr><td>train_fn_ab4e8af9</td><td style=\"text-align: right;\">  0.908948</td><td style=\"text-align: right;\">      0.0408165</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0399348</td><td style=\"text-align: right;\">        0.916722</td><td style=\"text-align: right;\">  0.0381714 </td></tr>\n",
       "<tr><td>train_fn_ac3ad2d5</td><td style=\"text-align: right;\">  0.925903</td><td style=\"text-align: right;\">      0.0376204</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0288391</td><td style=\"text-align: right;\">        0.967799</td><td style=\"text-align: right;\">  0.0112765 </td></tr>\n",
       "<tr><td>train_fn_ae5bca6c</td><td style=\"text-align: right;\">  0.917425</td><td style=\"text-align: right;\">      0.0367419</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0338125</td><td style=\"text-align: right;\">        0.931728</td><td style=\"text-align: right;\">  0.0279537 </td></tr>\n",
       "<tr><td>train_fn_b081ffc3</td><td style=\"text-align: right;\">  0.9281  </td><td style=\"text-align: right;\">      0.0338247</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0278383</td><td style=\"text-align: right;\">        0.956442</td><td style=\"text-align: right;\">  0.0158656 </td></tr>\n",
       "<tr><td>train_fn_b0ed2ac1</td><td style=\"text-align: right;\">  0.909576</td><td style=\"text-align: right;\">      0.0422111</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0401062</td><td style=\"text-align: right;\">        0.898036</td><td style=\"text-align: right;\">  0.044316  </td></tr>\n",
       "<tr><td>train_fn_b4b39181</td><td style=\"text-align: right;\">  0.696389</td><td style=\"text-align: right;\">      0.184679 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.184515 </td><td style=\"text-align: right;\">        0.575553</td><td style=\"text-align: right;\">  0.184843  </td></tr>\n",
       "<tr><td>train_fn_b4b9c4e9</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.109556 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0985974</td><td style=\"text-align: right;\">        0.876939</td><td style=\"text-align: right;\">  0.120514  </td></tr>\n",
       "<tr><td>train_fn_b5fa2bcb</td><td style=\"text-align: right;\">  0.816327</td><td style=\"text-align: right;\">      0.185325 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.18439  </td><td style=\"text-align: right;\">        0.570604</td><td style=\"text-align: right;\">  0.186261  </td></tr>\n",
       "<tr><td>train_fn_bb71d314</td><td style=\"text-align: right;\">  0.926845</td><td style=\"text-align: right;\">      0.042551 </td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0349156</td><td style=\"text-align: right;\">        0.946893</td><td style=\"text-align: right;\">  0.0196447 </td></tr>\n",
       "<tr><td>train_fn_bd106eb8</td><td style=\"text-align: right;\">  0.920565</td><td style=\"text-align: right;\">      0.0406162</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0300595</td><td style=\"text-align: right;\">        0.97075 </td><td style=\"text-align: right;\">  0.00894599</td></tr>\n",
       "<tr><td>train_fn_be78d7b2</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0593937</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.057416 </td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0534607 </td></tr>\n",
       "<tr><td>train_fn_bedea489</td><td style=\"text-align: right;\">  0.917111</td><td style=\"text-align: right;\">      0.044753 </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0383358</td><td style=\"text-align: right;\">        0.936709</td><td style=\"text-align: right;\">  0.0255015 </td></tr>\n",
       "<tr><td>train_fn_c0520f72</td><td style=\"text-align: right;\">  0.928728</td><td style=\"text-align: right;\">      0.0361217</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0277927</td><td style=\"text-align: right;\">        0.967165</td><td style=\"text-align: right;\">  0.0111347 </td></tr>\n",
       "<tr><td>train_fn_c9324bbd</td><td style=\"text-align: right;\">  0.917425</td><td style=\"text-align: right;\">      0.0417266</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0355723</td><td style=\"text-align: right;\">        0.940643</td><td style=\"text-align: right;\">  0.0232637 </td></tr>\n",
       "<tr><td>train_fn_ce563672</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.058868 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0572238</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0539356 </td></tr>\n",
       "<tr><td>train_fn_d2e65972</td><td style=\"text-align: right;\">  0.927159</td><td style=\"text-align: right;\">      0.0363165</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0275532</td><td style=\"text-align: right;\">        0.967958</td><td style=\"text-align: right;\">  0.0100265 </td></tr>\n",
       "<tr><td>train_fn_d4ad7c65</td><td style=\"text-align: right;\">  0.925589</td><td style=\"text-align: right;\">      0.0364914</td><td style=\"text-align: right;\">                     9</td><td style=\"text-align: right;\">0.0286139</td><td style=\"text-align: right;\">        0.963072</td><td style=\"text-align: right;\">  0.0128588 </td></tr>\n",
       "<tr><td>train_fn_d6e284d4</td><td style=\"text-align: right;\">  0.913972</td><td style=\"text-align: right;\">      0.0481829</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0387797</td><td style=\"text-align: right;\">        0.947019</td><td style=\"text-align: right;\">  0.0199733 </td></tr>\n",
       "<tr><td>train_fn_d8602e9a</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0592307</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0575201</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0540989 </td></tr>\n",
       "<tr><td>train_fn_da8ede27</td><td style=\"text-align: right;\">  0.918367</td><td style=\"text-align: right;\">      0.0534259</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0391503</td><td style=\"text-align: right;\">        0.9691  </td><td style=\"text-align: right;\">  0.0105991 </td></tr>\n",
       "<tr><td>train_fn_def7ede9</td><td style=\"text-align: right;\">  0.928728</td><td style=\"text-align: right;\">      0.0348823</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0271228</td><td style=\"text-align: right;\">        0.966594</td><td style=\"text-align: right;\">  0.0116037 </td></tr>\n",
       "<tr><td>train_fn_df4621cf</td><td style=\"text-align: right;\">  0.921507</td><td style=\"text-align: right;\">      0.0360145</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.028335 </td><td style=\"text-align: right;\">        0.962374</td><td style=\"text-align: right;\">  0.0129761 </td></tr>\n",
       "<tr><td>train_fn_e2f73372</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.169074 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.16888  </td><td style=\"text-align: right;\">        0.781638</td><td style=\"text-align: right;\">  0.168492  </td></tr>\n",
       "<tr><td>train_fn_e34c82e7</td><td style=\"text-align: right;\">  0.924019</td><td style=\"text-align: right;\">      0.0363402</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.027498 </td><td style=\"text-align: right;\">        0.968846</td><td style=\"text-align: right;\">  0.00981354</td></tr>\n",
       "<tr><td>train_fn_e75aaa28</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0590471</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0573424</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.053933  </td></tr>\n",
       "<tr><td>train_fn_edbc9818</td><td style=\"text-align: right;\">  0.9281  </td><td style=\"text-align: right;\">      0.0374842</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0290477</td><td style=\"text-align: right;\">        0.965864</td><td style=\"text-align: right;\">  0.0121745 </td></tr>\n",
       "<tr><td>train_fn_ee36f293</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0579388</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0577699</td><td style=\"text-align: right;\">        0.889756</td><td style=\"text-align: right;\">  0.0574321 </td></tr>\n",
       "<tr><td>train_fn_f5488994</td><td style=\"text-align: right;\">  0.921507</td><td style=\"text-align: right;\">      0.0363026</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0313137</td><td style=\"text-align: right;\">        0.945148</td><td style=\"text-align: right;\">  0.0213358 </td></tr>\n",
       "<tr><td>train_fn_f6b38bd4</td><td style=\"text-align: right;\">  0.921821</td><td style=\"text-align: right;\">      0.0432314</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0359795</td><td style=\"text-align: right;\">        0.94445 </td><td style=\"text-align: right;\">  0.0214756 </td></tr>\n",
       "<tr><td>train_fn_f768ed09</td><td style=\"text-align: right;\">  0.885086</td><td style=\"text-align: right;\">      0.0595379</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0576401</td><td style=\"text-align: right;\">        0.889724</td><td style=\"text-align: right;\">  0.0538445 </td></tr>\n",
       "<tr><td>train_fn_fb0c861f</td><td style=\"text-align: right;\">  0.921193</td><td style=\"text-align: right;\">      0.0353133</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0282649</td><td style=\"text-align: right;\">        0.96082 </td><td style=\"text-align: right;\">  0.0141679 </td></tr>\n",
       "<tr><td>train_fn_fccbaed7</td><td style=\"text-align: right;\">  0.925589</td><td style=\"text-align: right;\">      0.0380234</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0298937</td><td style=\"text-align: right;\">        0.963358</td><td style=\"text-align: right;\">  0.0136341 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=480)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=480)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=562)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=562)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=694)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=694)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=785)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=785)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=863)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=863)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=955)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=955)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1041)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1041)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1129)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1129)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1201)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1201)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1286)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1286)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1372)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1372)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1456)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1456)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1571)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1571)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1663)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1663)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1779)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1779)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1863)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1863)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1963)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1963)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2049)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2049)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2135)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2135)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2234)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2234)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2306)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2306)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2412)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2412)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2505)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2505)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2580)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2580)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2681)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2681)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2756)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2756)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2857)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2857)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2932)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2932)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3019)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3019)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3104)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3104)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3177)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3177)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3281)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3281)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3353)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3353)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3458)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3458)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3529)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3529)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3616)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3616)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3716)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3716)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3789)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3789)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3878)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3878)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3987)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3987)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4076)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4076)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4157)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4157)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4272)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4272)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4353)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4353)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4469)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4469)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4551)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4551)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4640)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4640)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4729)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4729)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4810)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4810)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4888)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4888)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4970)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4970)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5063)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5063)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5145)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5145)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5261)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5261)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5343)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5343)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5442)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5442)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5528)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5528)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5617)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5617)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5721)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5721)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5795)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5795)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5891)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5891)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5971)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5971)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6061)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6061)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6168)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6168)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6258)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6258)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6365)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6365)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6455)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6455)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6540)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6540)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6613)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6613)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6699)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6699)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6770)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6770)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6856)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6856)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6927)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6927)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7053)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7053)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7124)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7124)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7228)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7228)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7316)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7316)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7353)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7353)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7447)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7447)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7512)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7512)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7605)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7605)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7677)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7677)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7763)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7763)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7875)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7875)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7960)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7960)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8072)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8072)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8157)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8157)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8269)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8269)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8354)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8354)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8465)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8465)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8537)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8537)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8622)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8622)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8694)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8694)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8781)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8781)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8876)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8876)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8962)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8962)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=9056)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=9056)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=9137)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=9137)\u001b[0m   warnings.warn(\n",
      "2024-08-29 16:46:57,724\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_fn_2024-08-29_11-38-20' in 0.0502s.\n",
      "2024-08-29 16:46:57,779\tINFO tune.py:1041 -- Total run time: 18517.57 seconds (18506.63 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /kaggle/working/tensorboard_logs\n",
    "\n",
    "\n",
    "# ray.init(ignore_reinit_error=True)\n",
    "# reporter = CLIReporter(\n",
    "#     metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "#     print_intermediate_tables=False\n",
    "# )\n",
    "\n",
    "reporter = JupyterNotebookReporter(\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "    print_intermediate_tables=False,\n",
    ")\n",
    "\n",
    "# tuner = tune.run(\n",
    "#     train_fn,\n",
    "#     config=search_space,\n",
    "#     num_samples=25,\n",
    "#     progress_reporter=reporter,\n",
    "#     resources_per_trial={\"cpu\": 4, \"gpu\": 2},  # Adjust resources as needed\n",
    "#     scheduler=ASHAScheduler(\n",
    "#         metric=\"accuracy\",\n",
    "#         mode=\"max\",\n",
    "#         max_t=15,\n",
    "#         grace_period=1,\n",
    "#         reduction_factor=2\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# reporter = TensorBoardReporter(\n",
    "#     metric_columns=[\"loss\", \"accuracy\", \"training_iteration\", \"train_loss\", \"train_accuracy\"]\n",
    "# )\n",
    "\n",
    "\n",
    "result = tune.run(\n",
    "    train_fn,\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=100,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    search_alg=optuna_search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfffbc3",
   "metadata": {
    "papermill": {
     "duration": 0.046235,
     "end_time": "2024-08-29T16:46:58.075955",
     "exception": false,
     "start_time": "2024-08-29T16:46:58.029720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## REPORTER RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff8711cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T16:46:58.171211Z",
     "iopub.status.busy": "2024-08-29T16:46:58.169804Z",
     "iopub.status.idle": "2024-08-29T16:46:58.178488Z",
     "shell.execute_reply": "2024-08-29T16:46:58.177621Z"
    },
    "papermill": {
     "duration": 0.059122,
     "end_time": "2024-08-29T16:46:58.181262",
     "exception": false,
     "start_time": "2024-08-29T16:46:58.122140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'dropout_rate': 0.27, 'batch_size': 32, 'weight_decay': 0.01, 'lr': 2e-05, 'epochs': 10}\n",
      "Best trial final validation loss: 0.02565886005759239\n",
      "Best trial final validation accuracy: 0.9296703296703297\n",
      "Best trial final training loss: 0.010343745499453012\n",
      "Best trial final training accuracy: 0.9689413406935059\n",
      "Best trial final custom_metric: 0.03331641733666208\n",
      "Best trial final Early Stopping Epoch: 10\n",
      "NOTE: Both the accuracy are based on converting values > 0.5 to 1 and values < 0.5 to 0, hence, rely on the loss, MSE here.\n"
     ]
    }
   ],
   "source": [
    "# best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "# best_checkpoint_dir = best_trial.checkpoint.value\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "print(f\"Best trial final training loss: {best_trial.last_result['train_loss']}\")\n",
    "print(f\"Best trial final training accuracy: {best_trial.last_result['train_accuracy']}\")\n",
    "print(f\"Best trial final custom_metric: {best_trial.last_result['custom_metric']}\")\n",
    "print(f\"Best trial final Early Stopping Epoch: {best_trial.last_result['early_stopping_epoch']}\")\n",
    "print(f\"NOTE: Both the accuracy are based on converting values > 0.5 to 1 and values < 0.5 to 0, hence, rely on the loss, MSE here.\")\n",
    "\n",
    "\n",
    "# print(\"\\nModel Parameters:\")\n",
    "# for name, param in best_model.named_parameters():\n",
    "#     print(f\"{name}: {param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97613e7d",
   "metadata": {
    "papermill": {
     "duration": 0.047009,
     "end_time": "2024-08-29T16:46:58.278613",
     "exception": false,
     "start_time": "2024-08-29T16:46:58.231604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to Train the Model with the Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26095cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T16:46:58.375618Z",
     "iopub.status.busy": "2024-08-29T16:46:58.375160Z",
     "iopub.status.idle": "2024-08-29T16:46:58.398676Z",
     "shell.execute_reply": "2024-08-29T16:46:58.397622Z"
    },
    "papermill": {
     "duration": 0.075001,
     "end_time": "2024-08-29T16:46:58.401098",
     "exception": false,
     "start_time": "2024-08-29T16:46:58.326097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_best_model(config, device, train_dataset = train_ds_pd, val_dataset = validation_ds_pd):\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Tuning Model with:\n",
    "    {config}\n",
    "    \"\"\")\n",
    "    \n",
    "#     model = EmotionModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=7,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "    model = EmotionModel(\n",
    "        roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "        n_classes=7,\n",
    "        dropout_rate=config[\"dropout_rate\"]\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    ## DATASET ENCODING\n",
    "    \n",
    "    train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "    val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_path = None\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    \n",
    "    ### Running for config epochs +patience+1 for introducing noise. \n",
    "    \n",
    "    for epoch in range((config[\"epochs\"]+patience+1)):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            with autocast(device_type=device.type):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "            correct_train_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "            total_train_preds += labels.numel()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val_preds = 0\n",
    "        total_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "                correct_val_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "                total_val_preds += labels.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_preds / total_val_preds\n",
    "\n",
    "        # Save the model checkpoint after each epoch\n",
    "        checkpoint_path = f\"/kaggle/working/best_checkpoint.pt\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "#         # Save the best model based on validation loss\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             best_model_path = checkpoint_path\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_since_improvement = 0\n",
    "            best_model_path = checkpoint_path\n",
    "            # Save the model checkpoint with the best validation loss\n",
    "#             checkpoint_path = f\"/kaggle/working/best_checkpoint.pt\"\n",
    "#             torch.save(model.state_dict(), checkpoint_path)\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config  # Save configuration\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            print(f\"Best Model Epoch Saved: {epoch - patience+1}\")\n",
    "#             print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "#         if epochs_since_improvement >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "#             print(f\"Best Model Epoch Saved: {epoch - patience}\")\n",
    "#             break\n",
    "            \n",
    "        custom_metric = calculate_custom_metric(avg_val_loss, avg_train_loss)        \n",
    "#         custom_metric = calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy)\n",
    "        print(f\"\"\"\n",
    "        Validation Loss: {avg_val_loss},\n",
    "        Training Loss: {avg_train_loss},\n",
    "        Argmax Binary Validation Accuracy: {val_accuracy},\n",
    "        Argmax Binary Training Accuracy: {train_accuracy},\n",
    "        Custom Metric: {custom_metric},\n",
    "        Epochs: {epoch+1}\n",
    "        \"\"\")\n",
    "    \n",
    "    print(f\"Best Validation Loss: {best_val_loss}, Best Validation accuracy: {val_accuracy}\")\n",
    "    \n",
    "    return model, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1700be5",
   "metadata": {
    "papermill": {
     "duration": 0.047008,
     "end_time": "2024-08-29T16:46:58.498973",
     "exception": false,
     "start_time": "2024-08-29T16:46:58.451965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BEST MODEL AFTER FINAL TRAINING & VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26b65325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T16:46:58.596151Z",
     "iopub.status.busy": "2024-08-29T16:46:58.595781Z",
     "iopub.status.idle": "2024-08-29T17:01:50.553496Z",
     "shell.execute_reply": "2024-08-29T17:01:50.552351Z"
    },
    "papermill": {
     "duration": 892.059243,
     "end_time": "2024-08-29T17:01:50.605678",
     "exception": false,
     "start_time": "2024-08-29T16:46:58.546435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Tuning Model with:\n",
      "    {'dropout_rate': 0.27, 'batch_size': 32, 'weight_decay': 0.01, 'lr': 2e-05, 'epochs': 10}\n",
      "    \n",
      "\n",
      "        Validation Loss: 0.050274262328942616,\n",
      "        Training Loss: 0.08263761790614602,\n",
      "        Argmax Binary Validation Accuracy: 0.8850863422291994,\n",
      "        Argmax Binary Training Accuracy: 0.84721296913169,\n",
      "        Custom Metric: 0.06645594011754431,\n",
      "        Epochs: 1\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03766917511820793,\n",
      "        Training Loss: 0.0426598615706601,\n",
      "        Argmax Binary Validation Accuracy: 0.9152276295133438,\n",
      "        Argmax Binary Training Accuracy: 0.9027632372069414,\n",
      "        Custom Metric: 0.04016451834443402,\n",
      "        Epochs: 2\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.040525172278285025,\n",
      "        Training Loss: 0.03281943859360742,\n",
      "        Argmax Binary Validation Accuracy: 0.9145996860282575,\n",
      "        Argmax Binary Training Accuracy: 0.9216712667745313,\n",
      "        Custom Metric: 0.044378039120623824,\n",
      "        Epochs: 3\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.031855240712563196,\n",
      "        Training Loss: 0.026609879581535115,\n",
      "        Argmax Binary Validation Accuracy: 0.9221350078492936,\n",
      "        Argmax Binary Training Accuracy: 0.9318232289584721,\n",
      "        Custom Metric: 0.034477921278077236,\n",
      "        Epochs: 4\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.0329391590009133,\n",
      "        Training Loss: 0.02230595823060325,\n",
      "        Argmax Binary Validation Accuracy: 0.9199372056514914,\n",
      "        Argmax Binary Training Accuracy: 0.9413724183877415,\n",
      "        Custom Metric: 0.03825575938606832,\n",
      "        Epochs: 5\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.029573649168014526,\n",
      "        Training Loss: 0.018990926206746,\n",
      "        Argmax Binary Validation Accuracy: 0.9240188383045526,\n",
      "        Argmax Binary Training Accuracy: 0.950033311125916,\n",
      "        Custom Metric: 0.03486501064864879,\n",
      "        Epochs: 6\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.028144451479117077,\n",
      "        Training Loss: 0.016221339226508816,\n",
      "        Argmax Binary Validation Accuracy: 0.9271585557299843,\n",
      "        Argmax Binary Training Accuracy: 0.9575203832365724,\n",
      "        Custom Metric: 0.03410600760542121,\n",
      "        Epochs: 7\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.028131451085209848,\n",
      "        Training Loss: 0.014072294165347273,\n",
      "        Argmax Binary Validation Accuracy: 0.9262166405023547,\n",
      "        Argmax Binary Training Accuracy: 0.9618666920465722,\n",
      "        Custom Metric: 0.03516102954514114,\n",
      "        Epochs: 8\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.028838978707790376,\n",
      "        Training Loss: 0.012199268841468696,\n",
      "        Argmax Binary Validation Accuracy: 0.9218210361067504,\n",
      "        Argmax Binary Training Accuracy: 0.9641826084197836,\n",
      "        Custom Metric: 0.03715883364095122,\n",
      "        Epochs: 9\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.027457192043463387,\n",
      "        Training Loss: 0.011137869222261381,\n",
      "        Argmax Binary Validation Accuracy: 0.926530612244898,\n",
      "        Argmax Binary Training Accuracy: 0.9660543764474477,\n",
      "        Custom Metric: 0.03561685345406439,\n",
      "        Epochs: 10\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.027228819206357002,\n",
      "        Training Loss: 0.009911481621311911,\n",
      "        Argmax Binary Validation Accuracy: 0.9281004709576138,\n",
      "        Argmax Binary Training Accuracy: 0.968592366993433,\n",
      "        Custom Metric: 0.035887487998879544,\n",
      "        Epochs: 11\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.02773384836812814,\n",
      "        Training Loss: 0.00869549632878274,\n",
      "        Argmax Binary Validation Accuracy: 0.9237048665620095,\n",
      "        Argmax Binary Training Accuracy: 0.9707813838393452,\n",
      "        Custom Metric: 0.03725302438780084,\n",
      "        Epochs: 12\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.025899574160575867,\n",
      "        Training Loss: 0.007597317451822525,\n",
      "        Argmax Binary Validation Accuracy: 0.9262166405023547,\n",
      "        Argmax Binary Training Accuracy: 0.9723359030487612,\n",
      "        Custom Metric: 0.03505070251495254,\n",
      "        Epochs: 13\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.02651520197590192,\n",
      "        Training Loss: 0.007389455293932705,\n",
      "        Argmax Binary Validation Accuracy: 0.926530612244898,\n",
      "        Argmax Binary Training Accuracy: 0.9729704006852574,\n",
      "        Custom Metric: 0.03607807531688653,\n",
      "        Epochs: 14\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.025264722605546314,\n",
      "        Training Loss: 0.006322807323597107,\n",
      "        Argmax Binary Validation Accuracy: 0.9306122448979591,\n",
      "        Argmax Binary Training Accuracy: 0.9762380635132134,\n",
      "        Custom Metric: 0.03473568024652092,\n",
      "        Epochs: 15\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.025840206692616146,\n",
      "        Training Loss: 0.005639523145551166,\n",
      "        Argmax Binary Validation Accuracy: 0.9255886970172684,\n",
      "        Argmax Binary Training Accuracy: 0.9770311855588338,\n",
      "        Custom Metric: 0.035940548466148636,\n",
      "        Epochs: 16\n",
      "        \n",
      "Best Validation Loss: 0.025264722605546314, Best Validation accuracy: 0.9255886970172684\n"
     ]
    }
   ],
   "source": [
    "best_config = best_trial.config\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model with the best configuration\n",
    "best_model, best_model_path = train_best_model(best_config, device, train_ds_pd, validation_ds_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5dee2a",
   "metadata": {
    "papermill": {
     "duration": 0.048822,
     "end_time": "2024-08-29T17:01:50.702442",
     "exception": false,
     "start_time": "2024-08-29T17:01:50.653620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f54564a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:50.798826Z",
     "iopub.status.busy": "2024-08-29T17:01:50.798446Z",
     "iopub.status.idle": "2024-08-29T17:01:52.137438Z",
     "shell.execute_reply": "2024-08-29T17:01:52.136247Z"
    },
    "papermill": {
     "duration": 1.389763,
     "end_time": "2024-08-29T17:01:52.139449",
     "exception": false,
     "start_time": "2024-08-29T17:01:50.749686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: /kaggle/working/final_best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "#### Comment when wanting to select the model which is stopped early to avoid over or under fitting\n",
    "\n",
    "# final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# torch.save(best_model.state_dict(), final_model_path) \n",
    "\n",
    "#### Comment when wanting to select the model which is stopped early to avoid over or under fitting\n",
    "\n",
    "final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# torch.save(best_model.state_dict(), final_model_path)\n",
    "torch.save({'model_state_dict': best_model.state_dict(),'config': best_config}, final_model_path)\n",
    "\n",
    "print(f\"Best model saved at: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33102244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:52.237460Z",
     "iopub.status.busy": "2024-08-29T17:01:52.237104Z",
     "iopub.status.idle": "2024-08-29T17:01:52.241156Z",
     "shell.execute_reply": "2024-08-29T17:01:52.240347Z"
    },
    "papermill": {
     "duration": 0.055032,
     "end_time": "2024-08-29T17:01:52.243036",
     "exception": false,
     "start_time": "2024-08-29T17:01:52.188004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_model = EmotionModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=7,\n",
    "#     dropout_rate=best_trial.config[\"dropout_rate\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c14058a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:52.342079Z",
     "iopub.status.busy": "2024-08-29T17:01:52.341734Z",
     "iopub.status.idle": "2024-08-29T17:01:52.345971Z",
     "shell.execute_reply": "2024-08-29T17:01:52.344838Z"
    },
    "papermill": {
     "duration": 0.056841,
     "end_time": "2024-08-29T17:01:52.347915",
     "exception": false,
     "start_time": "2024-08-29T17:01:52.291074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kaggle_working_dir = '/kaggle/working'\n",
    "# best_model_path = os.path.join(kaggle_working_dir, \"checkpoint.pt\")\n",
    "# best_model.load_state_dict(torch.load(best_model_path))\n",
    "# best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2941bcfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:52.456420Z",
     "iopub.status.busy": "2024-08-29T17:01:52.456037Z",
     "iopub.status.idle": "2024-08-29T17:01:52.461008Z",
     "shell.execute_reply": "2024-08-29T17:01:52.460049Z"
    },
    "papermill": {
     "duration": 0.065616,
     "end_time": "2024-08-29T17:01:52.463048",
     "exception": false,
     "start_time": "2024-08-29T17:01:52.397432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # best_config = result.get_best_config(metric='accuracy', mode='max')\n",
    "# # best_trial = result.get_best_trial(metric='accuracy', mode='max')\n",
    "\n",
    "# best_config = result.get_best_config(metric='custom_metric', mode='min')\n",
    "# best_trial = result.get_best_trial(metric='custom_metric', mode='min')\n",
    "\n",
    "# print(f\"Best trial config: {best_trial.config}\")\n",
    "# print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "# print(\"\\nModel Parameters:\")\n",
    "# for name, param in best_model.named_parameters():\n",
    "#     print(f\"{name}: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80a2662e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:52.561661Z",
     "iopub.status.busy": "2024-08-29T17:01:52.561337Z",
     "iopub.status.idle": "2024-08-29T17:01:52.565305Z",
     "shell.execute_reply": "2024-08-29T17:01:52.564373Z"
    },
    "papermill": {
     "duration": 0.055728,
     "end_time": "2024-08-29T17:01:52.567158",
     "exception": false,
     "start_time": "2024-08-29T17:01:52.511430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_trial = tuner.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "# print(f\"Best trial config: {best_trial.config}\")\n",
    "# print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6615cdec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:52.666796Z",
     "iopub.status.busy": "2024-08-29T17:01:52.666498Z",
     "iopub.status.idle": "2024-08-29T17:01:52.670763Z",
     "shell.execute_reply": "2024-08-29T17:01:52.669881Z"
    },
    "papermill": {
     "duration": 0.057018,
     "end_time": "2024-08-29T17:01:52.672590",
     "exception": false,
     "start_time": "2024-08-29T17:01:52.615572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load the best model\n",
    "# best_model = EmotionModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=7,\n",
    "#     dropout_rate=best_trial.config[\"dropout_rate\"]\n",
    "# )\n",
    "\n",
    "# best_model_path = os.path.join(best_checkpoint_dir, \"checkpoint.pt\")\n",
    "# best_model.load_state_dict(torch.load(best_model_path))\n",
    "# best_model.eval()\n",
    "\n",
    "# print(f\"Best trial config: {best_trial.config}\")\n",
    "# print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52c3b843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:52.773667Z",
     "iopub.status.busy": "2024-08-29T17:01:52.773338Z",
     "iopub.status.idle": "2024-08-29T17:01:52.777440Z",
     "shell.execute_reply": "2024-08-29T17:01:52.776570Z"
    },
    "papermill": {
     "duration": 0.057032,
     "end_time": "2024-08-29T17:01:52.779319",
     "exception": false,
     "start_time": "2024-08-29T17:01:52.722287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save the best model\n",
    "# best_model = EmotionClassifier(hidden_size=best_config['hidden_size'], dropout_rate=best_config['dropout_rate']).to(device)\n",
    "# checkpoint_path = \"best_model.pth\"\n",
    "# torch.save(best_model.state_dict(), checkpoint_path)\n",
    "# print(f\"Best model saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7856c9ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:52.885205Z",
     "iopub.status.busy": "2024-08-29T17:01:52.884144Z",
     "iopub.status.idle": "2024-08-29T17:01:52.889691Z",
     "shell.execute_reply": "2024-08-29T17:01:52.888531Z"
    },
    "papermill": {
     "duration": 0.064965,
     "end_time": "2024-08-29T17:01:52.892670",
     "exception": false,
     "start_time": "2024-08-29T17:01:52.827705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save the best model\n",
    "# best_model = EmotionClassifier(hidden_size=best_config['hidden_size'], dropout_rate=best_config['dropout_rate']).to(device)\n",
    "# bestmodel_path = f\"/kaggle/working/bestmodel.pt\"\n",
    "# torch.save(best_model.state_dict(), checkpoint_path)\n",
    "# print(f\"Best model saved to {bestmodel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d2e8f",
   "metadata": {
    "papermill": {
     "duration": 0.048288,
     "end_time": "2024-08-29T17:01:53.000249",
     "exception": false,
     "start_time": "2024-08-29T17:01:52.951961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3f532950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:53.106878Z",
     "iopub.status.busy": "2024-08-29T17:01:53.106516Z",
     "iopub.status.idle": "2024-08-29T17:01:54.669983Z",
     "shell.execute_reply": "2024-08-29T17:01:54.668544Z"
    },
    "papermill": {
     "duration": 1.615038,
     "end_time": "2024-08-29T17:01:54.672103",
     "exception": false,
     "start_time": "2024-08-29T17:01:53.057065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /kaggle/working/final_best_model.pt for inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# final_model_path = \"/kaggle/input/tachygraphy-lv2-emotion-moodtags-v1/transformers/default/1/final_best_model.pt\"\n",
    "\n",
    "\n",
    "checkpoint = torch.load(final_model_path)\n",
    "config = checkpoint['config']\n",
    "\n",
    "# loaded_model = EmotionModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=7,\n",
    "#     dropout_rate=config[\"dropout_rate\"]\n",
    "# )\n",
    "loaded_model = EmotionModel(\n",
    "    roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "    n_classes=7,\n",
    "    dropout_rate=config[\"dropout_rate\"]\n",
    ")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"Loaded model from {final_model_path} for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffafaf",
   "metadata": {
    "papermill": {
     "duration": 0.049737,
     "end_time": "2024-08-29T17:01:54.772175",
     "exception": false,
     "start_time": "2024-08-29T17:01:54.722438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PREDICT ON RANDOM INPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395de3b",
   "metadata": {
    "papermill": {
     "duration": 0.047979,
     "end_time": "2024-08-29T17:01:54.871011",
     "exception": false,
     "start_time": "2024-08-29T17:01:54.823032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ff3cc0d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:54.973974Z",
     "iopub.status.busy": "2024-08-29T17:01:54.973138Z",
     "iopub.status.idle": "2024-08-29T17:01:54.979982Z",
     "shell.execute_reply": "2024-08-29T17:01:54.979063Z"
    },
    "papermill": {
     "duration": 0.059065,
     "end_time": "2024-08-29T17:01:54.981834",
     "exception": false,
     "start_time": "2024-08-29T17:01:54.922769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, device, max_len=128):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    return torch.sigmoid(outputs).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41fb75ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:55.080196Z",
     "iopub.status.busy": "2024-08-29T17:01:55.079859Z",
     "iopub.status.idle": "2024-08-29T17:01:55.089851Z",
     "shell.execute_reply": "2024-08-29T17:01:55.089163Z"
    },
    "papermill": {
     "duration": 0.061455,
     "end_time": "2024-08-29T17:01:55.091695",
     "exception": false,
     "start_time": "2024-08-29T17:01:55.030240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = loaded_model\n",
    "best_model = best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e3e8d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:55.191831Z",
     "iopub.status.busy": "2024-08-29T17:01:55.191182Z",
     "iopub.status.idle": "2024-08-29T17:01:55.810948Z",
     "shell.execute_reply": "2024-08-29T17:01:55.810133Z"
    },
    "papermill": {
     "duration": 0.671539,
     "end_time": "2024-08-29T17:01:55.813238",
     "exception": false,
     "start_time": "2024-08-29T17:01:55.141699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc9e7a",
   "metadata": {
    "papermill": {
     "duration": 0.050171,
     "end_time": "2024-08-29T17:01:55.914855",
     "exception": false,
     "start_time": "2024-08-29T17:01:55.864684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Samples & Random Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6101e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:56.014186Z",
     "iopub.status.busy": "2024-08-29T17:01:56.013849Z",
     "iopub.status.idle": "2024-08-29T17:01:58.006957Z",
     "shell.execute_reply": "2024-08-29T17:01:58.006064Z"
    },
    "papermill": {
     "duration": 2.044854,
     "end_time": "2024-08-29T17:01:58.009090",
     "exception": false,
     "start_time": "2024-08-29T17:01:55.964236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions for 'hey! hru, wanna ply valo toni8?': [[0.01633848 0.01452175 0.01502478 0.18976927 0.30348647 0.06020644\n",
      "  0.47681126]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"8a870967-8114-4939-a1ab-297aca85cb43\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8a870967-8114-4939-a1ab-297aca85cb43\")) {                    Plotly.newPlot(                        \"8a870967-8114-4939-a1ab-297aca85cb43\",                        [{\"hovertemplate\":\"r=%{r}\\u003cbr\\u003etheta=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"r\":[0.01633848,0.014521753,0.015024779,0.18976927,0.30348647,0.060206436,0.47681126,0.01633848],\"showlegend\":false,\"subplot\":\"polar\",\"theta\":[\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\",\"anger\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('8a870967-8114-4939-a1ab-297aca85cb43');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted emotions for '{sample_text}': {predictions}\")\n",
    "\n",
    "\n",
    "predictions_array = predictions.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "emotion_moodtags = []\n",
    "for items in predictions_array:\n",
    "    emotion_moodtags.append(items)\n",
    "\n",
    "fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Since predictions are probabilistic values, normalize to ensure they sum up to 1\n",
    "normalized_predictions = predictions_array / predictions_array.sum()  # Normalize the values\n",
    "\n",
    "\n",
    "## Horizontal Stacked Bar Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "left = 0\n",
    "for i in range(len(normalized_predictions)):\n",
    "    ax.barh(0, normalized_predictions[i], color=plt.cm.tab10(i), left=left, label=EMOTION_LABELS[i])\n",
    "    left += normalized_predictions[i]\n",
    "\n",
    "# Add legend\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(EMOTION_LABELS))\n",
    "\n",
    "# Add title\n",
    "plt.title('Emotion Prediction Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d405809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:58.110636Z",
     "iopub.status.busy": "2024-08-29T17:01:58.110293Z",
     "iopub.status.idle": "2024-08-29T17:01:58.478153Z",
     "shell.execute_reply": "2024-08-29T17:01:58.477217Z"
    },
    "papermill": {
     "duration": 0.420381,
     "end_time": "2024-08-29T17:01:58.480298",
     "exception": false,
     "start_time": "2024-08-29T17:01:58.059917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions for 'what a badass char is arthur, he is the best game char ever made, i luv rdr2': [[0.00858516 0.00519679 0.01073068 0.807425   0.04654592 0.0220181\n",
      "  0.18400446]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"d9839eea-6ab2-4252-8171-c960f2c3bb54\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d9839eea-6ab2-4252-8171-c960f2c3bb54\")) {                    Plotly.newPlot(                        \"d9839eea-6ab2-4252-8171-c960f2c3bb54\",                        [{\"hovertemplate\":\"r=%{r}\\u003cbr\\u003etheta=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"r\":[0.008585161,0.0051967856,0.010730677,0.807425,0.046545923,0.022018101,0.18400446,0.008585161],\"showlegend\":false,\"subplot\":\"polar\",\"theta\":[\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\",\"anger\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('d9839eea-6ab2-4252-8171-c960f2c3bb54');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"what a badass char is arthur, he is the best game char ever made, i luv rdr2\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted emotions for '{sample_text}': {predictions}\")\n",
    "\n",
    "predictions_array = predictions.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "\n",
    "emotion_moodtags = []\n",
    "for items in predictions_array:\n",
    "    emotion_moodtags.append(items)\n",
    "\n",
    "fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "fig.show()\n",
    "\n",
    "# fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "# fig.show()\n",
    "\n",
    "    \n",
    "# Since predictions are probabilistic values, normalize to ensure they sum up to 1\n",
    "normalized_predictions = predictions_array / predictions_array.sum()  # Normalize the values\n",
    "\n",
    "## Horizontal Stacked Bar Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "left = 0\n",
    "for i in range(len(normalized_predictions)):\n",
    "    ax.barh(0, normalized_predictions[i], color=plt.cm.tab10(i), left=left, label=EMOTION_LABELS[i])\n",
    "    left += normalized_predictions[i]\n",
    "\n",
    "# Add legend\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(EMOTION_LABELS))\n",
    "\n",
    "# Add title\n",
    "plt.title('Emotion Prediction Distribution')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d90d1727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:58.584759Z",
     "iopub.status.busy": "2024-08-29T17:01:58.584395Z",
     "iopub.status.idle": "2024-08-29T17:01:58.955559Z",
     "shell.execute_reply": "2024-08-29T17:01:58.954662Z"
    },
    "papermill": {
     "duration": 0.4252,
     "end_time": "2024-08-29T17:01:58.957719",
     "exception": false,
     "start_time": "2024-08-29T17:01:58.532519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions for 'I don't no fr y hes sooo sad.': [[0.0152045  0.00364618 0.00738286 0.01595281 0.01736875 0.95293915\n",
      "  0.01861271]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"5efb7159-a099-4726-a8d0-592ca07eaaac\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5efb7159-a099-4726-a8d0-592ca07eaaac\")) {                    Plotly.newPlot(                        \"5efb7159-a099-4726-a8d0-592ca07eaaac\",                        [{\"hovertemplate\":\"r=%{r}\\u003cbr\\u003etheta=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"r\":[0.0152044995,0.0036461758,0.0073828558,0.01595281,0.017368747,0.95293915,0.01861271,0.0152044995],\"showlegend\":false,\"subplot\":\"polar\",\"theta\":[\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\",\"anger\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5efb7159-a099-4726-a8d0-592ca07eaaac');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"I don't no fr y hes sooo sad.\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted emotions for '{sample_text}': {predictions}\")\n",
    "\n",
    "predictions_array = predictions.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "\n",
    "emotion_moodtags = []\n",
    "for items in predictions_array:\n",
    "    emotion_moodtags.append(items)\n",
    "\n",
    "fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "fig.show()\n",
    "\n",
    "# fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "# fig.show()\n",
    "\n",
    "    \n",
    "# Since predictions are probabilistic values, normalize to ensure they sum up to 1\n",
    "normalized_predictions = predictions_array / predictions_array.sum()  # Normalize the values\n",
    "\n",
    "## Horizontal Stacked Bar Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "left = 0\n",
    "for i in range(len(normalized_predictions)):\n",
    "    ax.barh(0, normalized_predictions[i], color=plt.cm.tab10(i), left=left, label=EMOTION_LABELS[i])\n",
    "    left += normalized_predictions[i]\n",
    "\n",
    "# Add legend\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(EMOTION_LABELS))\n",
    "\n",
    "# Add title\n",
    "plt.title('Emotion Prediction Distribution')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d80c628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:59.062137Z",
     "iopub.status.busy": "2024-08-29T17:01:59.061795Z",
     "iopub.status.idle": "2024-08-29T17:01:59.065809Z",
     "shell.execute_reply": "2024-08-29T17:01:59.065038Z"
    },
    "papermill": {
     "duration": 0.058178,
     "end_time": "2024-08-29T17:01:59.067623",
     "exception": false,
     "start_time": "2024-08-29T17:01:59.009445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "# predictions = predict(sample_text, best_model, tokenizer, max_len=128, device=device)\n",
    "# print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0844f9bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:59.171834Z",
     "iopub.status.busy": "2024-08-29T17:01:59.171528Z",
     "iopub.status.idle": "2024-08-29T17:01:59.175600Z",
     "shell.execute_reply": "2024-08-29T17:01:59.174695Z"
    },
    "papermill": {
     "duration": 0.05903,
     "end_time": "2024-08-29T17:01:59.177496",
     "exception": false,
     "start_time": "2024-08-29T17:01:59.118466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "# predictions = predict(sample_text, best_model, tokenizer, max_len=128, device=device)\n",
    "# print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "17c0d8a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T17:01:59.284338Z",
     "iopub.status.busy": "2024-08-29T17:01:59.283657Z",
     "iopub.status.idle": "2024-08-29T17:01:59.287724Z",
     "shell.execute_reply": "2024-08-29T17:01:59.286824Z"
    },
    "papermill": {
     "duration": 0.057777,
     "end_time": "2024-08-29T17:01:59.289539",
     "exception": false,
     "start_time": "2024-08-29T17:01:59.231762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "# best_model = RoBERTaEmotionModel('roberta-base').to(device)\n",
    "# best_model.load_state_dict(torch.load(\"best_model.pth\"))  # Assuming best model is saved during training\n",
    "# predicted_emotions = predict_emotions(best_model, sample_text, tokenizer, best_params['max_len'], device)\n",
    "# print(predicted_emotions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5557640,
     "sourceId": 9273793,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19467.983838,
   "end_time": "2024-08-29T17:02:04.664357",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-29T11:37:36.680519",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00653afd6d544febb08fd86cce7563df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "06c99010c86a46efa9da7417bdea0346": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "084a2ab988824327b03f3162afe69dbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2bc0edfce8444e51944d301772c360bd",
       "placeholder": "​",
       "style": "IPY_MODEL_06c99010c86a46efa9da7417bdea0346",
       "value": " 579/579 [00:00&lt;00:00, 47.9kB/s]"
      }
     },
     "086f14a817a84311a8e6342c10d8ab23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "14c832b0e97943b886a2a1e820f8fef9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c9c1498cbbce4d32a59c08cb0a926f4e",
        "IPY_MODEL_195e3a1d998943dd9b7f157c99d02bb5",
        "IPY_MODEL_d1e6d7f0eccf44b99550a7632cb27f06"
       ],
       "layout": "IPY_MODEL_086f14a817a84311a8e6342c10d8ab23"
      }
     },
     "1532bee6085940b7a8982ccf2b7a75e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16e5e7263b9240a48736502bc4b3e1c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "195e3a1d998943dd9b7f157c99d02bb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ce2fac228c4d43e59f39136428328d05",
       "max": 2464616.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5d6ef1cf3f6948b895ef3fb5d20c1b0a",
       "value": 2464616.0
      }
     },
     "1c3cc510f3ff4b5b80541d73d98027f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1d2ea7ef4bbc43d98eba568534a46575": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e332006586547d893f1fe2b52912eee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2579e9c607f840e797900663da0da819",
        "IPY_MODEL_2ca1d70c145a4930a8b82253f4ff2b16",
        "IPY_MODEL_084a2ab988824327b03f3162afe69dbb"
       ],
       "layout": "IPY_MODEL_8720d5dd52544908a33c562cd722637c"
      }
     },
     "2579e9c607f840e797900663da0da819": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_58bcf4e891af407eadd374800338c390",
       "placeholder": "​",
       "style": "IPY_MODEL_64f7b23f84874ddb910c308c74bc3c81",
       "value": "config.json: 100%"
      }
     },
     "2b9b5ad0680e49a9a9e3a2a25f0fa045": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1d2ea7ef4bbc43d98eba568534a46575",
       "placeholder": "​",
       "style": "IPY_MODEL_44b58dd5aed640fbb97bda42b5332751",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "2bc0edfce8444e51944d301772c360bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2ca1d70c145a4930a8b82253f4ff2b16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ab1b2569c18d40dab8380a4f5c59137d",
       "max": 579.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bf5f9c90f1a445869ee290ac0bb4d8c1",
       "value": 579.0
      }
     },
     "34c9031e05c94da097ea37953f26822b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "44b58dd5aed640fbb97bda42b5332751": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "46ae0d1453a74ce9af927ae3b2c7a3aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b8f0578f4474aa9a776fddb4f74ef40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2b9b5ad0680e49a9a9e3a2a25f0fa045",
        "IPY_MODEL_9a5fdc6e587a419eba1e7486466a42b2",
        "IPY_MODEL_f80e2ff84f3f4b078c8decdced7494fd"
       ],
       "layout": "IPY_MODEL_46ae0d1453a74ce9af927ae3b2c7a3aa"
      }
     },
     "58bcf4e891af407eadd374800338c390": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d6ef1cf3f6948b895ef3fb5d20c1b0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "64f7b23f84874ddb910c308c74bc3c81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8720d5dd52544908a33c562cd722637c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9a5fdc6e587a419eba1e7486466a42b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16e5e7263b9240a48736502bc4b3e1c3",
       "max": 52.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fbe0853405e143d2858fc6fc3ffb256f",
       "value": 52.0
      }
     },
     "ab1b2569c18d40dab8380a4f5c59137d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf5f9c90f1a445869ee290ac0bb4d8c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c9c1498cbbce4d32a59c08cb0a926f4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c9f24596e1604589a4e7f0eab69bb96c",
       "placeholder": "​",
       "style": "IPY_MODEL_00653afd6d544febb08fd86cce7563df",
       "value": "spm.model: 100%"
      }
     },
     "c9f24596e1604589a4e7f0eab69bb96c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce2fac228c4d43e59f39136428328d05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1e6d7f0eccf44b99550a7632cb27f06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1532bee6085940b7a8982ccf2b7a75e1",
       "placeholder": "​",
       "style": "IPY_MODEL_f06346de2e164467b0bfea9994c012f5",
       "value": " 2.46M/2.46M [00:00&lt;00:00, 12.4MB/s]"
      }
     },
     "f06346de2e164467b0bfea9994c012f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f80e2ff84f3f4b078c8decdced7494fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_34c9031e05c94da097ea37953f26822b",
       "placeholder": "​",
       "style": "IPY_MODEL_1c3cc510f3ff4b5b80541d73d98027f3",
       "value": " 52.0/52.0 [00:00&lt;00:00, 3.89kB/s]"
      }
     },
     "fbe0853405e143d2858fc6fc3ffb256f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
