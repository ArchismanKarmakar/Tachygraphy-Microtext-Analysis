{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc56c0a1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:52.742380Z",
     "iopub.status.busy": "2024-08-31T04:19:52.741724Z",
     "iopub.status.idle": "2024-08-31T04:19:53.531872Z",
     "shell.execute_reply": "2024-08-31T04:19:53.530321Z"
    },
    "papermill": {
     "duration": 0.821621,
     "end_time": "2024-08-31T04:19:53.533961",
     "exception": false,
     "start_time": "2024-08-31T04:19:52.712340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_EmotionMoodtags_Dataset.csv\n",
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_dataset_main.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911394d",
   "metadata": {
    "papermill": {
     "duration": 0.025764,
     "end_time": "2024-08-31T04:19:53.586328",
     "exception": false,
     "start_time": "2024-08-31T04:19:53.560564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATASET & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc47a89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:53.639755Z",
     "iopub.status.busy": "2024-08-31T04:19:53.639284Z",
     "iopub.status.idle": "2024-08-31T04:19:53.880255Z",
     "shell.execute_reply": "2024-08-31T04:19:53.879346Z"
    },
    "papermill": {
     "duration": 0.269641,
     "end_time": "2024-08-31T04:19:53.882277",
     "exception": false,
     "start_time": "2024-08-31T04:19:53.612636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For emoji cleaning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "'''For emoji cleaning'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87167d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:53.935907Z",
     "iopub.status.busy": "2024-08-31T04:19:53.935512Z",
     "iopub.status.idle": "2024-08-31T04:19:53.977260Z",
     "shell.execute_reply": "2024-08-31T04:19:53.976366Z"
    },
    "papermill": {
     "duration": 0.070389,
     "end_time": "2024-08-31T04:19:53.979140",
     "exception": false,
     "start_time": "2024-08-31T04:19:53.908751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/kaggle/input/dataset-tachygraphy/Tachygraphy_dataset_main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9675f6ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.032780Z",
     "iopub.status.busy": "2024-08-31T04:19:54.032485Z",
     "iopub.status.idle": "2024-08-31T04:19:54.036292Z",
     "shell.execute_reply": "2024-08-31T04:19:54.035456Z"
    },
    "papermill": {
     "duration": 0.032925,
     "end_time": "2024-08-31T04:19:54.038150",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.005225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d09456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.091169Z",
     "iopub.status.busy": "2024-08-31T04:19:54.090902Z",
     "iopub.status.idle": "2024-08-31T04:19:54.099487Z",
     "shell.execute_reply": "2024-08-31T04:19:54.098619Z"
    },
    "papermill": {
     "duration": 0.037423,
     "end_time": "2024-08-31T04:19:54.101395",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.063972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text         1\n",
      "Meaning      1\n",
      "Sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e11fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.155565Z",
     "iopub.status.busy": "2024-08-31T04:19:54.155232Z",
     "iopub.status.idle": "2024-08-31T04:19:54.177714Z",
     "shell.execute_reply": "2024-08-31T04:19:54.176885Z"
    },
    "papermill": {
     "duration": 0.051507,
     "end_time": "2024-08-31T04:19:54.179467",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.127960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
    "\n",
    "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
    "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
    "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "                'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd9286b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.232030Z",
     "iopub.status.busy": "2024-08-31T04:19:54.231748Z",
     "iopub.status.idle": "2024-08-31T04:19:54.244108Z",
     "shell.execute_reply": "2024-08-31T04:19:54.243274Z"
    },
    "papermill": {
     "duration": 0.040798,
     "end_time": "2024-08-31T04:19:54.245961",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.205163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "    text = str(text).lower()    #Making Text Lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for word in mapping.keys():\n",
    "        if \"\"+word+\"\" in text:\n",
    "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    '''Cleans special characters present(if any)'''   \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    '''Corrects common spelling errors'''   \n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1088ed4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.298563Z",
     "iopub.status.busy": "2024-08-31T04:19:54.298240Z",
     "iopub.status.idle": "2024-08-31T04:19:54.302985Z",
     "shell.execute_reply": "2024-08-31T04:19:54.302151Z"
    },
    "papermill": {
     "duration": 0.033217,
     "end_time": "2024-08-31T04:19:54.304939",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.271722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_text(text)\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_special_chars(text, punct, punct_mapping)\n",
    "#     text = correct_spelling(text, mispell_dict)\n",
    "    text = remove_space(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c14b0a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.357770Z",
     "iopub.status.busy": "2024-08-31T04:19:54.357477Z",
     "iopub.status.idle": "2024-08-31T04:19:54.366711Z",
     "shell.execute_reply": "2024-08-31T04:19:54.365928Z"
    },
    "papermill": {
     "duration": 0.037773,
     "end_time": "2024-08-31T04:19:54.368577",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.330804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Text':'text', 'Sentiment':'sentiment_polarity'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4686c851",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.421776Z",
     "iopub.status.busy": "2024-08-31T04:19:54.421482Z",
     "iopub.status.idle": "2024-08-31T04:19:54.430683Z",
     "shell.execute_reply": "2024-08-31T04:19:54.429926Z"
    },
    "papermill": {
     "duration": 0.037614,
     "end_time": "2024-08-31T04:19:54.432451",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.394837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Meaning'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e04ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.485884Z",
     "iopub.status.busy": "2024-08-31T04:19:54.485297Z",
     "iopub.status.idle": "2024-08-31T04:19:54.493193Z",
     "shell.execute_reply": "2024-08-31T04:19:54.492469Z"
    },
    "papermill": {
     "duration": 0.036619,
     "end_time": "2024-08-31T04:19:54.495114",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.458495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['text', 'sentiment_polarity'], inplace=True) # Dropping NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5102565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.548509Z",
     "iopub.status.busy": "2024-08-31T04:19:54.548154Z",
     "iopub.status.idle": "2024-08-31T04:19:54.568425Z",
     "shell.execute_reply": "2024-08-31T04:19:54.567638Z"
    },
    "papermill": {
     "duration": 0.048897,
     "end_time": "2024-08-31T04:19:54.570337",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.521440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4957 entries, 0 to 4957\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   text                4957 non-null   object\n",
      " 1   sentiment_polarity  4957 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 116.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "# df['text'] = df['text'].astype('str')\n",
    "# df['sentiment_polarity'] = df['sentiment_polarity'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c2f4cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:54.623676Z",
     "iopub.status.busy": "2024-08-31T04:19:54.623371Z",
     "iopub.status.idle": "2024-08-31T04:19:58.651650Z",
     "shell.execute_reply": "2024-08-31T04:19:58.650893Z"
    },
    "papermill": {
     "duration": 4.057096,
     "end_time": "2024-08-31T04:19:58.653754",
     "exception": false,
     "start_time": "2024-08-31T04:19:54.596658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: text_preprocessing_pipeline(x))\n",
    "df['sentiment_polarity'] = df['sentiment_polarity'].apply(lambda x: text_preprocessing_pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1062d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:58.718002Z",
     "iopub.status.busy": "2024-08-31T04:19:58.717239Z",
     "iopub.status.idle": "2024-08-31T04:19:58.731605Z",
     "shell.execute_reply": "2024-08-31T04:19:58.730714Z"
    },
    "papermill": {
     "duration": 0.053317,
     "end_time": "2024-08-31T04:19:58.733968",
     "exception": false,
     "start_time": "2024-08-31T04:19:58.680651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>last session of the day</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shanghai is also really exciting precisely sky...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>submit the report asap</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the ogs i like it</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>make a pet face wtf wrong with me tonight haha</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>i dnt care anymore boyz is not worth d drama</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>no relationship is perfect tho me bae goo from...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>over here tryna get my nail polishes and shit lol</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>no one was loved d way i luv u</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4957 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment_polarity\n",
       "0                               last session of the day            neutral\n",
       "1     shanghai is also really exciting precisely sky...           positive\n",
       "2                                submit the report asap           negative\n",
       "3                                            happy bday           positive\n",
       "4                                     the ogs i like it           positive\n",
       "...                                                 ...                ...\n",
       "4953     make a pet face wtf wrong with me tonight haha           negative\n",
       "4954       i dnt care anymore boyz is not worth d drama           negative\n",
       "4955  no relationship is perfect tho me bae goo from...           negative\n",
       "4956  over here tryna get my nail polishes and shit lol           negative\n",
       "4957                     no one was loved d way i luv u           positive\n",
       "\n",
       "[4957 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfbbcd62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:58.798835Z",
     "iopub.status.busy": "2024-08-31T04:19:58.798520Z",
     "iopub.status.idle": "2024-08-31T04:19:58.807582Z",
     "shell.execute_reply": "2024-08-31T04:19:58.806713Z"
    },
    "papermill": {
     "duration": 0.038884,
     "end_time": "2024-08-31T04:19:58.809946",
     "exception": false,
     "start_time": "2024-08-31T04:19:58.771062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_polar = pd.get_dummies(df['sentiment_polarity'], prefix='', prefix_sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64542030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:58.879116Z",
     "iopub.status.busy": "2024-08-31T04:19:58.878809Z",
     "iopub.status.idle": "2024-08-31T04:19:58.883290Z",
     "shell.execute_reply": "2024-08-31T04:19:58.882364Z"
    },
    "papermill": {
     "duration": 0.042335,
     "end_time": "2024-08-31T04:19:58.885622",
     "exception": false,
     "start_time": "2024-08-31T04:19:58.843287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_polar = bin_polar.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "804c5d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:58.986165Z",
     "iopub.status.busy": "2024-08-31T04:19:58.985611Z",
     "iopub.status.idle": "2024-08-31T04:19:58.999705Z",
     "shell.execute_reply": "2024-08-31T04:19:58.998699Z"
    },
    "papermill": {
     "duration": 0.048961,
     "end_time": "2024-08-31T04:19:59.001665",
     "exception": false,
     "start_time": "2024-08-31T04:19:58.952704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4957 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      negative  neutral  positive\n",
       "0            0        1         0\n",
       "1            0        0         1\n",
       "2            1        0         0\n",
       "3            0        0         1\n",
       "4            0        0         1\n",
       "...        ...      ...       ...\n",
       "4953         1        0         0\n",
       "4954         1        0         0\n",
       "4955         1        0         0\n",
       "4956         1        0         0\n",
       "4957         0        0         1\n",
       "\n",
       "[4957 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1267b2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.057252Z",
     "iopub.status.busy": "2024-08-31T04:19:59.056516Z",
     "iopub.status.idle": "2024-08-31T04:19:59.070742Z",
     "shell.execute_reply": "2024-08-31T04:19:59.069694Z"
    },
    "papermill": {
     "duration": 0.043861,
     "end_time": "2024-08-31T04:19:59.072825",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.028964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>last session of the day</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shanghai is also really exciting precisely sky...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>submit the report asap</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the ogs i like it</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment_polarity  \\\n",
       "0                            last session of the day            neutral   \n",
       "1  shanghai is also really exciting precisely sky...           positive   \n",
       "2                             submit the report asap           negative   \n",
       "3                                         happy bday           positive   \n",
       "4                                  the ogs i like it           positive   \n",
       "\n",
       "   negative  neutral  positive  \n",
       "0         0        1         0  \n",
       "1         0        0         1  \n",
       "2         1        0         0  \n",
       "3         0        0         1  \n",
       "4         0        0         1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, bin_polar], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25b81387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.130671Z",
     "iopub.status.busy": "2024-08-31T04:19:59.129897Z",
     "iopub.status.idle": "2024-08-31T04:19:59.142340Z",
     "shell.execute_reply": "2024-08-31T04:19:59.141434Z"
    },
    "papermill": {
     "duration": 0.045012,
     "end_time": "2024-08-31T04:19:59.144905",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.099893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>last session of the day</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shanghai is also really exciting precisely sky...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>submit the report asap</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the ogs i like it</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment_polarity  \\\n",
       "0                            last session of the day            neutral   \n",
       "1  shanghai is also really exciting precisely sky...           positive   \n",
       "2                             submit the report asap           negative   \n",
       "3                                         happy bday           positive   \n",
       "4                                  the ogs i like it           positive   \n",
       "\n",
       "   negative  neutral  positive  \n",
       "0         0        1         0  \n",
       "1         0        0         1  \n",
       "2         1        0         0  \n",
       "3         0        0         1  \n",
       "4         0        0         1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.drop(columns=['sentiment_polarity'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1ed71c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.203184Z",
     "iopub.status.busy": "2024-08-31T04:19:59.202465Z",
     "iopub.status.idle": "2024-08-31T04:19:59.215136Z",
     "shell.execute_reply": "2024-08-31T04:19:59.213824Z"
    },
    "papermill": {
     "duration": 0.04277,
     "end_time": "2024-08-31T04:19:59.217082",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.174312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4957 entries, 0 to 4957\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   text                4957 non-null   object\n",
      " 1   sentiment_polarity  4957 non-null   object\n",
      " 2   negative            4957 non-null   int64 \n",
      " 3   neutral             4957 non-null   int64 \n",
      " 4   positive            4957 non-null   int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 232.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e8792d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.272663Z",
     "iopub.status.busy": "2024-08-31T04:19:59.272267Z",
     "iopub.status.idle": "2024-08-31T04:19:59.276661Z",
     "shell.execute_reply": "2024-08-31T04:19:59.275752Z"
    },
    "papermill": {
     "duration": 0.03431,
     "end_time": "2024-08-31T04:19:59.278582",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.244272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_polarity_label_mapping = {\n",
    "    0: \"negative\", 1: \"neutral\", 2: \"positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8ebf19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.334700Z",
     "iopub.status.busy": "2024-08-31T04:19:59.333919Z",
     "iopub.status.idle": "2024-08-31T04:19:59.339887Z",
     "shell.execute_reply": "2024-08-31T04:19:59.339126Z"
    },
    "papermill": {
     "duration": 0.036293,
     "end_time": "2024-08-31T04:19:59.342058",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.305765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_polarity_label_mapping_rev = {\n",
    "    'negative': 0, 'neutral': 1, 'positive': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd6c2952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.402041Z",
     "iopub.status.busy": "2024-08-31T04:19:59.401692Z",
     "iopub.status.idle": "2024-08-31T04:19:59.405840Z",
     "shell.execute_reply": "2024-08-31T04:19:59.404988Z"
    },
    "papermill": {
     "duration": 0.034881,
     "end_time": "2024-08-31T04:19:59.408148",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.373267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SENTIMENT_POLARITY_LABELS = [\n",
    "    \"negative\", \"neutral\", \"positive\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "811caac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.465895Z",
     "iopub.status.busy": "2024-08-31T04:19:59.465617Z",
     "iopub.status.idle": "2024-08-31T04:19:59.470147Z",
     "shell.execute_reply": "2024-08-31T04:19:59.469232Z"
    },
    "papermill": {
     "duration": 0.034423,
     "end_time": "2024-08-31T04:19:59.472094",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.437671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_train_split(dataset, test_ratio=0.1):\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  return dataset[~test_indices], dataset[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e95bc05f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.528035Z",
     "iopub.status.busy": "2024-08-31T04:19:59.527469Z",
     "iopub.status.idle": "2024-08-31T04:19:59.535060Z",
     "shell.execute_reply": "2024-08-31T04:19:59.534150Z"
    },
    "papermill": {
     "duration": 0.037636,
     "end_time": "2024-08-31T04:19:59.536968",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.499332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4446 examples in training, 511 examples in testing.\n"
     ]
    }
   ],
   "source": [
    "train_ds_pd, validation_ds_pd = test_train_split(df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_ds_pd), len(validation_ds_pd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5346d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.593080Z",
     "iopub.status.busy": "2024-08-31T04:19:59.592801Z",
     "iopub.status.idle": "2024-08-31T04:19:59.597882Z",
     "shell.execute_reply": "2024-08-31T04:19:59.597103Z"
    },
    "papermill": {
     "duration": 0.035323,
     "end_time": "2024-08-31T04:19:59.599738",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.564415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_pd = train_ds_pd.reset_index(drop=True)\n",
    "validation_ds_pd = validation_ds_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13dc86a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.655963Z",
     "iopub.status.busy": "2024-08-31T04:19:59.655360Z",
     "iopub.status.idle": "2024-08-31T04:19:59.667044Z",
     "shell.execute_reply": "2024-08-31T04:19:59.666176Z"
    },
    "papermill": {
     "duration": 0.042273,
     "end_time": "2024-08-31T04:19:59.669327",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.627054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>last session of the day</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy bday</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the ogs i like it</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that is great weee visitors</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think everyone hates me on here lol</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text sentiment_polarity  negative  \\\n",
       "0                last session of the day            neutral         0   \n",
       "1                             happy bday           positive         0   \n",
       "2                      the ogs i like it           positive         0   \n",
       "3            that is great weee visitors           positive         0   \n",
       "4  i think everyone hates me on here lol           negative         1   \n",
       "\n",
       "   neutral  positive  \n",
       "0        1         0  \n",
       "1        0         1  \n",
       "2        0         1  \n",
       "3        0         1  \n",
       "4        0         0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6d435ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.733844Z",
     "iopub.status.busy": "2024-08-31T04:19:59.733181Z",
     "iopub.status.idle": "2024-08-31T04:19:59.740856Z",
     "shell.execute_reply": "2024-08-31T04:19:59.739951Z"
    },
    "papermill": {
     "duration": 0.040487,
     "end_time": "2024-08-31T04:19:59.742876",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.702389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'positive', 'positive', ..., 'negative', 'negative',\n",
       "       'positive'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_labels = np.array(train_ds_pd['sentiment_polarity'])\n",
    "sentiment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f882c922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.801731Z",
     "iopub.status.busy": "2024-08-31T04:19:59.801365Z",
     "iopub.status.idle": "2024-08-31T04:19:59.807867Z",
     "shell.execute_reply": "2024-08-31T04:19:59.807008Z"
    },
    "papermill": {
     "duration": 0.0369,
     "end_time": "2024-08-31T04:19:59.809792",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.772892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_pd.drop(columns=['sentiment_polarity'], inplace=True)\n",
    "validation_ds_pd.drop(columns=['sentiment_polarity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72fab591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.869017Z",
     "iopub.status.busy": "2024-08-31T04:19:59.868168Z",
     "iopub.status.idle": "2024-08-31T04:19:59.879586Z",
     "shell.execute_reply": "2024-08-31T04:19:59.878637Z"
    },
    "papermill": {
     "duration": 0.042849,
     "end_time": "2024-08-31T04:19:59.881645",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.838796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>last session of the day</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy bday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the ogs i like it</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that is great weee visitors</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think everyone hates me on here lol</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text  negative  neutral  positive\n",
       "0                last session of the day         0        1         0\n",
       "1                             happy bday         0        0         1\n",
       "2                      the ogs i like it         0        0         1\n",
       "3            that is great weee visitors         0        0         1\n",
       "4  i think everyone hates me on here lol         1        0         0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "458e2dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:19:59.939153Z",
     "iopub.status.busy": "2024-08-31T04:19:59.938811Z",
     "iopub.status.idle": "2024-08-31T04:19:59.949535Z",
     "shell.execute_reply": "2024-08-31T04:19:59.948664Z"
    },
    "papermill": {
     "duration": 0.041717,
     "end_time": "2024-08-31T04:19:59.951365",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.909648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shanghai is also really exciting precisely sky...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>submit the report asap</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i checked we did not win</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and you are on twitter did the tavern bore you...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you guys did not say hi or answer my questions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  negative  neutral  \\\n",
       "0  shanghai is also really exciting precisely sky...         0        0   \n",
       "1                             submit the report asap         1        0   \n",
       "2                           i checked we did not win         0        1   \n",
       "3  and you are on twitter did the tavern bore you...         0        1   \n",
       "4  you guys did not say hi or answer my questions...         0        0   \n",
       "\n",
       "   positive  \n",
       "0         1  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eff2eb84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:00.008926Z",
     "iopub.status.busy": "2024-08-31T04:20:00.008192Z",
     "iopub.status.idle": "2024-08-31T04:20:00.016887Z",
     "shell.execute_reply": "2024-08-31T04:20:00.015966Z"
    },
    "papermill": {
     "duration": 0.03958,
     "end_time": "2024-08-31T04:20:00.018787",
     "exception": false,
     "start_time": "2024-08-31T04:19:59.979207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, ..., 0, 0, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_indexed_labels = np.array([sentiment_polarity_label_mapping_rev[label] for label in sentiment_labels])\n",
    "sentiment_indexed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0db5c654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:00.078632Z",
     "iopub.status.busy": "2024-08-31T04:20:00.078340Z",
     "iopub.status.idle": "2024-08-31T04:20:03.504168Z",
     "shell.execute_reply": "2024-08-31T04:20:03.503385Z"
    },
    "papermill": {
     "duration": 3.45772,
     "end_time": "2024-08-31T04:20:03.506482",
     "exception": false,
     "start_time": "2024-08-31T04:20:00.048762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc86412",
   "metadata": {
    "papermill": {
     "duration": 0.027923,
     "end_time": "2024-08-31T04:20:03.563566",
     "exception": false,
     "start_time": "2024-08-31T04:20:03.535643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Calculating Class Weights for each labels to avoid imbalanced distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5470262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:03.622308Z",
     "iopub.status.busy": "2024-08-31T04:20:03.621383Z",
     "iopub.status.idle": "2024-08-31T04:20:03.726957Z",
     "shell.execute_reply": "2024-08-31T04:20:03.725819Z"
    },
    "papermill": {
     "duration": 0.136869,
     "end_time": "2024-08-31T04:20:03.729041",
     "exception": false,
     "start_time": "2024-08-31T04:20:03.592172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3513, 0.3134, 0.3353])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = np.bincount(sentiment_indexed_labels)\n",
    "total_samples = len(sentiment_labels)\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf785ef",
   "metadata": {
    "papermill": {
     "duration": 0.029237,
     "end_time": "2024-08-31T04:20:03.787015",
     "exception": false,
     "start_time": "2024-08-31T04:20:03.757778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Class Weight NOTE\n",
    "### This class weights are for the training dataset and are to be used while training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea3478",
   "metadata": {
    "papermill": {
     "duration": 0.028243,
     "end_time": "2024-08-31T04:20:03.843685",
     "exception": false,
     "start_time": "2024-08-31T04:20:03.815442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SETTING CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70616094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:03.901654Z",
     "iopub.status.busy": "2024-08-31T04:20:03.901303Z",
     "iopub.status.idle": "2024-08-31T04:20:03.984970Z",
     "shell.execute_reply": "2024-08-31T04:20:03.984174Z"
    },
    "papermill": {
     "duration": 0.114625,
     "end_time": "2024-08-31T04:20:03.986879",
     "exception": false,
     "start_time": "2024-08-31T04:20:03.872254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48b7cb06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:04.045006Z",
     "iopub.status.busy": "2024-08-31T04:20:04.044699Z",
     "iopub.status.idle": "2024-08-31T04:20:06.494806Z",
     "shell.execute_reply": "2024-08-31T04:20:06.493847Z"
    },
    "papermill": {
     "duration": 2.481581,
     "end_time": "2024-08-31T04:20:06.497048",
     "exception": false,
     "start_time": "2024-08-31T04:20:04.015467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import DebertaV2Model, DebertaV2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7e24c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:06.555831Z",
     "iopub.status.busy": "2024-08-31T04:20:06.554994Z",
     "iopub.status.idle": "2024-08-31T04:20:08.189281Z",
     "shell.execute_reply": "2024-08-31T04:20:08.188307Z"
    },
    "papermill": {
     "duration": 1.665707,
     "end_time": "2024-08-31T04:20:08.191495",
     "exception": false,
     "start_time": "2024-08-31T04:20:06.525788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b49e04ae8644cb1be96f27ab45020d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddb2741be6147bebd1e914e134e9c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6caf5ece1c4d58be966be62c98f8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d38187",
   "metadata": {
    "papermill": {
     "duration": 0.028991,
     "end_time": "2024-08-31T04:20:08.249954",
     "exception": false,
     "start_time": "2024-08-31T04:20:08.220963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TORCH IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bebdeb36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:08.309704Z",
     "iopub.status.busy": "2024-08-31T04:20:08.309321Z",
     "iopub.status.idle": "2024-08-31T04:20:24.987395Z",
     "shell.execute_reply": "2024-08-31T04:20:24.986614Z"
    },
    "papermill": {
     "duration": 16.710638,
     "end_time": "2024-08-31T04:20:24.989708",
     "exception": false,
     "start_time": "2024-08-31T04:20:08.279070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 04:20:10,312\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-08-31 04:20:10,851\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "# from optuna.integration import PyTorchLightningPruner\n",
    "from ray import tune\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "# from ray.tune.integration.pytorch import TuneReportCallback\n",
    "from torch.amp import GradScaler, autocast\n",
    "# from ray.tune.integration.optuna import OptunaSearch\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from torch import autocast\n",
    "# from ray import tune\n",
    "# from ray.tune.integration.tensorboard import TensorBoardReporter\n",
    "from ray.tune.logger import TBXLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ray.train import report\n",
    "# from ray.tune.integration.jupyter import JupyterNotebookReporter\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from ray.tune.schedulers import HyperBandScheduler, AsyncHyperBandScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e52b9",
   "metadata": {
    "papermill": {
     "duration": 0.029187,
     "end_time": "2024-08-31T04:20:25.048953",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.019766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATASET CONFIG & ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9cec663",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:25.108635Z",
     "iopub.status.busy": "2024-08-31T04:20:25.107893Z",
     "iopub.status.idle": "2024-08-31T04:20:25.116508Z",
     "shell.execute_reply": "2024-08-31T04:20:25.115644Z"
    },
    "papermill": {
     "duration": 0.040496,
     "end_time": "2024-08-31T04:20:25.118456",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.077960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         text = self.data.iloc[index, 0]\n",
    "#         labels = self.data.iloc[index, 1:].values.astype(float)\n",
    "        text = str(self.data.iloc[index]['text'])\n",
    "        labels = self.data.iloc[index][['negative', 'neutral', 'positive']].values.astype(np.float32)\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04688113",
   "metadata": {
    "papermill": {
     "duration": 0.028902,
     "end_time": "2024-08-31T04:20:25.176291",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.147389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd42c98c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:25.235507Z",
     "iopub.status.busy": "2024-08-31T04:20:25.235196Z",
     "iopub.status.idle": "2024-08-31T04:20:25.242106Z",
     "shell.execute_reply": "2024-08-31T04:20:25.241378Z"
    },
    "papermill": {
     "duration": 0.038747,
     "end_time": "2024-08-31T04:20:25.244011",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.205264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.roberta = roberta_model\n",
    "        self.drop = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.roberta.config.hidden_size, 256)  # Reduced neurons\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(256, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the RoBERTa output\n",
    "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token_state = output.last_hidden_state[:, 0, :]\n",
    "        # Pass through the custom layers\n",
    "        output = self.drop(cls_token_state)\n",
    "        output = self.relu(self.fc1(output))\n",
    "#         output = self.drop(output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee01a2a",
   "metadata": {
    "papermill": {
     "duration": 0.028767,
     "end_time": "2024-08-31T04:20:25.301985",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.273218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Other Models to try out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd0fc8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:25.361412Z",
     "iopub.status.busy": "2024-08-31T04:20:25.361108Z",
     "iopub.status.idle": "2024-08-31T04:20:25.365878Z",
     "shell.execute_reply": "2024-08-31T04:20:25.365063Z"
    },
    "papermill": {
     "duration": 0.036772,
     "end_time": "2024-08-31T04:20:25.367809",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.331037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SentimentModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "#         super(SentimentModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.fc1 = nn.Linear(self.roberta.config.hidden_size, 512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.out = nn.Linear(256, n_classes)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Get the RoBERTa output\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls_token_state = output.last_hidden_state[:, 0, :]\n",
    "#         # Pass through the custom layers\n",
    "#         output = self.drop(cls_token_state)\n",
    "# #         output = cls_token_state\n",
    "#         output = self.relu(self.fc1(output))\n",
    "#         output = self.drop(output)\n",
    "#         output = self.relu(self.fc2(output))\n",
    "# #         output = self.drop(output)\n",
    "#         return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02df36da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:25.426801Z",
     "iopub.status.busy": "2024-08-31T04:20:25.426512Z",
     "iopub.status.idle": "2024-08-31T04:20:25.430381Z",
     "shell.execute_reply": "2024-08-31T04:20:25.429566Z"
    },
    "papermill": {
     "duration": 0.03539,
     "end_time": "2024-08-31T04:20:25.432216",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.396826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class AdvancedPooling(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(AdvancedPooling, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#     def forward(self, hidden_states):\n",
    "#         cls_output = hidden_states[:, 0, :]  # [CLS] token output\n",
    "#         mean_output = hidden_states.mean(dim=1)  # Mean pooling over sequence\n",
    "#         max_output, _ = hidden_states.max(dim=1)  # Max pooling over sequence\n",
    "#         combined_output = torch.cat([cls_output, mean_output, max_output], dim=1)\n",
    "#         return combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a1f86ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:25.491701Z",
     "iopub.status.busy": "2024-08-31T04:20:25.491357Z",
     "iopub.status.idle": "2024-08-31T04:20:25.495895Z",
     "shell.execute_reply": "2024-08-31T04:20:25.495072Z"
    },
    "papermill": {
     "duration": 0.036442,
     "end_time": "2024-08-31T04:20:25.497809",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.461367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SentimentModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "#         super(SentimentModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.fc1 = nn.Linear(self.roberta.config.hidden_size * 3, 512)\n",
    "#         self.attention_pooling = AdvancedPooling(hidden_size=self.roberta.config.hidden_size)\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.out = nn.Linear(256, n_classes)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls_output = output.last_hidden_state[:, 0, :]  # Extract [CLS] token representation\n",
    "#         hidden_states = output.last_hidden_state  # Sequence hidden states\n",
    "#         pooled_output = self.attention_pooling(hidden_states)  # Attention pooling  # Combine CLS and attention pooling\n",
    "#         output = self.drop(pooled_output)\n",
    "#         output = self.relu(self.fc1(output))\n",
    "#         output = self.drop(output)\n",
    "#         output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "#         return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3975754",
   "metadata": {
    "papermill": {
     "duration": 0.029246,
     "end_time": "2024-08-31T04:20:25.555932",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.526686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TRAIN & VALIDATION\n",
    "### Custom metric is a metric for determining the best model free from any overfitting, underfitting. ```custom_metric = (avg_val_loss−val_accuracy) + α × ∣avg_train_loss−avg_val_loss∣ + β × ∣val_accuracy-train_accuracy∣``` This metric balances between low validation loss, high validation accuracy, and penalizing large discrepancies between training and validation loss. Note that, in the model selection we have used ```α = β = 0.5```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce57c0f",
   "metadata": {
    "papermill": {
     "duration": 0.029048,
     "end_time": "2024-08-31T04:20:25.614000",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.584952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### But, if we have probabilistic values as input and output we cannot directly calculate the accuracy, infact we need to rely on loss only. ```custom_metric = avg_val_loss + α × ∣avg_train_loss−avg_val_loss∣```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6e3d503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:25.681815Z",
     "iopub.status.busy": "2024-08-31T04:20:25.681523Z",
     "iopub.status.idle": "2024-08-31T04:20:25.686846Z",
     "shell.execute_reply": "2024-08-31T04:20:25.685729Z"
    },
    "papermill": {
     "duration": 0.045946,
     "end_time": "2024-08-31T04:20:25.688987",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.643041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom metric calculation function\n",
    "def calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy, alpha=0.5, beta=0.5):\n",
    "    loss_diff = abs(avg_train_loss - avg_val_loss)\n",
    "    accuracy_diff = abs(train_accuracy - val_accuracy)\n",
    "    custom_metric = avg_val_loss - val_accuracy + alpha * loss_diff + beta * accuracy_diff\n",
    "    return custom_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f22e1",
   "metadata": {
    "papermill": {
     "duration": 0.030685,
     "end_time": "2024-08-31T04:20:25.749846",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.719161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91cea0dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:25.809881Z",
     "iopub.status.busy": "2024-08-31T04:20:25.809575Z",
     "iopub.status.idle": "2024-08-31T04:20:25.816288Z",
     "shell.execute_reply": "2024-08-31T04:20:25.815471Z"
    },
    "papermill": {
     "duration": 0.038472,
     "end_time": "2024-08-31T04:20:25.818126",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.779654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        EarlyStopping to stop training when a metric has stopped improving.\n",
    "\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss improved to {val_loss:.4f}')\n",
    "#             torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        else:\n",
    "            self.counter += 1\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss did not improve. Patience: {self.patience}, Counter: {self.counter}')\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c79a3c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:25.879490Z",
     "iopub.status.busy": "2024-08-31T04:20:25.879153Z",
     "iopub.status.idle": "2024-08-31T04:20:25.902771Z",
     "shell.execute_reply": "2024-08-31T04:20:25.901934Z"
    },
    "papermill": {
     "duration": 0.055985,
     "end_time": "2024-08-31T04:20:25.904683",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.848698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the training function with variable parameters\n",
    "def train_model(config, train_dataset, val_dataset, device):\n",
    "#     model = SentimentModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=3,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "    \n",
    "    model = SentimentModel(\n",
    "        roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "        n_classes=3,\n",
    "        dropout_rate=config[\"dropout_rate\"]\n",
    "    )\n",
    "    model = model.to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "#     class_weights_tmp = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss(weight=class_weights_tmp)\n",
    "#     criterion = nn.BCELoss()\n",
    "    scaler = GradScaler()  # Initialize GradScaler\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):  # Ensure epochs are passed from config\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            with autocast(device_type=device.type):  # Mixed precision\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = torch.zeros_like(probabilities)\n",
    "            max_indices = torch.argmax(probabilities, dim=1)\n",
    "            predictions.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "            predictions = predictions.float()\n",
    "                \n",
    "#                 loss = criterion(predictions, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "#             outputs = torch.sigmoid(outputs)  \n",
    "            \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "# #             correct_train_preds += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "\n",
    "#             predicted_labels = (outputs > 0.5).float()\n",
    "#             correct_train_preds += (predicted_labels == labels).sum().item()\n",
    "#             total_train_preds += labels.numel()\n",
    "\n",
    "#             correct_train_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "#             total_train_preds += labels.numel()\n",
    "\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "\n",
    "            correct_train_preds += (predictions == labels).sum().item()\n",
    "            total_train_preds += labels.numel()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val_preds = 0\n",
    "        total_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "                predictions = torch.zeros_like(probabilities)\n",
    "                max_indices = torch.argmax(probabilities, dim=1)\n",
    "                predictions.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "                predictions = predictions.float()\n",
    "                \n",
    "#                     loss = criterion(predictions, labels)\n",
    "                    \n",
    "#                 outputs = torch.sigmoid(outputs)\n",
    "                \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "# #                 correct_val_preds += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "    \n",
    "#                 predicted_labels = (outputs > 0.5).float()\n",
    "#                 correct_val_preds += (predicted_labels == labels).sum().item()\n",
    "#                 total_val_preds += labels.numel()\n",
    "\n",
    "#                 correct_val_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "#                 total_val_preds += labels.numel()\n",
    "\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "\n",
    "                correct_val_preds += (predictions == labels).sum().item()\n",
    "                total_val_preds += labels.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_preds / total_val_preds\n",
    "#         custom_metric = avg_val_loss - val_accuracy\n",
    "        custom_metric = calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy)\n",
    "\n",
    "        # Report metrics using ray.train.report\n",
    "        report({\n",
    "            \"loss\": avg_val_loss,\n",
    "            \"accuracy\": val_accuracy,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"custom_metric\": custom_metric,\n",
    "            \"early_stopping_epoch\": epoch + 1,\n",
    "        })\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Check for early stopping\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "\n",
    "        # Save the model checkpoint after each epoch\n",
    "#         trial_dir = os.path.join(os.environ[\"TUNE_TRIAL_DIR\"], \"checkpoint.pt\")\n",
    "#         torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "#         checkpoint_path = f\"/kaggle/working/checkpoint_epoch_{epoch}.pt\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_fn(config):\n",
    "    # Load your data here\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "    train_dataset = SentimentDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "    val_dataset = SentimentDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(config, train_dataset, val_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c0193",
   "metadata": {
    "papermill": {
     "duration": 0.028687,
     "end_time": "2024-08-31T04:20:25.962267",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.933580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEARCH SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a9be7df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:26.021170Z",
     "iopub.status.busy": "2024-08-31T04:20:26.020886Z",
     "iopub.status.idle": "2024-08-31T04:20:26.026676Z",
     "shell.execute_reply": "2024-08-31T04:20:26.025835Z"
    },
    "papermill": {
     "duration": 0.037382,
     "end_time": "2024-08-31T04:20:26.028470",
     "exception": false,
     "start_time": "2024-08-31T04:20:25.991088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'dropout_rate': tune.choice([0.2, 0.25, 0.27, 0.3, 0.32, 0.35, 0.4]),\n",
    "    'batch_size': tune.choice([32, 64]),\n",
    "    'weight_decay': tune.choice([1e-6, 2e-6, 1e-5, 2e-5, 1e-4, 2e-4, 5e-5, 5e-6, 1e-2, 1e-3]),\n",
    "    'lr': tune.choice([1e-6, 1e-5, 2e-5, 1e-4, 2e-4, 2e-6, 3e-6, 5e-6, 3e-5, 5e-5, 2e-7, 1e-7, 1e-3, 5e-7]),\n",
    "    'epochs': tune.choice([5, 7, 10, 12, 15, 20])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c45e87a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:26.087719Z",
     "iopub.status.busy": "2024-08-31T04:20:26.087427Z",
     "iopub.status.idle": "2024-08-31T04:20:26.091091Z",
     "shell.execute_reply": "2024-08-31T04:20:26.090284Z"
    },
    "papermill": {
     "duration": 0.035518,
     "end_time": "2024-08-31T04:20:26.092961",
     "exception": false,
     "start_time": "2024-08-31T04:20:26.057443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.choice([5, 7, 10, 12, 15, 20])\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434896cd",
   "metadata": {
    "papermill": {
     "duration": 0.028888,
     "end_time": "2024-08-31T04:20:26.150897",
     "exception": false,
     "start_time": "2024-08-31T04:20:26.122009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEARCH ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2c27c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:26.213390Z",
     "iopub.status.busy": "2024-08-31T04:20:26.213083Z",
     "iopub.status.idle": "2024-08-31T04:20:26.217176Z",
     "shell.execute_reply": "2024-08-31T04:20:26.216447Z"
    },
    "papermill": {
     "duration": 0.03898,
     "end_time": "2024-08-31T04:20:26.219079",
     "exception": false,
     "start_time": "2024-08-31T04:20:26.180099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optuna_search = OptunaSearch(\n",
    "    metric=[\"accuracy\",\"custom_metric\"],\n",
    "    mode=[\"max\",\"min\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba02e8e",
   "metadata": {
    "papermill": {
     "duration": 0.028766,
     "end_time": "2024-08-31T04:20:26.277203",
     "exception": false,
     "start_time": "2024-08-31T04:20:26.248437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SCHEDULER ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bacbf142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:26.336431Z",
     "iopub.status.busy": "2024-08-31T04:20:26.336128Z",
     "iopub.status.idle": "2024-08-31T04:20:26.340261Z",
     "shell.execute_reply": "2024-08-31T04:20:26.339457Z"
    },
    "papermill": {
     "duration": 0.035847,
     "end_time": "2024-08-31T04:20:26.342113",
     "exception": false,
     "start_time": "2024-08-31T04:20:26.306266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    metric='accuracy',\n",
    "    mode='max',\n",
    "    max_t=22,\n",
    "    grace_period=3,\n",
    "    reduction_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b7ebd",
   "metadata": {
    "papermill": {
     "duration": 0.028943,
     "end_time": "2024-08-31T04:20:26.400530",
     "exception": false,
     "start_time": "2024-08-31T04:20:26.371587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## REPORTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6848c0fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:26.460040Z",
     "iopub.status.busy": "2024-08-31T04:20:26.459268Z",
     "iopub.status.idle": "2024-08-31T04:20:26.463157Z",
     "shell.execute_reply": "2024-08-31T04:20:26.462471Z"
    },
    "papermill": {
     "duration": 0.035527,
     "end_time": "2024-08-31T04:20:26.465041",
     "exception": false,
     "start_time": "2024-08-31T04:20:26.429514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To ignore warinings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bf2a1fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T04:20:26.524454Z",
     "iopub.status.busy": "2024-08-31T04:20:26.524136Z",
     "iopub.status.idle": "2024-08-31T07:25:06.765510Z",
     "shell.execute_reply": "2024-08-31T07:25:06.764618Z"
    },
    "papermill": {
     "duration": 11080.2737,
     "end_time": "2024-08-31T07:25:06.767790",
     "exception": false,
     "start_time": "2024-08-31T04:20:26.494090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-08-31 07:25:06</td></tr>\n",
       "<tr><td>Running for: </td><td>03:04:23.56        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.1/31.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=65<br>Bracket: Iter 12.000: None | Iter 6.000: 0.7834311806914547 | Iter 3.000: 0.766470971950424<br>Logical resource usage: 2.0/4 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  custom_metric</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">  early_stopping_epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_46bb3c01</td><td>TERMINATED</td><td>172.19.2.2:341 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.56917 </td><td style=\"text-align: right;\">  0.775603</td><td style=\"text-align: right;\">     0.0028309 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">    0.270375</td><td style=\"text-align: right;\">        0.895337</td><td style=\"text-align: right;\">                     4</td></tr>\n",
       "<tr><td>train_fn_6b6c9dd5</td><td>TERMINATED</td><td>172.19.2.2:377 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.481348</td><td style=\"text-align: right;\">  0.767776</td><td style=\"text-align: right;\">    -0.237512  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.430707</td><td style=\"text-align: right;\">        0.814965</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_d66fca06</td><td>TERMINATED</td><td>172.19.2.2:482 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.551103</td><td style=\"text-align: right;\">  0.784736</td><td style=\"text-align: right;\">     0.00673136</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.206467</td><td style=\"text-align: right;\">        0.920828</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_f3454da2</td><td>TERMINATED</td><td>172.19.2.2:557 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.498781</td><td style=\"text-align: right;\">  0.772994</td><td style=\"text-align: right;\">    -0.220412  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.558684</td><td style=\"text-align: right;\">        0.725296</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_750b9579</td><td>TERMINATED</td><td>172.19.2.2:646 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.637567</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0863775 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.637487</td><td style=\"text-align: right;\">        0.562903</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_c21da5bb</td><td>TERMINATED</td><td>172.19.2.2:718 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.47241 </td><td style=\"text-align: right;\">  0.76908 </td><td style=\"text-align: right;\">    -0.283125  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.480906</td><td style=\"text-align: right;\">        0.787674</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_cea60c23</td><td>TERMINATED</td><td>172.19.2.2:805 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.525503</td><td style=\"text-align: right;\">  0.762557</td><td style=\"text-align: right;\">    -0.181872  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.58496 </td><td style=\"text-align: right;\">        0.711651</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_e5aebc6d</td><td>TERMINATED</td><td>172.19.2.2:876 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.637977</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0863669 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.637388</td><td style=\"text-align: right;\">        0.561553</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_22381b35</td><td>TERMINATED</td><td>172.19.2.2:962 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.582038</td><td style=\"text-align: right;\">  0.792564</td><td style=\"text-align: right;\">     0.0410316 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.211237</td><td style=\"text-align: right;\">        0.924876</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_c43a8b2b</td><td>TERMINATED</td><td>172.19.2.2:1035</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.49044 </td><td style=\"text-align: right;\">  0.756034</td><td style=\"text-align: right;\">    -0.25132   </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.517486</td><td style=\"text-align: right;\">        0.757535</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_33394049</td><td>TERMINATED</td><td>172.19.2.2:1124</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.690055</td><td style=\"text-align: right;\">  0.553816</td><td style=\"text-align: right;\">     0.141615  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.69352 </td><td style=\"text-align: right;\">        0.561104</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_7c933db1</td><td>TERMINATED</td><td>172.19.2.2:1202</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.60222 </td><td style=\"text-align: right;\">  0.733855</td><td style=\"text-align: right;\">    -0.0539825 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.628109</td><td style=\"text-align: right;\">        0.604438</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_8a43c9d6</td><td>TERMINATED</td><td>172.19.2.2:1281</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.702028</td><td style=\"text-align: right;\">  0.770385</td><td style=\"text-align: right;\">     0.283422  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.166458</td><td style=\"text-align: right;\">        0.938372</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_179d2be5</td><td>TERMINATED</td><td>172.19.2.2:1360</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.490823</td><td style=\"text-align: right;\">  0.78865 </td><td style=\"text-align: right;\">    -0.190813  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.347794</td><td style=\"text-align: right;\">        0.859649</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_8bc6890b</td><td>TERMINATED</td><td>172.19.2.2:1453</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.49079 </td><td style=\"text-align: right;\">  0.789954</td><td style=\"text-align: right;\">    -0.177393  </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">    0.329836</td><td style=\"text-align: right;\">        0.872545</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_a41d5d93</td><td>TERMINATED</td><td>172.19.2.2:1531</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.501221</td><td style=\"text-align: right;\">  0.783431</td><td style=\"text-align: right;\">    -0.164044  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.347105</td><td style=\"text-align: right;\">        0.865647</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_14d50086</td><td>TERMINATED</td><td>172.19.2.2:1630</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.477092</td><td style=\"text-align: right;\">  0.767776</td><td style=\"text-align: right;\">    -0.280253  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.483602</td><td style=\"text-align: right;\">        0.782126</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_8280e3bd</td><td>TERMINATED</td><td>172.19.2.2:1701</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.698894</td><td style=\"text-align: right;\">  0.553816</td><td style=\"text-align: right;\">     0.149875  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.700602</td><td style=\"text-align: right;\">        0.561703</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_e95e283c</td><td>TERMINATED</td><td>172.19.2.2:1787</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.62029 </td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">     0.119696  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.193782</td><td style=\"text-align: right;\">        0.930424</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_e2de0a7c</td><td>TERMINATED</td><td>172.19.2.2:1858</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.680092</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.136554  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.694905</td><td style=\"text-align: right;\">        0.54416 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_8f403eff</td><td>TERMINATED</td><td>172.19.2.2:1945</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.669172</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.124511  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.676462</td><td style=\"text-align: right;\">        0.568751</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_9f7d3c88</td><td>TERMINATED</td><td>172.19.2.2:2025</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.604877</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">     0.0727223 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.226629</td><td style=\"text-align: right;\">        0.919478</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_1151f89f</td><td>TERMINATED</td><td>172.19.2.2:2102</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.647334</td><td style=\"text-align: right;\">  0.586432</td><td style=\"text-align: right;\">     0.077984  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.655569</td><td style=\"text-align: right;\">        0.560504</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_aa3c100c</td><td>TERMINATED</td><td>172.19.2.2:2188</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.643668</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.0973615 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.661642</td><td style=\"text-align: right;\">        0.562603</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_9e74f1d8</td><td>TERMINATED</td><td>172.19.2.2:2261</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.554925</td><td style=\"text-align: right;\">  0.774299</td><td style=\"text-align: right;\">     0.00272773</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.246006</td><td style=\"text-align: right;\">        0.909582</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_5e9dfa21</td><td>TERMINATED</td><td>172.19.2.2:2345</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.513256</td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">    -0.108087  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.287161</td><td style=\"text-align: right;\">        0.889339</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_98b9c0f2</td><td>TERMINATED</td><td>172.19.2.2:2434</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.528242</td><td style=\"text-align: right;\">  0.783431</td><td style=\"text-align: right;\">    -0.069862  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.273541</td><td style=\"text-align: right;\">        0.899385</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_634b94c8</td><td>TERMINATED</td><td>172.19.2.2:2516</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.497212</td><td style=\"text-align: right;\">  0.780822</td><td style=\"text-align: right;\">    -0.150064  </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">    0.322442</td><td style=\"text-align: right;\">        0.873144</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_683de762</td><td>TERMINATED</td><td>172.19.2.2:2605</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.504531</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">    -0.135489  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.304077</td><td style=\"text-align: right;\">        0.881541</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_ab3fff1f</td><td>TERMINATED</td><td>172.19.2.2:2692</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.507743</td><td style=\"text-align: right;\">  0.783431</td><td style=\"text-align: right;\">    -0.121065  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.300057</td><td style=\"text-align: right;\">        0.88499 </td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_886d0d4a</td><td>TERMINATED</td><td>172.19.2.2:2782</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.508416</td><td style=\"text-align: right;\">  0.782127</td><td style=\"text-align: right;\">    -0.111749  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.294705</td><td style=\"text-align: right;\">        0.892338</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_a73dfcf2</td><td>TERMINATED</td><td>172.19.2.2:2868</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.637115</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.084446  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.638135</td><td style=\"text-align: right;\">        0.559004</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_54789429</td><td>TERMINATED</td><td>172.19.2.2:2956</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.68092 </td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.133358  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.693683</td><td style=\"text-align: right;\">        0.565302</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_d000943a</td><td>TERMINATED</td><td>172.19.2.2:3027</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.609428</td><td style=\"text-align: right;\">  0.716895</td><td style=\"text-align: right;\">    -0.0509259 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.627597</td><td style=\"text-align: right;\">        0.621982</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_0be5afbd</td><td>TERMINATED</td><td>172.19.2.2:3114</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.625197</td><td style=\"text-align: right;\">  0.616438</td><td style=\"text-align: right;\">     0.0311501 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.634888</td><td style=\"text-align: right;\">        0.581347</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_2debfb4f</td><td>TERMINATED</td><td>172.19.2.2:3179</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.77035 </td><td style=\"text-align: right;\">  0.761252</td><td style=\"text-align: right;\">     0.433498  </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">    0.121011</td><td style=\"text-align: right;\">        0.960714</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_1c156e27</td><td>TERMINATED</td><td>172.19.2.2:3272</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.717367</td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">     0.285677  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.164148</td><td style=\"text-align: right;\">        0.94152 </td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_72dbf2e6</td><td>TERMINATED</td><td>172.19.2.2:3362</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.638262</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0839112 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.637787</td><td style=\"text-align: right;\">        0.554056</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_3a9d14b4</td><td>TERMINATED</td><td>172.19.2.2:3438</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.638988</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0890685 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.637416</td><td style=\"text-align: right;\">        0.563953</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_8674818a</td><td>TERMINATED</td><td>172.19.2.2:3519</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.637875</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0880309 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.636904</td><td style=\"text-align: right;\">        0.564702</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_178a66ae</td><td>TERMINATED</td><td>172.19.2.2:3597</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.556395</td><td style=\"text-align: right;\">  0.796477</td><td style=\"text-align: right;\">    -0.00107077</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.209021</td><td style=\"text-align: right;\">        0.927126</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_73143f00</td><td>TERMINATED</td><td>172.19.2.2:3678</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.527911</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">    -0.0716827 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">    0.263249</td><td style=\"text-align: right;\">        0.898186</td><td style=\"text-align: right;\">                     4</td></tr>\n",
       "<tr><td>train_fn_ea8cf60d</td><td>TERMINATED</td><td>172.19.2.2:3768</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.617559</td><td style=\"text-align: right;\">  0.778213</td><td style=\"text-align: right;\">     0.121054  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.204706</td><td style=\"text-align: right;\">        0.928775</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_60b4092c</td><td>TERMINATED</td><td>172.19.2.2:3842</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.545302</td><td style=\"text-align: right;\">  0.728637</td><td style=\"text-align: right;\">    -0.175105  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.534432</td><td style=\"text-align: right;\">        0.723047</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_1ad5c6d3</td><td>TERMINATED</td><td>172.19.2.2:3929</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.633954</td><td style=\"text-align: right;\">  0.784736</td><td style=\"text-align: right;\">     0.137398  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.196686</td><td style=\"text-align: right;\">        0.923827</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_7e00f412</td><td>TERMINATED</td><td>172.19.2.2:4010</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.469452</td><td style=\"text-align: right;\">  0.776908</td><td style=\"text-align: right;\">    -0.246395  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.401429</td><td style=\"text-align: right;\">        0.831009</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_4fcfb882</td><td>TERMINATED</td><td>172.19.2.2:4095</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.515498</td><td style=\"text-align: right;\">  0.759948</td><td style=\"text-align: right;\">    -0.234068  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.533849</td><td style=\"text-align: right;\">        0.757535</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_fd019dd3</td><td>TERMINATED</td><td>172.19.2.2:4181</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.526295</td><td style=\"text-align: right;\">  0.749511</td><td style=\"text-align: right;\">    -0.156599  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.58853 </td><td style=\"text-align: right;\">        0.678513</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_2010c52e</td><td>TERMINATED</td><td>172.19.2.2:4246</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.558573</td><td style=\"text-align: right;\">  0.746902</td><td style=\"text-align: right;\">    -0.124005  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.607884</td><td style=\"text-align: right;\">        0.667566</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_cafabe3b</td><td>TERMINATED</td><td>172.19.2.2:4338</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.531133</td><td style=\"text-align: right;\">  0.754729</td><td style=\"text-align: right;\">    -0.174774  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.577601</td><td style=\"text-align: right;\">        0.703554</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_c4c1b662</td><td>TERMINATED</td><td>172.19.2.2:4403</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.693194</td><td style=\"text-align: right;\">  0.539465</td><td style=\"text-align: right;\">     0.166027  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.702299</td><td style=\"text-align: right;\">        0.554956</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_8d945055</td><td>TERMINATED</td><td>172.19.2.2:4496</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.494594</td><td style=\"text-align: right;\">  0.793868</td><td style=\"text-align: right;\">    -0.179994  </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">    0.330961</td><td style=\"text-align: right;\">        0.868796</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_acf66b11</td><td>TERMINATED</td><td>172.19.2.2:4560</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.50602 </td><td style=\"text-align: right;\">  0.759948</td><td style=\"text-align: right;\">    -0.207578  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.563617</td><td style=\"text-align: right;\">        0.724846</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_79197302</td><td>TERMINATED</td><td>172.19.2.2:4653</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.53841 </td><td style=\"text-align: right;\">  0.750815</td><td style=\"text-align: right;\">    -0.158646  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.589671</td><td style=\"text-align: right;\">        0.694557</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_53ed870e</td><td>TERMINATED</td><td>172.19.2.2:4739</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.494062</td><td style=\"text-align: right;\">  0.774299</td><td style=\"text-align: right;\">    -0.17413   </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.364201</td><td style=\"text-align: right;\">        0.85665 </td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_03b53385</td><td>TERMINATED</td><td>172.19.2.2:4813</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.483486</td><td style=\"text-align: right;\">  0.762557</td><td style=\"text-align: right;\">    -0.266731  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.492346</td><td style=\"text-align: right;\">        0.778378</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_11296e3e</td><td>TERMINATED</td><td>172.19.2.2:4900</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.48436 </td><td style=\"text-align: right;\">  0.780822</td><td style=\"text-align: right;\">    -0.19092   </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.350754</td><td style=\"text-align: right;\">        0.8583  </td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_dd15de38</td><td>TERMINATED</td><td>172.19.2.2:4981</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.637735</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0969815 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.631028</td><td style=\"text-align: right;\">        0.577148</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_d65384b2</td><td>TERMINATED</td><td>172.19.2.2:5067</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.637886</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0849265 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.637747</td><td style=\"text-align: right;\">        0.559304</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_2f6d778e</td><td>TERMINATED</td><td>172.19.2.2:5142</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.571261</td><td style=\"text-align: right;\">  0.78865 </td><td style=\"text-align: right;\">     0.0385169 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.200775</td><td style=\"text-align: right;\">        0.929975</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_1f8b0fc1</td><td>TERMINATED</td><td>172.19.2.2:5225</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.665992</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.109507  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.666457</td><td style=\"text-align: right;\">        0.555706</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_875df40d</td><td>TERMINATED</td><td>172.19.2.2:5311</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.571399</td><td style=\"text-align: right;\">  0.778213</td><td style=\"text-align: right;\">     0.0522743 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.203036</td><td style=\"text-align: right;\">        0.928025</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_83296986</td><td>TERMINATED</td><td>172.19.2.2:5380</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.657535</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.107234  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.663771</td><td style=\"text-align: right;\">        0.566352</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_e4bfa6d0</td><td>TERMINATED</td><td>172.19.2.2:5468</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.504844</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">    -0.123345  </td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">    0.291075</td><td style=\"text-align: right;\">        0.891888</td><td style=\"text-align: right;\">                     8</td></tr>\n",
       "<tr><td>train_fn_f0086783</td><td>TERMINATED</td><td>172.19.2.2:5548</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.510329</td><td style=\"text-align: right;\">  0.775603</td><td style=\"text-align: right;\">    -0.120441  </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">    0.321801</td><td style=\"text-align: right;\">        0.876743</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_24e07855</td><td>TERMINATED</td><td>172.19.2.2:5649</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.500363</td><td style=\"text-align: right;\">  0.761252</td><td style=\"text-align: right;\">    -0.248538  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.493836</td><td style=\"text-align: right;\">        0.779427</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_a33b97db</td><td>TERMINATED</td><td>172.19.2.2:5724</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.486876</td><td style=\"text-align: right;\">  0.775603</td><td style=\"text-align: right;\">    -0.175415  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.347746</td><td style=\"text-align: right;\">        0.863098</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_1c8f379b</td><td>TERMINATED</td><td>172.19.2.2:5806</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.638997</td><td style=\"text-align: right;\">  0.626875</td><td style=\"text-align: right;\">     0.0463996 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.652878</td><td style=\"text-align: right;\">        0.5722  </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_3cc01db4</td><td>TERMINATED</td><td>172.19.2.2:5892</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.620436</td><td style=\"text-align: right;\">  0.629485</td><td style=\"text-align: right;\">     0.0109687 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.630776</td><td style=\"text-align: right;\">        0.59979 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_1c43399d</td><td>TERMINATED</td><td>172.19.2.2:5966</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.720857</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.174208  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.72792 </td><td style=\"text-align: right;\">        0.565002</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_7fd31b1a</td><td>TERMINATED</td><td>172.19.2.2:6051</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.676014</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.123857  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.684235</td><td style=\"text-align: right;\">        0.554806</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_5aff7598</td><td>TERMINATED</td><td>172.19.2.2:6125</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.712518</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.165084  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.71771 </td><td style=\"text-align: right;\">        0.565302</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_4f424f68</td><td>TERMINATED</td><td>172.19.2.2:6210</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.524441</td><td style=\"text-align: right;\">  0.793868</td><td style=\"text-align: right;\">    -0.110432  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.299824</td><td style=\"text-align: right;\">        0.887239</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_bc283978</td><td>TERMINATED</td><td>172.19.2.2:6284</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.49656 </td><td style=\"text-align: right;\">  0.795173</td><td style=\"text-align: right;\">    -0.166867  </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">    0.318387</td><td style=\"text-align: right;\">        0.880492</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_01fbc048</td><td>TERMINATED</td><td>172.19.2.2:6376</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.493344</td><td style=\"text-align: right;\">  0.782127</td><td style=\"text-align: right;\">    -0.161773  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.329892</td><td style=\"text-align: right;\">        0.872695</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_d357fda2</td><td>TERMINATED</td><td>172.19.2.2:6462</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.479799</td><td style=\"text-align: right;\">  0.796477</td><td style=\"text-align: right;\">    -0.188652  </td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">    0.311508</td><td style=\"text-align: right;\">        0.884241</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_7fd24a2b</td><td>TERMINATED</td><td>172.19.2.2:6547</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.504219</td><td style=\"text-align: right;\">  0.761252</td><td style=\"text-align: right;\">    -0.220751  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.553274</td><td style=\"text-align: right;\">        0.737742</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_3d22a7f1</td><td>TERMINATED</td><td>172.19.2.2:6633</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.540788</td><td style=\"text-align: right;\">  0.745597</td><td style=\"text-align: right;\">    -0.150169  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.593629</td><td style=\"text-align: right;\">        0.689159</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_ad04b40c</td><td>TERMINATED</td><td>172.19.2.2:6708</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.53456 </td><td style=\"text-align: right;\">  0.763862</td><td style=\"text-align: right;\">    -0.166231  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.592146</td><td style=\"text-align: right;\">        0.695307</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_c0b10490</td><td>TERMINATED</td><td>172.19.2.2:6790</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.536433</td><td style=\"text-align: right;\">  0.733855</td><td style=\"text-align: right;\">    -0.142825  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.596584</td><td style=\"text-align: right;\">        0.68481 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_b9dc47d9</td><td>TERMINATED</td><td>172.19.2.2:6865</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.539686</td><td style=\"text-align: right;\">  0.761252</td><td style=\"text-align: right;\">    -0.152895  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.598486</td><td style=\"text-align: right;\">        0.682711</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_8fa6a05e</td><td>TERMINATED</td><td>172.19.2.2:6948</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.636092</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">     0.154602  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.17271 </td><td style=\"text-align: right;\">        0.935673</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_3406b32c</td><td>TERMINATED</td><td>172.19.2.2:7023</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.634995</td><td style=\"text-align: right;\">  0.784736</td><td style=\"text-align: right;\">     0.139666  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.200522</td><td style=\"text-align: right;\">        0.929075</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_c37ffb38</td><td>TERMINATED</td><td>172.19.2.2:7114</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.602287</td><td style=\"text-align: right;\">  0.782127</td><td style=\"text-align: right;\">     0.0997541 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.192746</td><td style=\"text-align: right;\">        0.931774</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_0b408d83</td><td>TERMINATED</td><td>172.19.2.2:7196</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.608802</td><td style=\"text-align: right;\">  0.682975</td><td style=\"text-align: right;\">    -0.0409949 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.622562</td><td style=\"text-align: right;\">        0.630379</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_150b2464</td><td>TERMINATED</td><td>172.19.2.2:7281</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.669248</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.118693  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.681424</td><td style=\"text-align: right;\">        0.559904</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_bfe2aaa8</td><td>TERMINATED</td><td>172.19.2.2:7352</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.693129</td><td style=\"text-align: right;\">  0.579909</td><td style=\"text-align: right;\">     0.127908  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.698901</td><td style=\"text-align: right;\">        0.556305</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_639d3c7b</td><td>TERMINATED</td><td>172.19.2.2:7438</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.667768</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.12126   </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.674512</td><td style=\"text-align: right;\">        0.565602</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_48e7c941</td><td>TERMINATED</td><td>172.19.2.2:7510</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.56786 </td><td style=\"text-align: right;\">  0.771689</td><td style=\"text-align: right;\">     0.0325118 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.23427 </td><td style=\"text-align: right;\">        0.910781</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_da57b757</td><td>TERMINATED</td><td>172.19.2.2:7595</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.682451</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.131469  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.683395</td><td style=\"text-align: right;\">        0.562453</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_6e5b4bda</td><td>TERMINATED</td><td>172.19.2.2:7677</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.619984</td><td style=\"text-align: right;\">  0.776908</td><td style=\"text-align: right;\">     0.131289  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.196325</td><td style=\"text-align: right;\">        0.929675</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_523f0374</td><td>TERMINATED</td><td>172.19.2.2:7751</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.684363</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.129432  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.688986</td><td style=\"text-align: right;\">        0.556755</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_452eb968</td><td>TERMINATED</td><td>172.19.2.2:7840</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.494653</td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">    -0.169939  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.331063</td><td style=\"text-align: right;\">        0.865347</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_9cd6cf80</td><td>TERMINATED</td><td>172.19.2.2:7920</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.479583</td><td style=\"text-align: right;\">  0.778213</td><td style=\"text-align: right;\">    -0.21355   </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.376915</td><td style=\"text-align: right;\">        0.845704</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_4cb10d4a</td><td>TERMINATED</td><td>172.19.2.2:8010</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.469933</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">    -0.248981  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.384831</td><td style=\"text-align: right;\">        0.839106</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_5b980e2a</td><td>TERMINATED</td><td>172.19.2.2:8087</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.49376 </td><td style=\"text-align: right;\">  0.783431</td><td style=\"text-align: right;\">    -0.17308   </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">    0.342643</td><td style=\"text-align: right;\">        0.865497</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_7f7fd554</td><td>TERMINATED</td><td>172.19.2.2:8176</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.487217</td><td style=\"text-align: right;\">  0.757339</td><td style=\"text-align: right;\">    -0.253267  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">    0.519529</td><td style=\"text-align: right;\">        0.758734</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_31f4f7fb</td><td>TERMINATED</td><td>172.19.2.2:8258</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.491432</td><td style=\"text-align: right;\">  0.791259</td><td style=\"text-align: right;\">    -0.186539  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.336845</td><td style=\"text-align: right;\">        0.863248</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_b41e28da</td><td>TERMINATED</td><td>172.19.2.2:8333</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.500806</td><td style=\"text-align: right;\">  0.78865 </td><td style=\"text-align: right;\">    -0.163747  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.33261 </td><td style=\"text-align: right;\">        0.868646</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_e570c40b</td><td>TERMINATED</td><td>172.19.2.2:8429</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.32</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.487605</td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">    -0.182899  </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">    0.337338</td><td style=\"text-align: right;\">        0.866847</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 04:20:30,844\tINFO worker.py:1753 -- Started a local Ray instance.\n",
      "2024-08-31 04:20:32,252\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-08-31 04:20:32,257\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "[I 2024-08-31 04:20:32,306] A new study created in memory with name: optuna\n",
      "\u001b[36m(train_fn pid=341)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=341)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=377)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=377)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  custom_metric</th><th style=\"text-align: right;\">  early_stopping_epoch</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_01fbc048</td><td style=\"text-align: right;\">  0.782127</td><td style=\"text-align: right;\">    -0.161773  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.493344</td><td style=\"text-align: right;\">        0.872695</td><td style=\"text-align: right;\">    0.329892</td></tr>\n",
       "<tr><td>train_fn_03b53385</td><td style=\"text-align: right;\">  0.762557</td><td style=\"text-align: right;\">    -0.266731  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.483486</td><td style=\"text-align: right;\">        0.778378</td><td style=\"text-align: right;\">    0.492346</td></tr>\n",
       "<tr><td>train_fn_0b408d83</td><td style=\"text-align: right;\">  0.682975</td><td style=\"text-align: right;\">    -0.0409949 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.608802</td><td style=\"text-align: right;\">        0.630379</td><td style=\"text-align: right;\">    0.622562</td></tr>\n",
       "<tr><td>train_fn_0be5afbd</td><td style=\"text-align: right;\">  0.616438</td><td style=\"text-align: right;\">     0.0311501 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.625197</td><td style=\"text-align: right;\">        0.581347</td><td style=\"text-align: right;\">    0.634888</td></tr>\n",
       "<tr><td>train_fn_11296e3e</td><td style=\"text-align: right;\">  0.780822</td><td style=\"text-align: right;\">    -0.19092   </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.48436 </td><td style=\"text-align: right;\">        0.8583  </td><td style=\"text-align: right;\">    0.350754</td></tr>\n",
       "<tr><td>train_fn_1151f89f</td><td style=\"text-align: right;\">  0.586432</td><td style=\"text-align: right;\">     0.077984  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.647334</td><td style=\"text-align: right;\">        0.560504</td><td style=\"text-align: right;\">    0.655569</td></tr>\n",
       "<tr><td>train_fn_14d50086</td><td style=\"text-align: right;\">  0.767776</td><td style=\"text-align: right;\">    -0.280253  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.477092</td><td style=\"text-align: right;\">        0.782126</td><td style=\"text-align: right;\">    0.483602</td></tr>\n",
       "<tr><td>train_fn_150b2464</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.118693  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.669248</td><td style=\"text-align: right;\">        0.559904</td><td style=\"text-align: right;\">    0.681424</td></tr>\n",
       "<tr><td>train_fn_178a66ae</td><td style=\"text-align: right;\">  0.796477</td><td style=\"text-align: right;\">    -0.00107077</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.556395</td><td style=\"text-align: right;\">        0.927126</td><td style=\"text-align: right;\">    0.209021</td></tr>\n",
       "<tr><td>train_fn_179d2be5</td><td style=\"text-align: right;\">  0.78865 </td><td style=\"text-align: right;\">    -0.190813  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.490823</td><td style=\"text-align: right;\">        0.859649</td><td style=\"text-align: right;\">    0.347794</td></tr>\n",
       "<tr><td>train_fn_1ad5c6d3</td><td style=\"text-align: right;\">  0.784736</td><td style=\"text-align: right;\">     0.137398  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.633954</td><td style=\"text-align: right;\">        0.923827</td><td style=\"text-align: right;\">    0.196686</td></tr>\n",
       "<tr><td>train_fn_1c156e27</td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">     0.285677  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.717367</td><td style=\"text-align: right;\">        0.94152 </td><td style=\"text-align: right;\">    0.164148</td></tr>\n",
       "<tr><td>train_fn_1c43399d</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.174208  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.720857</td><td style=\"text-align: right;\">        0.565002</td><td style=\"text-align: right;\">    0.72792 </td></tr>\n",
       "<tr><td>train_fn_1c8f379b</td><td style=\"text-align: right;\">  0.626875</td><td style=\"text-align: right;\">     0.0463996 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.638997</td><td style=\"text-align: right;\">        0.5722  </td><td style=\"text-align: right;\">    0.652878</td></tr>\n",
       "<tr><td>train_fn_1f8b0fc1</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.109507  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.665992</td><td style=\"text-align: right;\">        0.555706</td><td style=\"text-align: right;\">    0.666457</td></tr>\n",
       "<tr><td>train_fn_2010c52e</td><td style=\"text-align: right;\">  0.746902</td><td style=\"text-align: right;\">    -0.124005  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.558573</td><td style=\"text-align: right;\">        0.667566</td><td style=\"text-align: right;\">    0.607884</td></tr>\n",
       "<tr><td>train_fn_22381b35</td><td style=\"text-align: right;\">  0.792564</td><td style=\"text-align: right;\">     0.0410316 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.582038</td><td style=\"text-align: right;\">        0.924876</td><td style=\"text-align: right;\">    0.211237</td></tr>\n",
       "<tr><td>train_fn_24e07855</td><td style=\"text-align: right;\">  0.761252</td><td style=\"text-align: right;\">    -0.248538  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.500363</td><td style=\"text-align: right;\">        0.779427</td><td style=\"text-align: right;\">    0.493836</td></tr>\n",
       "<tr><td>train_fn_2debfb4f</td><td style=\"text-align: right;\">  0.761252</td><td style=\"text-align: right;\">     0.433498  </td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.77035 </td><td style=\"text-align: right;\">        0.960714</td><td style=\"text-align: right;\">    0.121011</td></tr>\n",
       "<tr><td>train_fn_2f6d778e</td><td style=\"text-align: right;\">  0.78865 </td><td style=\"text-align: right;\">     0.0385169 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.571261</td><td style=\"text-align: right;\">        0.929975</td><td style=\"text-align: right;\">    0.200775</td></tr>\n",
       "<tr><td>train_fn_31f4f7fb</td><td style=\"text-align: right;\">  0.791259</td><td style=\"text-align: right;\">    -0.186539  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.491432</td><td style=\"text-align: right;\">        0.863248</td><td style=\"text-align: right;\">    0.336845</td></tr>\n",
       "<tr><td>train_fn_33394049</td><td style=\"text-align: right;\">  0.553816</td><td style=\"text-align: right;\">     0.141615  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.690055</td><td style=\"text-align: right;\">        0.561104</td><td style=\"text-align: right;\">    0.69352 </td></tr>\n",
       "<tr><td>train_fn_3406b32c</td><td style=\"text-align: right;\">  0.784736</td><td style=\"text-align: right;\">     0.139666  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.634995</td><td style=\"text-align: right;\">        0.929075</td><td style=\"text-align: right;\">    0.200522</td></tr>\n",
       "<tr><td>train_fn_3a9d14b4</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0890685 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.638988</td><td style=\"text-align: right;\">        0.563953</td><td style=\"text-align: right;\">    0.637416</td></tr>\n",
       "<tr><td>train_fn_3cc01db4</td><td style=\"text-align: right;\">  0.629485</td><td style=\"text-align: right;\">     0.0109687 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.620436</td><td style=\"text-align: right;\">        0.59979 </td><td style=\"text-align: right;\">    0.630776</td></tr>\n",
       "<tr><td>train_fn_3d22a7f1</td><td style=\"text-align: right;\">  0.745597</td><td style=\"text-align: right;\">    -0.150169  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.540788</td><td style=\"text-align: right;\">        0.689159</td><td style=\"text-align: right;\">    0.593629</td></tr>\n",
       "<tr><td>train_fn_452eb968</td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">    -0.169939  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.494653</td><td style=\"text-align: right;\">        0.865347</td><td style=\"text-align: right;\">    0.331063</td></tr>\n",
       "<tr><td>train_fn_46bb3c01</td><td style=\"text-align: right;\">  0.775603</td><td style=\"text-align: right;\">     0.0028309 </td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.56917 </td><td style=\"text-align: right;\">        0.895337</td><td style=\"text-align: right;\">    0.270375</td></tr>\n",
       "<tr><td>train_fn_48e7c941</td><td style=\"text-align: right;\">  0.771689</td><td style=\"text-align: right;\">     0.0325118 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.56786 </td><td style=\"text-align: right;\">        0.910781</td><td style=\"text-align: right;\">    0.23427 </td></tr>\n",
       "<tr><td>train_fn_4cb10d4a</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">    -0.248981  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.469933</td><td style=\"text-align: right;\">        0.839106</td><td style=\"text-align: right;\">    0.384831</td></tr>\n",
       "<tr><td>train_fn_4f424f68</td><td style=\"text-align: right;\">  0.793868</td><td style=\"text-align: right;\">    -0.110432  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.524441</td><td style=\"text-align: right;\">        0.887239</td><td style=\"text-align: right;\">    0.299824</td></tr>\n",
       "<tr><td>train_fn_4fcfb882</td><td style=\"text-align: right;\">  0.759948</td><td style=\"text-align: right;\">    -0.234068  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.515498</td><td style=\"text-align: right;\">        0.757535</td><td style=\"text-align: right;\">    0.533849</td></tr>\n",
       "<tr><td>train_fn_523f0374</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.129432  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.684363</td><td style=\"text-align: right;\">        0.556755</td><td style=\"text-align: right;\">    0.688986</td></tr>\n",
       "<tr><td>train_fn_53ed870e</td><td style=\"text-align: right;\">  0.774299</td><td style=\"text-align: right;\">    -0.17413   </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.494062</td><td style=\"text-align: right;\">        0.85665 </td><td style=\"text-align: right;\">    0.364201</td></tr>\n",
       "<tr><td>train_fn_54789429</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.133358  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.68092 </td><td style=\"text-align: right;\">        0.565302</td><td style=\"text-align: right;\">    0.693683</td></tr>\n",
       "<tr><td>train_fn_5aff7598</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.165084  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.712518</td><td style=\"text-align: right;\">        0.565302</td><td style=\"text-align: right;\">    0.71771 </td></tr>\n",
       "<tr><td>train_fn_5b980e2a</td><td style=\"text-align: right;\">  0.783431</td><td style=\"text-align: right;\">    -0.17308   </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.49376 </td><td style=\"text-align: right;\">        0.865497</td><td style=\"text-align: right;\">    0.342643</td></tr>\n",
       "<tr><td>train_fn_5e9dfa21</td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">    -0.108087  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.513256</td><td style=\"text-align: right;\">        0.889339</td><td style=\"text-align: right;\">    0.287161</td></tr>\n",
       "<tr><td>train_fn_60b4092c</td><td style=\"text-align: right;\">  0.728637</td><td style=\"text-align: right;\">    -0.175105  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.545302</td><td style=\"text-align: right;\">        0.723047</td><td style=\"text-align: right;\">    0.534432</td></tr>\n",
       "<tr><td>train_fn_634b94c8</td><td style=\"text-align: right;\">  0.780822</td><td style=\"text-align: right;\">    -0.150064  </td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.497212</td><td style=\"text-align: right;\">        0.873144</td><td style=\"text-align: right;\">    0.322442</td></tr>\n",
       "<tr><td>train_fn_639d3c7b</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.12126   </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.667768</td><td style=\"text-align: right;\">        0.565602</td><td style=\"text-align: right;\">    0.674512</td></tr>\n",
       "<tr><td>train_fn_683de762</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">    -0.135489  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.504531</td><td style=\"text-align: right;\">        0.881541</td><td style=\"text-align: right;\">    0.304077</td></tr>\n",
       "<tr><td>train_fn_6b6c9dd5</td><td style=\"text-align: right;\">  0.767776</td><td style=\"text-align: right;\">    -0.237512  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.481348</td><td style=\"text-align: right;\">        0.814965</td><td style=\"text-align: right;\">    0.430707</td></tr>\n",
       "<tr><td>train_fn_6e5b4bda</td><td style=\"text-align: right;\">  0.776908</td><td style=\"text-align: right;\">     0.131289  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.619984</td><td style=\"text-align: right;\">        0.929675</td><td style=\"text-align: right;\">    0.196325</td></tr>\n",
       "<tr><td>train_fn_72dbf2e6</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0839112 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.638262</td><td style=\"text-align: right;\">        0.554056</td><td style=\"text-align: right;\">    0.637787</td></tr>\n",
       "<tr><td>train_fn_73143f00</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">    -0.0716827 </td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.527911</td><td style=\"text-align: right;\">        0.898186</td><td style=\"text-align: right;\">    0.263249</td></tr>\n",
       "<tr><td>train_fn_750b9579</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0863775 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.637567</td><td style=\"text-align: right;\">        0.562903</td><td style=\"text-align: right;\">    0.637487</td></tr>\n",
       "<tr><td>train_fn_79197302</td><td style=\"text-align: right;\">  0.750815</td><td style=\"text-align: right;\">    -0.158646  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.53841 </td><td style=\"text-align: right;\">        0.694557</td><td style=\"text-align: right;\">    0.589671</td></tr>\n",
       "<tr><td>train_fn_7c933db1</td><td style=\"text-align: right;\">  0.733855</td><td style=\"text-align: right;\">    -0.0539825 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.60222 </td><td style=\"text-align: right;\">        0.604438</td><td style=\"text-align: right;\">    0.628109</td></tr>\n",
       "<tr><td>train_fn_7e00f412</td><td style=\"text-align: right;\">  0.776908</td><td style=\"text-align: right;\">    -0.246395  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.469452</td><td style=\"text-align: right;\">        0.831009</td><td style=\"text-align: right;\">    0.401429</td></tr>\n",
       "<tr><td>train_fn_7f7fd554</td><td style=\"text-align: right;\">  0.757339</td><td style=\"text-align: right;\">    -0.253267  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.487217</td><td style=\"text-align: right;\">        0.758734</td><td style=\"text-align: right;\">    0.519529</td></tr>\n",
       "<tr><td>train_fn_7fd24a2b</td><td style=\"text-align: right;\">  0.761252</td><td style=\"text-align: right;\">    -0.220751  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.504219</td><td style=\"text-align: right;\">        0.737742</td><td style=\"text-align: right;\">    0.553274</td></tr>\n",
       "<tr><td>train_fn_7fd31b1a</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.123857  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.676014</td><td style=\"text-align: right;\">        0.554806</td><td style=\"text-align: right;\">    0.684235</td></tr>\n",
       "<tr><td>train_fn_8280e3bd</td><td style=\"text-align: right;\">  0.553816</td><td style=\"text-align: right;\">     0.149875  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.698894</td><td style=\"text-align: right;\">        0.561703</td><td style=\"text-align: right;\">    0.700602</td></tr>\n",
       "<tr><td>train_fn_83296986</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.107234  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.657535</td><td style=\"text-align: right;\">        0.566352</td><td style=\"text-align: right;\">    0.663771</td></tr>\n",
       "<tr><td>train_fn_8674818a</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0880309 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.637875</td><td style=\"text-align: right;\">        0.564702</td><td style=\"text-align: right;\">    0.636904</td></tr>\n",
       "<tr><td>train_fn_875df40d</td><td style=\"text-align: right;\">  0.778213</td><td style=\"text-align: right;\">     0.0522743 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.571399</td><td style=\"text-align: right;\">        0.928025</td><td style=\"text-align: right;\">    0.203036</td></tr>\n",
       "<tr><td>train_fn_886d0d4a</td><td style=\"text-align: right;\">  0.782127</td><td style=\"text-align: right;\">    -0.111749  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.508416</td><td style=\"text-align: right;\">        0.892338</td><td style=\"text-align: right;\">    0.294705</td></tr>\n",
       "<tr><td>train_fn_8a43c9d6</td><td style=\"text-align: right;\">  0.770385</td><td style=\"text-align: right;\">     0.283422  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.702028</td><td style=\"text-align: right;\">        0.938372</td><td style=\"text-align: right;\">    0.166458</td></tr>\n",
       "<tr><td>train_fn_8bc6890b</td><td style=\"text-align: right;\">  0.789954</td><td style=\"text-align: right;\">    -0.177393  </td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.49079 </td><td style=\"text-align: right;\">        0.872545</td><td style=\"text-align: right;\">    0.329836</td></tr>\n",
       "<tr><td>train_fn_8d945055</td><td style=\"text-align: right;\">  0.793868</td><td style=\"text-align: right;\">    -0.179994  </td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.494594</td><td style=\"text-align: right;\">        0.868796</td><td style=\"text-align: right;\">    0.330961</td></tr>\n",
       "<tr><td>train_fn_8f403eff</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.124511  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.669172</td><td style=\"text-align: right;\">        0.568751</td><td style=\"text-align: right;\">    0.676462</td></tr>\n",
       "<tr><td>train_fn_8fa6a05e</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">     0.154602  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.636092</td><td style=\"text-align: right;\">        0.935673</td><td style=\"text-align: right;\">    0.17271 </td></tr>\n",
       "<tr><td>train_fn_98b9c0f2</td><td style=\"text-align: right;\">  0.783431</td><td style=\"text-align: right;\">    -0.069862  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.528242</td><td style=\"text-align: right;\">        0.899385</td><td style=\"text-align: right;\">    0.273541</td></tr>\n",
       "<tr><td>train_fn_9cd6cf80</td><td style=\"text-align: right;\">  0.778213</td><td style=\"text-align: right;\">    -0.21355   </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.479583</td><td style=\"text-align: right;\">        0.845704</td><td style=\"text-align: right;\">    0.376915</td></tr>\n",
       "<tr><td>train_fn_9e74f1d8</td><td style=\"text-align: right;\">  0.774299</td><td style=\"text-align: right;\">     0.00272773</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.554925</td><td style=\"text-align: right;\">        0.909582</td><td style=\"text-align: right;\">    0.246006</td></tr>\n",
       "<tr><td>train_fn_9f7d3c88</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">     0.0727223 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.604877</td><td style=\"text-align: right;\">        0.919478</td><td style=\"text-align: right;\">    0.226629</td></tr>\n",
       "<tr><td>train_fn_a33b97db</td><td style=\"text-align: right;\">  0.775603</td><td style=\"text-align: right;\">    -0.175415  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.486876</td><td style=\"text-align: right;\">        0.863098</td><td style=\"text-align: right;\">    0.347746</td></tr>\n",
       "<tr><td>train_fn_a41d5d93</td><td style=\"text-align: right;\">  0.783431</td><td style=\"text-align: right;\">    -0.164044  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.501221</td><td style=\"text-align: right;\">        0.865647</td><td style=\"text-align: right;\">    0.347105</td></tr>\n",
       "<tr><td>train_fn_a73dfcf2</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.084446  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.637115</td><td style=\"text-align: right;\">        0.559004</td><td style=\"text-align: right;\">    0.638135</td></tr>\n",
       "<tr><td>train_fn_aa3c100c</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.0973615 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.643668</td><td style=\"text-align: right;\">        0.562603</td><td style=\"text-align: right;\">    0.661642</td></tr>\n",
       "<tr><td>train_fn_ab3fff1f</td><td style=\"text-align: right;\">  0.783431</td><td style=\"text-align: right;\">    -0.121065  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.507743</td><td style=\"text-align: right;\">        0.88499 </td><td style=\"text-align: right;\">    0.300057</td></tr>\n",
       "<tr><td>train_fn_acf66b11</td><td style=\"text-align: right;\">  0.759948</td><td style=\"text-align: right;\">    -0.207578  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.50602 </td><td style=\"text-align: right;\">        0.724846</td><td style=\"text-align: right;\">    0.563617</td></tr>\n",
       "<tr><td>train_fn_ad04b40c</td><td style=\"text-align: right;\">  0.763862</td><td style=\"text-align: right;\">    -0.166231  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.53456 </td><td style=\"text-align: right;\">        0.695307</td><td style=\"text-align: right;\">    0.592146</td></tr>\n",
       "<tr><td>train_fn_b41e28da</td><td style=\"text-align: right;\">  0.78865 </td><td style=\"text-align: right;\">    -0.163747  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.500806</td><td style=\"text-align: right;\">        0.868646</td><td style=\"text-align: right;\">    0.33261 </td></tr>\n",
       "<tr><td>train_fn_b9dc47d9</td><td style=\"text-align: right;\">  0.761252</td><td style=\"text-align: right;\">    -0.152895  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.539686</td><td style=\"text-align: right;\">        0.682711</td><td style=\"text-align: right;\">    0.598486</td></tr>\n",
       "<tr><td>train_fn_bc283978</td><td style=\"text-align: right;\">  0.795173</td><td style=\"text-align: right;\">    -0.166867  </td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.49656 </td><td style=\"text-align: right;\">        0.880492</td><td style=\"text-align: right;\">    0.318387</td></tr>\n",
       "<tr><td>train_fn_bfe2aaa8</td><td style=\"text-align: right;\">  0.579909</td><td style=\"text-align: right;\">     0.127908  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.693129</td><td style=\"text-align: right;\">        0.556305</td><td style=\"text-align: right;\">    0.698901</td></tr>\n",
       "<tr><td>train_fn_c0b10490</td><td style=\"text-align: right;\">  0.733855</td><td style=\"text-align: right;\">    -0.142825  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.536433</td><td style=\"text-align: right;\">        0.68481 </td><td style=\"text-align: right;\">    0.596584</td></tr>\n",
       "<tr><td>train_fn_c21da5bb</td><td style=\"text-align: right;\">  0.76908 </td><td style=\"text-align: right;\">    -0.283125  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.47241 </td><td style=\"text-align: right;\">        0.787674</td><td style=\"text-align: right;\">    0.480906</td></tr>\n",
       "<tr><td>train_fn_c37ffb38</td><td style=\"text-align: right;\">  0.782127</td><td style=\"text-align: right;\">     0.0997541 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.602287</td><td style=\"text-align: right;\">        0.931774</td><td style=\"text-align: right;\">    0.192746</td></tr>\n",
       "<tr><td>train_fn_c43a8b2b</td><td style=\"text-align: right;\">  0.756034</td><td style=\"text-align: right;\">    -0.25132   </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.49044 </td><td style=\"text-align: right;\">        0.757535</td><td style=\"text-align: right;\">    0.517486</td></tr>\n",
       "<tr><td>train_fn_c4c1b662</td><td style=\"text-align: right;\">  0.539465</td><td style=\"text-align: right;\">     0.166027  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.693194</td><td style=\"text-align: right;\">        0.554956</td><td style=\"text-align: right;\">    0.702299</td></tr>\n",
       "<tr><td>train_fn_cafabe3b</td><td style=\"text-align: right;\">  0.754729</td><td style=\"text-align: right;\">    -0.174774  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.531133</td><td style=\"text-align: right;\">        0.703554</td><td style=\"text-align: right;\">    0.577601</td></tr>\n",
       "<tr><td>train_fn_cea60c23</td><td style=\"text-align: right;\">  0.762557</td><td style=\"text-align: right;\">    -0.181872  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.525503</td><td style=\"text-align: right;\">        0.711651</td><td style=\"text-align: right;\">    0.58496 </td></tr>\n",
       "<tr><td>train_fn_d000943a</td><td style=\"text-align: right;\">  0.716895</td><td style=\"text-align: right;\">    -0.0509259 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.609428</td><td style=\"text-align: right;\">        0.621982</td><td style=\"text-align: right;\">    0.627597</td></tr>\n",
       "<tr><td>train_fn_d357fda2</td><td style=\"text-align: right;\">  0.796477</td><td style=\"text-align: right;\">    -0.188652  </td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.479799</td><td style=\"text-align: right;\">        0.884241</td><td style=\"text-align: right;\">    0.311508</td></tr>\n",
       "<tr><td>train_fn_d65384b2</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0849265 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.637886</td><td style=\"text-align: right;\">        0.559304</td><td style=\"text-align: right;\">    0.637747</td></tr>\n",
       "<tr><td>train_fn_d66fca06</td><td style=\"text-align: right;\">  0.784736</td><td style=\"text-align: right;\">     0.00673136</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.551103</td><td style=\"text-align: right;\">        0.920828</td><td style=\"text-align: right;\">    0.206467</td></tr>\n",
       "<tr><td>train_fn_da57b757</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.131469  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.682451</td><td style=\"text-align: right;\">        0.562453</td><td style=\"text-align: right;\">    0.683395</td></tr>\n",
       "<tr><td>train_fn_dd15de38</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0969815 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.637735</td><td style=\"text-align: right;\">        0.577148</td><td style=\"text-align: right;\">    0.631028</td></tr>\n",
       "<tr><td>train_fn_e2de0a7c</td><td style=\"text-align: right;\">  0.55773 </td><td style=\"text-align: right;\">     0.136554  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.680092</td><td style=\"text-align: right;\">        0.54416 </td><td style=\"text-align: right;\">    0.694905</td></tr>\n",
       "<tr><td>train_fn_e4bfa6d0</td><td style=\"text-align: right;\">  0.787345</td><td style=\"text-align: right;\">    -0.123345  </td><td style=\"text-align: right;\">                     8</td><td style=\"text-align: right;\">0.504844</td><td style=\"text-align: right;\">        0.891888</td><td style=\"text-align: right;\">    0.291075</td></tr>\n",
       "<tr><td>train_fn_e570c40b</td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">    -0.182899  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.487605</td><td style=\"text-align: right;\">        0.866847</td><td style=\"text-align: right;\">    0.337338</td></tr>\n",
       "<tr><td>train_fn_e5aebc6d</td><td style=\"text-align: right;\">  0.555121</td><td style=\"text-align: right;\">     0.0863669 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.637977</td><td style=\"text-align: right;\">        0.561553</td><td style=\"text-align: right;\">    0.637388</td></tr>\n",
       "<tr><td>train_fn_e95e283c</td><td style=\"text-align: right;\">  0.78604 </td><td style=\"text-align: right;\">     0.119696  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.62029 </td><td style=\"text-align: right;\">        0.930424</td><td style=\"text-align: right;\">    0.193782</td></tr>\n",
       "<tr><td>train_fn_ea8cf60d</td><td style=\"text-align: right;\">  0.778213</td><td style=\"text-align: right;\">     0.121054  </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.617559</td><td style=\"text-align: right;\">        0.928775</td><td style=\"text-align: right;\">    0.204706</td></tr>\n",
       "<tr><td>train_fn_f0086783</td><td style=\"text-align: right;\">  0.775603</td><td style=\"text-align: right;\">    -0.120441  </td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.510329</td><td style=\"text-align: right;\">        0.876743</td><td style=\"text-align: right;\">    0.321801</td></tr>\n",
       "<tr><td>train_fn_f3454da2</td><td style=\"text-align: right;\">  0.772994</td><td style=\"text-align: right;\">    -0.220412  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.498781</td><td style=\"text-align: right;\">        0.725296</td><td style=\"text-align: right;\">    0.558684</td></tr>\n",
       "<tr><td>train_fn_fd019dd3</td><td style=\"text-align: right;\">  0.749511</td><td style=\"text-align: right;\">    -0.156599  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.526295</td><td style=\"text-align: right;\">        0.678513</td><td style=\"text-align: right;\">    0.58853 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=482)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=482)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=557)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=557)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=646)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=646)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=718)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=718)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=805)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=805)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=876)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=876)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=962)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=962)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1035)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1035)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1124)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1124)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1202)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1202)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1281)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1281)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1360)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1360)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1453)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1453)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1531)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1531)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1630)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1630)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1701)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1701)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1787)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1787)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1858)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1858)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1945)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1945)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2025)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2025)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2102)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2102)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2188)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2188)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2261)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2261)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2345)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2345)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2434)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2434)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2516)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2516)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2605)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2605)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2692)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2692)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2782)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2782)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2868)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2868)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2956)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2956)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3027)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3027)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3114)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3114)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3179)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3179)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3272)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3272)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3362)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3362)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3438)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3438)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3519)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3519)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3597)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3597)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3678)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3678)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3768)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3768)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3842)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3842)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3929)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3929)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4010)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4010)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4095)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4095)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4181)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4181)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4246)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4246)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4338)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4338)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4403)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4403)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4496)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4496)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4560)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4560)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4653)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4653)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4739)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4739)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4813)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4813)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4900)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4900)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4981)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4981)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5067)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5067)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5142)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5142)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5225)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5225)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5311)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5311)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5380)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5380)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5468)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5468)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5548)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5548)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5649)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5649)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5724)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5724)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5806)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5806)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5892)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5892)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5966)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5966)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6051)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6051)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6125)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6125)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6210)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6210)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6284)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6284)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6376)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6376)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6462)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6462)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6547)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6547)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6633)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6633)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6708)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6708)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6790)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6790)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6865)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6865)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6948)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6948)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7023)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7023)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7114)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7114)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7196)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7196)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7281)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7281)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7352)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7352)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7438)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7438)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7510)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7510)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7595)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7595)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7677)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7677)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7751)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7751)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7840)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7840)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7920)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7920)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8010)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8010)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8087)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8087)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8176)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8176)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8258)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8258)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8333)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8333)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8429)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8429)\u001b[0m   warnings.warn(\n",
      "2024-08-31 07:25:06,533\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_fn_2024-08-31_04-20-32' in 0.0451s.\n",
      "2024-08-31 07:25:06,584\tINFO tune.py:1041 -- Total run time: 11074.33 seconds (11063.51 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /kaggle/working/tensorboard_logs\n",
    "\n",
    "\n",
    "# ray.init(ignore_reinit_error=True)\n",
    "# reporter = CLIReporter(\n",
    "#     metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "#     print_intermediate_tables=False\n",
    "# )\n",
    "\n",
    "reporter = JupyterNotebookReporter(\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "    print_intermediate_tables=False\n",
    ")\n",
    "\n",
    "# tuner = tune.run(\n",
    "#     train_fn,\n",
    "#     config=search_space,\n",
    "#     num_samples=25,\n",
    "#     progress_reporter=reporter,\n",
    "#     resources_per_trial={\"cpu\": 4, \"gpu\": 2},  # Adjust resources as needed\n",
    "#     scheduler=ASHAScheduler(\n",
    "#         metric=\"accuracy\",\n",
    "#         mode=\"max\",\n",
    "#         max_t=15,\n",
    "#         grace_period=1,\n",
    "#         reduction_factor=2\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# reporter = TensorBoardReporter(\n",
    "#     metric_columns=[\"loss\", \"accuracy\", \"training_iteration\", \"train_loss\", \"train_accuracy\"]\n",
    "# )\n",
    "\n",
    "\n",
    "result = tune.run(\n",
    "    train_fn,\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=100,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    search_alg=optuna_search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7a4b1",
   "metadata": {
    "papermill": {
     "duration": 0.039519,
     "end_time": "2024-08-31T07:25:06.848591",
     "exception": false,
     "start_time": "2024-08-31T07:25:06.809072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## REPORTER RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd289781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:25:06.947287Z",
     "iopub.status.busy": "2024-08-31T07:25:06.946285Z",
     "iopub.status.idle": "2024-08-31T07:25:06.954197Z",
     "shell.execute_reply": "2024-08-31T07:25:06.953316Z"
    },
    "papermill": {
     "duration": 0.059099,
     "end_time": "2024-08-31T07:25:06.956679",
     "exception": false,
     "start_time": "2024-08-31T07:25:06.897580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'dropout_rate': 0.4, 'batch_size': 32, 'weight_decay': 1e-05, 'lr': 2e-05, 'epochs': 5}\n",
      "Best trial final validation loss: 0.5563954301178455\n",
      "Best trial final validation accuracy: 0.7964774951076321\n",
      "Best trial final training loss: 0.20902084457359726\n",
      "Best trial final training accuracy: 0.9271255060728745\n",
      "Best trial final custom_metric: -0.0010707667350412509\n",
      "Best trial final Early Stopping Epoch: 5\n"
     ]
    }
   ],
   "source": [
    "# best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "# best_checkpoint_dir = best_trial.checkpoint.value\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "print(f\"Best trial final training loss: {best_trial.last_result['train_loss']}\")\n",
    "print(f\"Best trial final training accuracy: {best_trial.last_result['train_accuracy']}\")\n",
    "print(f\"Best trial final custom_metric: {best_trial.last_result['custom_metric']}\")\n",
    "print(f\"Best trial final Early Stopping Epoch: {best_trial.last_result['early_stopping_epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6b658",
   "metadata": {
    "papermill": {
     "duration": 0.040266,
     "end_time": "2024-08-31T07:25:07.036055",
     "exception": false,
     "start_time": "2024-08-31T07:25:06.995789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to Train the Model with the Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b93dd024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:25:07.117619Z",
     "iopub.status.busy": "2024-08-31T07:25:07.117317Z",
     "iopub.status.idle": "2024-08-31T07:25:07.143014Z",
     "shell.execute_reply": "2024-08-31T07:25:07.142203Z"
    },
    "papermill": {
     "duration": 0.069438,
     "end_time": "2024-08-31T07:25:07.144927",
     "exception": false,
     "start_time": "2024-08-31T07:25:07.075489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_best_model(config, device, train_dataset = train_ds_pd, val_dataset = validation_ds_pd):\n",
    "    print(f\"\"\"\n",
    "    Tuning Model with:\n",
    "    {config}\n",
    "    \"\"\")\n",
    "#     model = SentimentModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=3,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "    model = SentimentModel(\n",
    "        roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "        n_classes=3,\n",
    "        dropout_rate=config[\"dropout_rate\"]\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "#     class_weights_tmp = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss(weight=class_weights_tmp)\n",
    "#     criterion = nn.BCELoss()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    ## DATASET ENCODING\n",
    "    \n",
    "    train_dataset = SentimentDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "    val_dataset = SentimentDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_path = None\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    \n",
    "    ### Running for config epochs +patience+1 for introducing noise. \n",
    "    \n",
    "    for epoch in range((config[\"epochs\"]+patience+1)):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            with autocast(device_type=device.type):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = torch.zeros_like(probabilities)\n",
    "            max_indices = torch.argmax(probabilities, dim=1)\n",
    "            predictions.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "            predictions = predictions.float()\n",
    "                \n",
    "#                 loss = criterion(predictions, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "#             outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "# #             correct_train_preds += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "\n",
    "#             predicted_labels = (outputs > 0.5).float()\n",
    "#             correct_train_preds += (predicted_labels == labels).sum().item()\n",
    "#             total_train_preds += labels.numel()\n",
    "\n",
    "#             correct_train_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "#             total_train_preds += labels.numel()\n",
    "\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "\n",
    "            correct_train_preds += (predictions == labels).sum().item()\n",
    "            total_train_preds += labels.numel()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val_preds = 0\n",
    "        total_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "                predictions = torch.zeros_like(probabilities)\n",
    "                max_indices = torch.argmax(probabilities, dim=1)\n",
    "                predictions.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "                predictions = predictions.float()\n",
    "\n",
    "#                     loss = criterion(predictions, labels)\n",
    "                \n",
    "#                 outputs = torch.sigmoid(outputs)\n",
    "                \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "# #                 correct_val_preds += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "    \n",
    "#                 predicted_labels = (outputs > 0.5).float()\n",
    "#                 correct_val_preds += (predicted_labels == labels).sum().item()\n",
    "#                 total_val_preds += labels.numel()\n",
    "\n",
    "#                 correct_val_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "#                 total_val_preds += labels.numel()\n",
    "\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "\n",
    "                correct_val_preds += (predictions == labels).sum().item()\n",
    "                total_val_preds += labels.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_preds / total_val_preds\n",
    "\n",
    "        # Save the model checkpoint after each epoch\n",
    "        checkpoint_path = f\"/kaggle/working/best_checkpoint.pt\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "#         # Save the best model based on validation loss\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             best_model_path = checkpoint_path\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_since_improvement = 0\n",
    "            best_model_path = checkpoint_path\n",
    "            # Save the model checkpoint with the best validation loss\n",
    "#             checkpoint_path = f\"/kaggle/working/best_checkpoint.pt\"\n",
    "#             torch.save(model.state_dict(), checkpoint_path)\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config  # Save configuration\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            print(f\"Best Model Epoch Saved: {epoch - patience +1}\")\n",
    "#             print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "#         if epochs_since_improvement >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "#             print(f\"Best Model Epoch Saved: {epoch - patience}\")\n",
    "#             break\n",
    "            \n",
    "#         custom_metric = calculate_custom_metric(avg_val_loss, avg_train_loss)        \n",
    "        custom_metric = calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy)\n",
    "        print(f\"\"\"\n",
    "        Validation Loss: {avg_val_loss},\n",
    "        Training Loss: {avg_train_loss},\n",
    "        Argmax Binary Validation Accuracy: {val_accuracy},\n",
    "        Argmax Binary Training Accuracy: {train_accuracy},\n",
    "        Custom Metric: {custom_metric},\n",
    "        Epochs: {epoch+1}\n",
    "        \"\"\")\n",
    "    \n",
    "    print(f\"Best Validation Loss: {best_val_loss}, Best Validation accuracy: {val_accuracy}\")\n",
    "    \n",
    "    return model, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920908f",
   "metadata": {
    "papermill": {
     "duration": 0.042782,
     "end_time": "2024-08-31T07:25:07.233788",
     "exception": false,
     "start_time": "2024-08-31T07:25:07.191006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BEST MODEL AFTER FINAL TRAINING & VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00f9216b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:25:07.315968Z",
     "iopub.status.busy": "2024-08-31T07:25:07.315644Z",
     "iopub.status.idle": "2024-08-31T07:31:24.602267Z",
     "shell.execute_reply": "2024-08-31T07:31:24.601202Z"
    },
    "papermill": {
     "duration": 377.369224,
     "end_time": "2024-08-31T07:31:24.644549",
     "exception": false,
     "start_time": "2024-08-31T07:25:07.275325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Tuning Model with:\n",
      "    {'dropout_rate': 0.4, 'batch_size': 32, 'weight_decay': 1e-05, 'lr': 2e-05, 'epochs': 5}\n",
      "    \n",
      "\n",
      "        Validation Loss: 0.46481654420495033,\n",
      "        Training Loss: 0.552163476995427,\n",
      "        Argmax Binary Validation Accuracy: 0.7729941291585127,\n",
      "        Argmax Binary Training Accuracy: 0.6921577447893237,\n",
      "        Custom Metric: -0.22408592637372954,\n",
      "        Epochs: 1\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.45534227043390274,\n",
      "        Training Loss: 0.4039356545578662,\n",
      "        Argmax Binary Validation Accuracy: 0.7782126549249837,\n",
      "        Argmax Binary Training Accuracy: 0.8220122956965062,\n",
      "        Custom Metric: -0.2752672561673014,\n",
      "        Epochs: 2\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.49286471493542194,\n",
      "        Training Loss: 0.32313298610903374,\n",
      "        Argmax Binary Validation Accuracy: 0.7886497064579256,\n",
      "        Argmax Binary Training Accuracy: 0.8705952916479233,\n",
      "        Custom Metric: -0.1699463345143108,\n",
      "        Epochs: 3\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.5527901519089937,\n",
      "        Training Loss: 0.2496644074646689,\n",
      "        Argmax Binary Validation Accuracy: 0.7886497064579256,\n",
      "        Argmax Binary Training Accuracy: 0.9074823811665917,\n",
      "        Custom Metric: -0.024880344972436463,\n",
      "        Epochs: 4\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.5637007690966129,\n",
      "        Training Loss: 0.19576215861941412,\n",
      "        Argmax Binary Validation Accuracy: 0.7886497064579256,\n",
      "        Argmax Binary Training Accuracy: 0.9311740890688259,\n",
      "        Custom Metric: 0.030282559182736812,\n",
      "        Epochs: 5\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.6178949438035488,\n",
      "        Training Loss: 0.13516921316869826,\n",
      "        Argmax Binary Validation Accuracy: 0.7873450750163079,\n",
      "        Argmax Binary Training Accuracy: 0.957714799820063,\n",
      "        Custom Metric: 0.15709759650654376,\n",
      "        Epochs: 6\n",
      "        \n",
      "Early stopping at epoch 7\n",
      "Best Model Epoch Saved: 2\n",
      "Best Validation Loss: 0.45534227043390274, Best Validation accuracy: 0.7873450750163079\n"
     ]
    }
   ],
   "source": [
    "best_config = best_trial.config\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model with the best configuration\n",
    "best_model, best_model_path = train_best_model(best_config, device, train_ds_pd, validation_ds_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546a91c",
   "metadata": {
    "papermill": {
     "duration": 0.040809,
     "end_time": "2024-08-31T07:31:24.725825",
     "exception": false,
     "start_time": "2024-08-31T07:31:24.685016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ba6034e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:31:24.809759Z",
     "iopub.status.busy": "2024-08-31T07:31:24.809333Z",
     "iopub.status.idle": "2024-08-31T07:31:25.989162Z",
     "shell.execute_reply": "2024-08-31T07:31:25.988235Z"
    },
    "papermill": {
     "duration": 1.225583,
     "end_time": "2024-08-31T07:31:25.991472",
     "exception": false,
     "start_time": "2024-08-31T07:31:24.765889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: /kaggle/working/final_best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "#### Comment when wanting to select the model which is stopped early to avoid over or under fitting\n",
    "\n",
    "# final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# torch.save(best_model.state_dict(), final_model_path) \n",
    "\n",
    "#### Comment when wanting to select the model which is stopped early to avoid over or under fitting\n",
    "\n",
    "final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# torch.save(best_model.state_dict(), final_model_path)\n",
    "torch.save({'model_state_dict': best_model.state_dict(),'config': best_config}, final_model_path)\n",
    "\n",
    "print(f\"Best model saved at: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d871d2",
   "metadata": {
    "papermill": {
     "duration": 0.040342,
     "end_time": "2024-08-31T07:31:26.074800",
     "exception": false,
     "start_time": "2024-08-31T07:31:26.034458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29ed7a30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:31:26.158037Z",
     "iopub.status.busy": "2024-08-31T07:31:26.157690Z",
     "iopub.status.idle": "2024-08-31T07:31:27.739995Z",
     "shell.execute_reply": "2024-08-31T07:31:27.738836Z"
    },
    "papermill": {
     "duration": 1.627322,
     "end_time": "2024-08-31T07:31:27.742038",
     "exception": false,
     "start_time": "2024-08-31T07:31:26.114716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /kaggle/working/final_best_model.pt for inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# final_model_path = \"/kaggle/input/tachygraphy-lv2-emotion-moodtags-v1/transformers/default/1/final_best_model.pt\"\n",
    "\n",
    "\n",
    "checkpoint = torch.load(final_model_path)\n",
    "config = checkpoint['config']\n",
    "\n",
    "# loaded_model = SentimentModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=3,\n",
    "#     dropout_rate=config[\"dropout_rate\"]\n",
    "# )\n",
    "loaded_model = SentimentModel(\n",
    "    roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "    n_classes=3,\n",
    "    dropout_rate=config[\"dropout_rate\"]\n",
    ")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"Loaded model from {final_model_path} for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa844b",
   "metadata": {
    "papermill": {
     "duration": 0.041756,
     "end_time": "2024-08-31T07:31:27.824330",
     "exception": false,
     "start_time": "2024-08-31T07:31:27.782574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PREDICT ON RANDOM INPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e1de3",
   "metadata": {
    "papermill": {
     "duration": 0.040056,
     "end_time": "2024-08-31T07:31:27.904368",
     "exception": false,
     "start_time": "2024-08-31T07:31:27.864312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2939609",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:31:27.987127Z",
     "iopub.status.busy": "2024-08-31T07:31:27.986728Z",
     "iopub.status.idle": "2024-08-31T07:31:27.994176Z",
     "shell.execute_reply": "2024-08-31T07:31:27.993260Z"
    },
    "papermill": {
     "duration": 0.05061,
     "end_time": "2024-08-31T07:31:27.996003",
     "exception": false,
     "start_time": "2024-08-31T07:31:27.945393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, device, max_len=128):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    return torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "#     # Convert logits to probabilities using sigmoid\n",
    "#     probabilities = torch.sigmoid(logits)\n",
    "    \n",
    "#     # Convert probabilities to binary predictions\n",
    "#     predictions = torch.zeros_like(probabilities)\n",
    "#     max_indices = torch.argmax(probabilities, dim=1)\n",
    "#     predictions.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "    \n",
    "#     # Move predictions to CPU and convert to numpy for easy manipulation\n",
    "#     predictions_array = predictions.cpu().numpy().squeeze()\n",
    "\n",
    "#     return predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4265034a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:31:28.078018Z",
     "iopub.status.busy": "2024-08-31T07:31:28.077455Z",
     "iopub.status.idle": "2024-08-31T07:31:28.086586Z",
     "shell.execute_reply": "2024-08-31T07:31:28.085657Z"
    },
    "papermill": {
     "duration": 0.051932,
     "end_time": "2024-08-31T07:31:28.088352",
     "exception": false,
     "start_time": "2024-08-31T07:31:28.036420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = loaded_model\n",
    "best_model = best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb205388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:31:28.169932Z",
     "iopub.status.busy": "2024-08-31T07:31:28.169614Z",
     "iopub.status.idle": "2024-08-31T07:31:28.750938Z",
     "shell.execute_reply": "2024-08-31T07:31:28.749821Z"
    },
    "papermill": {
     "duration": 0.624716,
     "end_time": "2024-08-31T07:31:28.753262",
     "exception": false,
     "start_time": "2024-08-31T07:31:28.128546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dcd591",
   "metadata": {
    "papermill": {
     "duration": 0.041324,
     "end_time": "2024-08-31T07:31:28.838086",
     "exception": false,
     "start_time": "2024-08-31T07:31:28.796762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Samples & Random Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "829549e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:31:28.921607Z",
     "iopub.status.busy": "2024-08-31T07:31:28.920880Z",
     "iopub.status.idle": "2024-08-31T07:31:29.227318Z",
     "shell.execute_reply": "2024-08-31T07:31:29.226436Z"
    },
    "papermill": {
     "duration": 0.351238,
     "end_time": "2024-08-31T07:31:29.229334",
     "exception": false,
     "start_time": "2024-08-31T07:31:28.878096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment polarities for 'hey! hru, wanna ply valo toni8?': [[0.00807754 0.963955   0.03434299]]\n",
      "NEAGTIVE: 0.0, NEUTRAL: 1.0, POSITIVE: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted sentiment polarities for '{sample_text}': {predictions}\")\n",
    "\n",
    "\n",
    "probabilities_array = predictions.squeeze()\n",
    "binary_predictions = np.zeros_like(probabilities_array)\n",
    "max_indices = np.argmax(probabilities_array)\n",
    "binary_predictions[max_indices] = 1\n",
    "\n",
    "print(f\"NEAGTIVE: {binary_predictions[0]}, NEUTRAL: {binary_predictions[1]}, POSITIVE: {binary_predictions[2]}\")\n",
    "\n",
    "# labels = ['Negative', 'Neutral', 'Positive']\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "ax.barh(SENTIMENT_POLARITY_LABELS, probabilities_array, color=['red', 'blue', 'green'])\n",
    "\n",
    "# Adding labels\n",
    "for i, (label, prob) in enumerate(zip(SENTIMENT_POLARITY_LABELS, probabilities_array)):\n",
    "    ax.text(prob, i, f'{prob:.2f}', va='center')\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Probability')\n",
    "ax.set_title('Sentiment Probabilities')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9a47e248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:31:29.313328Z",
     "iopub.status.busy": "2024-08-31T07:31:29.312422Z",
     "iopub.status.idle": "2024-08-31T07:31:29.506434Z",
     "shell.execute_reply": "2024-08-31T07:31:29.505428Z"
    },
    "papermill": {
     "duration": 0.238228,
     "end_time": "2024-08-31T07:31:29.508578",
     "exception": false,
     "start_time": "2024-08-31T07:31:29.270350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment polarities for 'what a badass char is arthur, he is the best game char ever made, i luv rdr2': [[0.00511268 0.01354392 0.9853373 ]]\n",
      "NEAGTIVE: 0.0, NEUTRAL: 0.0, POSITIVE: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"what a badass char is arthur, he is the best game char ever made, i luv rdr2\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted sentiment polarities for '{sample_text}': {predictions}\")\n",
    "\n",
    "\n",
    "probabilities_array = predictions.squeeze()\n",
    "binary_predictions = np.zeros_like(probabilities_array)\n",
    "max_indices = np.argmax(probabilities_array)\n",
    "binary_predictions[max_indices] = 1\n",
    "\n",
    "print(f\"NEAGTIVE: {binary_predictions[0]}, NEUTRAL: {binary_predictions[1]}, POSITIVE: {binary_predictions[2]}\")\n",
    "\n",
    "# labels = ['Negative', 'Neutral', 'Positive']\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "ax.barh(SENTIMENT_POLARITY_LABELS, probabilities_array, color=['red', 'blue', 'green'])\n",
    "\n",
    "# Adding labels\n",
    "for i, (label, prob) in enumerate(zip(SENTIMENT_POLARITY_LABELS, probabilities_array)):\n",
    "    ax.text(prob, i, f'{prob:.2f}', va='center')\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Probability')\n",
    "ax.set_title('Sentiment Probabilities')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d880eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:31:29.594927Z",
     "iopub.status.busy": "2024-08-31T07:31:29.594572Z",
     "iopub.status.idle": "2024-08-31T07:31:29.768453Z",
     "shell.execute_reply": "2024-08-31T07:31:29.767207Z"
    },
    "papermill": {
     "duration": 0.220199,
     "end_time": "2024-08-31T07:31:29.772105",
     "exception": false,
     "start_time": "2024-08-31T07:31:29.551906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment polarities for 'I don't no fr y hes sooo sad.': [[0.98280627 0.01122371 0.01203231]]\n",
      "NEAGTIVE: 1.0, NEUTRAL: 0.0, POSITIVE: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"I don't no fr y hes sooo sad.\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted sentiment polarities for '{sample_text}': {predictions}\")\n",
    "\n",
    "\n",
    "probabilities_array = predictions.squeeze()\n",
    "binary_predictions = np.zeros_like(probabilities_array)\n",
    "max_indices = np.argmax(probabilities_array)\n",
    "binary_predictions[max_indices] = 1\n",
    "\n",
    "print(f\"NEAGTIVE: {binary_predictions[0]}, NEUTRAL: {binary_predictions[1]}, POSITIVE: {binary_predictions[2]}\")\n",
    "\n",
    "\n",
    "# labels = ['Negative', 'Neutral', 'Positive']\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "ax.barh(SENTIMENT_POLARITY_LABELS, probabilities_array, color=['red', 'blue', 'green'])\n",
    "\n",
    "# Adding labels\n",
    "for i, (label, prob) in enumerate(zip(SENTIMENT_POLARITY_LABELS, probabilities_array)):\n",
    "    ax.text(prob, i, f'{prob:.2f}', va='center')\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Probability')\n",
    "ax.set_title('Sentiment Probabilities')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5557640,
     "sourceId": 9273793,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11505.11713,
   "end_time": "2024-08-31T07:31:35.048948",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-31T04:19:49.931818",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "010309c1c28e4dbfbdc859af6fc25fd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0a3ac452c3bc4b8ab1ea504b325f27a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1f64f5a7fec640569208d51627dd171a",
       "max": 52.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1b744328e2c6468f8719f91452684d40",
       "value": 52.0
      }
     },
     "0ddb2741be6147bebd1e914e134e9c4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8293acdf7ca449b29df2134b53d8b9e7",
        "IPY_MODEL_bfe768774b214be18d20661c0e034dff",
        "IPY_MODEL_5744486eba7c46ad87068a3740a68004"
       ],
       "layout": "IPY_MODEL_ea7cf76d5a8b459b9be623ce456f5b7b"
      }
     },
     "0f68ba4f701a4010829c0f01941fcabb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11bee1d45c194d24abc8b020a8ba36c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19608913ae3f4073bb8a7cbe85de8bd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1b744328e2c6468f8719f91452684d40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1f64f5a7fec640569208d51627dd171a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "205a439b8a3c4b7586a82e8fd4cad3dc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25da59b2912e4e4b83f120c77441e0c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "29683edade154cfe96c08b30fe084896": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2a6caf5ece1c4d58be966be62c98f8bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2dae532d13cb4d8494592cdc7ea75540",
        "IPY_MODEL_8d9b919f0b144d6fbb99fea7c6428f23",
        "IPY_MODEL_819eb2309e9b4f32a35bbbe04b8f13ba"
       ],
       "layout": "IPY_MODEL_205a439b8a3c4b7586a82e8fd4cad3dc"
      }
     },
     "2dae532d13cb4d8494592cdc7ea75540": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d3168b5cbaad4c389d8c2d5193f09dba",
       "placeholder": "​",
       "style": "IPY_MODEL_7fdb90a9b9c14c78bc4c2c4e8f0a3c79",
       "value": "config.json: 100%"
      }
     },
     "3b49e04ae8644cb1be96f27ab45020d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b11441bc485d4a508e1e3f040b540c64",
        "IPY_MODEL_0a3ac452c3bc4b8ab1ea504b325f27a9",
        "IPY_MODEL_717eff8f25014f1586b68a20e634abc7"
       ],
       "layout": "IPY_MODEL_11bee1d45c194d24abc8b020a8ba36c9"
      }
     },
     "3c7d7662bac2432999e0e868be7a5f84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "44005e65095c419fbb80752aa64a7471": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5744486eba7c46ad87068a3740a68004": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_44005e65095c419fbb80752aa64a7471",
       "placeholder": "​",
       "style": "IPY_MODEL_010309c1c28e4dbfbdc859af6fc25fd7",
       "value": " 2.46M/2.46M [00:00&lt;00:00, 40.4MB/s]"
      }
     },
     "700a8397a27941d8a54bc9f25e791250": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "717eff8f25014f1586b68a20e634abc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3c7d7662bac2432999e0e868be7a5f84",
       "placeholder": "​",
       "style": "IPY_MODEL_eee1bbf4c5a84e5dbae00c3b40d27764",
       "value": " 52.0/52.0 [00:00&lt;00:00, 4.43kB/s]"
      }
     },
     "7fdb90a9b9c14c78bc4c2c4e8f0a3c79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "819eb2309e9b4f32a35bbbe04b8f13ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0f68ba4f701a4010829c0f01941fcabb",
       "placeholder": "​",
       "style": "IPY_MODEL_832051128f674ccb831e2c83140c85a9",
       "value": " 579/579 [00:00&lt;00:00, 48.2kB/s]"
      }
     },
     "8293acdf7ca449b29df2134b53d8b9e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c84cb666fdf34f3f94c7f1f49ef3a9e7",
       "placeholder": "​",
       "style": "IPY_MODEL_19608913ae3f4073bb8a7cbe85de8bd4",
       "value": "spm.model: 100%"
      }
     },
     "832051128f674ccb831e2c83140c85a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8b029e55d8344070b051ba2aa9190d62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d9b919f0b144d6fbb99fea7c6428f23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bfdc018186a24aa39e8d9304a3606367",
       "max": 579.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_29683edade154cfe96c08b30fe084896",
       "value": 579.0
      }
     },
     "b11441bc485d4a508e1e3f040b540c64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_25da59b2912e4e4b83f120c77441e0c1",
       "placeholder": "​",
       "style": "IPY_MODEL_db7e90c812f44f77aa0191f06dc95471",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "bfdc018186a24aa39e8d9304a3606367": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bfe768774b214be18d20661c0e034dff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8b029e55d8344070b051ba2aa9190d62",
       "max": 2464616.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_700a8397a27941d8a54bc9f25e791250",
       "value": 2464616.0
      }
     },
     "c84cb666fdf34f3f94c7f1f49ef3a9e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3168b5cbaad4c389d8c2d5193f09dba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db7e90c812f44f77aa0191f06dc95471": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ea7cf76d5a8b459b9be623ce456f5b7b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eee1bbf4c5a84e5dbae00c3b40d27764": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
