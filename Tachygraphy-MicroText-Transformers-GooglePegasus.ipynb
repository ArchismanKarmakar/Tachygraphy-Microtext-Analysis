{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c874e7",
   "metadata": {
    "papermill": {
     "duration": 0.003059,
     "end_time": "2024-09-28T19:08:48.597070",
     "exception": false,
     "start_time": "2024-09-28T19:08:48.594011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Google Pegasus\n",
    "### Tachygraphy Transformers Generative Model\n",
    "### PyTorch Distributed Data Parallel in use.\n",
    "### Dated - 25.09.2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd20861",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-28T19:08:48.604594Z",
     "iopub.status.busy": "2024-09-28T19:08:48.604217Z",
     "iopub.status.idle": "2024-09-28T19:08:48.628799Z",
     "shell.execute_reply": "2024-09-28T19:08:48.627423Z"
    },
    "papermill": {
     "duration": 0.031498,
     "end_time": "2024-09-28T19:08:48.631141",
     "exception": false,
     "start_time": "2024-09-28T19:08:48.599643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp.py\n",
    "\n",
    "\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, BertModel, BertTokenizer\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ray\n",
    "from ray import tune\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "\n",
    "import argparse # CPMP\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# CPMP imports for DDP\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "# from optuna.integration import PyTorchLightningPruner\n",
    "from ray import tune\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "# from ray.tune.integration.pytorch import TuneReportCallback\n",
    "from torch.amp import GradScaler, autocast\n",
    "# from ray.tune.integration.optuna import OptunaSearch\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from torch import autocast\n",
    "# from ray import tune\n",
    "# from ray.tune.integration.tensorboard import TensorBoardReporter\n",
    "from ray.tune.logger import TBXLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ray.train import report\n",
    "# from ray.tune.integration.jupyter import JupyterNotebookReporter\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from ray.tune.schedulers import HyperBandScheduler, AsyncHyperBandScheduler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate import Accelerator\n",
    "# import evaluate\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'  # Use a port that's available on Kaggle\n",
    "    os.environ['RANK'] = str(rank)\n",
    "    os.environ['WORLD_SIZE'] = str(world_size)\n",
    "    # Initialize process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataset, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = dataset['input']\n",
    "        self.targets = dataset['target']\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "        encodings = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        target_encodings = self.tokenizer(target, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encodings['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encodings['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(target_encodings['input_ids'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n",
    "def test_train_split(dataset, test_ratio=0.1):\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  return dataset[~test_indices], dataset[test_indices]\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(pred_text, target_text, model, tokenizer, device, model_type=\"pegasus\"):\n",
    "    # Move model to GPU if available\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize and encode both texts, moving inputs to the GPU\n",
    "    pred_inputs = tokenizer(pred_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    target_inputs = tokenizer(target_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Get embeddings based on the model type\n",
    "    with torch.no_grad():\n",
    "#         with torch.cuda.amp.autocast(enabled=True):  # Autocast for mixed precision training/inference\n",
    "        if model_type in [\"bart\", \"pegasus\", \"t5\"]:  # For seq2seq models like BART, Pegasus, T5\n",
    "            # Use encoder outputs\n",
    "            pred_embeddings = model.model.encoder(input_ids=pred_inputs.input_ids, attention_mask=pred_inputs.attention_mask).last_hidden_state.mean(dim=1).to(device)\n",
    "            target_embeddings = model.model.encoder(input_ids=target_inputs.input_ids, attention_mask=target_inputs.attention_mask).last_hidden_state.mean(dim=1).to(device)\n",
    "\n",
    "        elif model_type == \"llama\":  # For LLaMA models\n",
    "            pred_embeddings = model(**pred_inputs).hidden_states[-1].mean(dim=1).to(device)\n",
    "            target_embeddings = model(**target_inputs).hidden_states[-1].mean(dim=1).to(device)\n",
    "\n",
    "        else:  # For models like BERT, RoBERTa (directly exposing `last_hidden_state`)\n",
    "            pred_embeddings = model(**pred_inputs).last_hidden_state.mean(dim=1).to(device)\n",
    "            target_embeddings = model(**target_inputs).last_hidden_state.mean(dim=1).to(device)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(pred_embeddings.cpu(), target_embeddings.cpu())[0].item()\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        EarlyStopping to stop training when a metric has stopped improving.\n",
    "\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss improved to {val_loss:.4f}')\n",
    "#             torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        else:\n",
    "            self.counter += 1\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss did not improve. Patience: {self.patience}, Counter: {self.counter}')\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "def train_model(rank, config, train_dataset, val_dataset, world_size):\n",
    "    \n",
    "    setup(rank, world_size)\n",
    "    \n",
    "#     tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "#     model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "    tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "    model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large')\n",
    "#     bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#     bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     bert_model.to(device)\n",
    "\n",
    "#     model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "#     model.to(device)\n",
    "    model = model.to(rank)\n",
    "    model = DistributedDataParallel(model, device_ids=[rank])\n",
    "    \n",
    "#     train_texts = train_ds_pd['input']\n",
    "#     train_labels = train_ds_pd['target']\n",
    "#     val_texts = validation_ds_pd['input']\n",
    "#     val_labels = validation_ds_pd['target']\n",
    "\n",
    "#     train_dataset = TextDataset(tokenizer, train_texts, train_labels)\n",
    "#     val_dataset = TextDataset(tokenizer, val_texts, val_labels)\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset,\n",
    "                                       num_replicas=world_size,\n",
    "                                       rank=rank,\n",
    "                                       shuffle=False,\n",
    "                                       )\n",
    "    valid_sampler = DistributedSampler(val_dataset,\n",
    "                                       num_replicas=world_size,\n",
    "                                       rank=rank,\n",
    "                                       shuffle=False,\n",
    "                                       )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=int(config['batch_size']),\n",
    "                              shuffle=False,\n",
    "                              sampler=train_sampler)\n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                            batch_size=int(config['batch_size']),\n",
    "                            shuffle=False,\n",
    "                            sampler=valid_sampler)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config['lr'])\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Initialize the gradient scaler for mixed precision\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "    # Training loop with mixed precision\n",
    "    for epoch in range(int(config['epochs'])):\n",
    "        model.train()\n",
    "        \n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(rank)\n",
    "            attention_mask = batch['attention_mask'].to(rank)\n",
    "            labels = batch['labels'].to(rank)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             with torch.cuda.amp.autocast(enabled=True):  # Use autocast for mixed precision\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Scale the loss and backpropagate\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation loop with mixed precision\n",
    "        model.eval()\n",
    "        \n",
    "        total_val_loss = 0.0\n",
    "        val_similarity = 0.0\n",
    "        num_val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(rank)\n",
    "                attention_mask = batch['attention_mask'].to(rank)\n",
    "                labels = batch['labels'].to(rank)\n",
    "                \n",
    "#                 with torch.cuda.amp.autocast(enabled=True):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                preds = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=512)\n",
    "#                 preds = model.module.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "                target_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "                for pred_text, target_text in zip(pred_texts, target_texts):\n",
    "#                     similarity = calculate_cosine_similarity(pred_text, target_text, bert_model, bert_tokenizer)\n",
    "                    similarity = calculate_cosine_similarity(pred_text, target_text, model.module, tokenizer, device=rank)\n",
    "                    val_similarity += similarity\n",
    "                    num_val_samples += 1\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "#         avg_val_similarity = val_similarity / len(val_loader)\n",
    "        avg_val_similarity = val_similarity / num_val_samples\n",
    "        \n",
    "\n",
    "\n",
    "        # Report both loss and similarity\n",
    "#         tune.report(train_loss=avg_train_loss, val_loss=avg_val_loss, val_similarity=avg_val_similarity)\n",
    "        \n",
    "#         report({\n",
    "#             \"loss\": avg_val_loss,\n",
    "#             \"similarity\": avg_val_similarity,\n",
    "#             \"train_loss\": avg_train_loss,\n",
    "#             \"early_stopping_epoch\": epoch + 1,\n",
    "#         })\n",
    "\n",
    "        print(f\"\"\"\n",
    "             loss: {avg_val_loss},\n",
    "             similarity: {avg_val_similarity},\n",
    "             train_loss: {avg_train_loss},\n",
    "             early_stopping_epoch: {epoch + 1},\n",
    "            \"\"\")\n",
    "        \n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        \n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    if rank == 0:  # Only save from the main process\n",
    "        torch.save(model.module.state_dict(), \"ddp_model.pth\")\n",
    "    \n",
    "    cleanup()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_ddp(demo_fn, world_size):\n",
    "    \n",
    "    \n",
    "    dataset = pd.read_excel('/kaggle/input/dataset-tachygraphy/Tachygraphy_MicroText-AIO-V2.xlsx')\n",
    "    \n",
    "    df = dataset\n",
    "    \n",
    "    df.rename(columns={'Informal Text':'input', 'Expanded Meaning':'target'}, inplace = True)\n",
    "    \n",
    "    df['input'] = df['input'].astype(str)\n",
    "    df['target'] = df['target'].astype(str)\n",
    "    \n",
    "    train_ds_pd, validation_ds_pd = test_train_split(df)\n",
    "    print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_ds_pd), len(validation_ds_pd)))\n",
    "    \n",
    "    train_ds_pd = train_ds_pd.reset_index(drop=True)\n",
    "    validation_ds_pd = validation_ds_pd.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    config = {\n",
    "        \"lr\": 5e-6,\n",
    "        \"batch_size\": 2,\n",
    "        \"epochs\": 5\n",
    "    }\n",
    "    \n",
    "    tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "#     bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    train_dataset = TextDataset(tokenizer, train_ds_pd)\n",
    "    val_dataset = TextDataset(tokenizer, validation_ds_pd)\n",
    "    \n",
    "    torch.multiprocessing.spawn(demo_fn,\n",
    "             args=(config, train_dataset, val_dataset, world_size),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import os\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"total GPUs: {n_gpus}\")\n",
    "    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n",
    "    world_size = n_gpus\n",
    "    \n",
    "    config = {\n",
    "        \"lr\": 5e-6,\n",
    "        \"batch_size\": 2,\n",
    "        \"epochs\": 5\n",
    "    }\n",
    "    \n",
    "    run_ddp(train_model, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00802d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T19:08:48.637910Z",
     "iopub.status.busy": "2024-09-28T19:08:48.637127Z",
     "iopub.status.idle": "2024-09-28T19:53:40.054273Z",
     "shell.execute_reply": "2024-09-28T19:53:40.053114Z"
    },
    "papermill": {
     "duration": 2691.422974,
     "end_time": "2024-09-28T19:53:40.056724",
     "exception": false,
     "start_time": "2024-09-28T19:08:48.633750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_EmotionMoodtags_Dataset.csv\r\n",
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_dataset_main.csv\r\n",
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_MicroText-AIO-V2.xlsx\r\n",
      "total GPUs: 2\r\n",
      "9326 examples in training, 954 examples in testing.\r\n",
      "tokenizer_config.json: 100%|██████████████████| 88.0/88.0 [00:00<00:00, 534kB/s]\r\n",
      "spiece.model: 100%|████████████████████████| 1.91M/1.91M [00:00<00:00, 8.98MB/s]\r\n",
      "special_tokens_map.json: 100%|████████████████| 65.0/65.0 [00:00<00:00, 485kB/s]\r\n",
      "config.json: 100%|█████████████████████████| 3.09k/3.09k [00:00<00:00, 33.7MB/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\r\n",
      "  warnings.warn(\r\n",
      "pytorch_model.bin: 100%|████████████████████| 2.28G/2.28G [00:09<00:00, 237MB/s]\r\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "generation_config.json: 100%|██████████████████| 260/260 [00:00<00:00, 1.71MB/s]\r\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/kaggle/working/ddp.py:266: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Initialize the gradient scaler for mixed precision\r\n",
      "/kaggle/working/ddp.py:266: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Initialize the gradient scaler for mixed precision\r\n",
      "W0928 19:53:38.448000 134090306938688 torch/multiprocessing/spawn.py:146] Terminating process 62 via signal SIGTERM\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/ddp.py\", line 433, in <module>\r\n",
      "    run_ddp(train_model, world_size)\r\n",
      "  File \"/kaggle/working/ddp.py\", line 408, in run_ddp\r\n",
      "    torch.multiprocessing.spawn(demo_fn,\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 282, in spawn\r\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 238, in start_processes\r\n",
      "    while not context.join():\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 189, in join\r\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \r\n",
      "\r\n",
      "-- Process 1 terminated with the following error:\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 76, in _wrap\r\n",
      "    fn(i, *args)\r\n",
      "  File \"/kaggle/working/ddp.py\", line 316, in train_model\r\n",
      "    preds = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=512)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1729, in __getattr__\r\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\r\n",
      "AttributeError: 'DistributedDataParallel' object has no attribute 'generate'\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/ddp.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5557640,
     "sourceId": 9500773,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2694.322236,
   "end_time": "2024-09-28T19:53:40.285046",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-28T19:08:45.962810",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
